{"index":{"title":"Zero's Garden","links":["notes/Programming","notes/SRE","notes/Taking-Notes","notes/AI/AI-Tools"],"tags":[],"content":"Welcome to my digital garden, where I keep notes of my thoughts and knowledge about topics I’m intretested in like computer science.\nNotes are loosely grouped into topics and interconnected, and there is no strict hierarchy. Here are some top-level links to expand on.\n\nProgramming\nSRE\nTaking Notes\nAI Tools\n\nPersonal Use Notice §\nContent on this website is provided “as is” and the author may move or delete content at any time, causing link rot.\nIt is therefore not recommeded to directly link to any specific notes on this website, except as required by the CC BY-NC-ND 4.0 license.\nYou may contact the author if you need a stable reference. The author may decide, at their sole discretion, to write a blog post hosted elsewhere that has a stable URL.\nLegal Notice §\nUnless otherwise stated, content on this website is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License."},"notes/AI/AI-Tools":{"title":"AI Tools","links":["notes/AI/ChatGPT","notes/AI/LLM-Models"],"tags":[],"content":"Chatbots §\nChatGPT\nCoding Assistant §\n\nGitHub Copilot\nAmazon CodeWhisperer\n\nModels §\n\nLLM Models\n\nToolkit §\n\nLangChain\n"},"notes/AI/ChatGPT":{"title":"ChatGPT","links":["notes/AI/New-Bing","notes/AI/Poe","notes/AI/Forefront-Chat"],"tags":[],"content":"Self-hosted Web UI §\nChatGPT-Next-Web\nDerivatives §\n\nNew Bing\nPoe\nForefront Chat\n"},"notes/AI/Forefront-Chat":{"title":"Forefront Chat","links":[],"tags":[],"content":"www.forefront.ai\nForefront chat provides free access to GPT-4 for now."},"notes/AI/GPT-4-Chat-Demo":{"title":"GPT-4 Chat Demo","links":[],"tags":[],"content":"\n\n                  \n                  Warning \n                  \n                \nThe chat was saved to demostrate how to ask GPT-4 questions and provide hints. The formula may be incorrect.\n\nChat Text §\nCould you estimate the average time a TCP request waits in the surge queue of an AWS CLB from CloudWatch metrics? §\nTo estimate the average time a TCP request waits in the surge queue of an Amazon Web Services (AWS) Classic Load Balancer (CLB), you can use the SurgeQueueLength and RequestCount metrics from Amazon CloudWatch.\nSurgeQueueLength is the number of requests in the surge queue, and RequestCount is the total number of requests received by the load balancer.\n\nFirst, get the average SurgeQueueLength for a specific time period. To do this, use the avg statistic for the SurgeQueueLength metric in CloudWatch.\n\nNamespace: AWS/ELB\nMetricName: SurgeQueueLength\nStatistic: Average\nDimensions: LoadBalancerName=&lt;your_load_balancer_name&gt;\n\n\nNext, get the total RequestCount for the same time period. Use the sum statistic for the RequestCount metric in CloudWatch.\n\nNamespace: AWS/ELB\nMetricName: RequestCount\nStatistic: Sum\nDimensions: LoadBalancerName=&lt;your_load_balancer_name&gt;\n\n\nDivide the average SurgeQueueLength by the total RequestCount to get the average time a TCP request waits in the surge queue.\n\nAverageTimeInSurgeQueue = AverageSurgeQueueLength / TotalRequestCount\n\nKeep in mind that this is an estimate, and the actual time a request spends in the surge queue may vary depending on factors such as request rate, backend instance performance, and load balancer configuration.\nAdditionally, you may want to monitor the SpilloverCount metric, which represents the number of requests that have been rejected due to the surge queue being full. A high SpilloverCount could indicate that the load balancer is struggling to handle incoming traffic, and you may need to take action, such as adding more backend instances or increasing the surge queue size.\navg statistics of SurgeQueueLength is averaged across individual load balancer nodes. Should the SampleCount statistics be used to calculate the total length of surge queue? §\nYes, you’re correct. Since the avg statistic for SurgeQueueLength is an average value across individual load balancer nodes, it’s not ideal for calculating the total length of the surge queue. Instead, you can use the SampleCount statistic, which gives you the number of data points used for the computation of the average.\nTo calculate the total length of the surge queue for the desired time period, you can multiply the avg statistic by the SampleCount:\nTotalSurgeQueueLength = AverageSurgeQueueLength * SampleCount\n\nNow you can use the TotalSurgeQueueLength to estimate the average time a TCP request waits in the surge queue:\nAverageTimeInSurgeQueue = TotalSurgeQueueLength / TotalRequestCount\n\nThis approach will give you a more accurate estimation of the average time a TCP request waits in the surge queue of an AWS Classic Load Balancer.\nCould you explain the theory behind this formula? §\nCertainly! The formula I provided earlier is based on the Little’s Law, which is a principle used in queuing theory. Little’s Law states that the average number of items in a queuing system (L) is equal to the average arrival rate of items (λ) multiplied by the average time an item spends in the system (W):\nL = λ * W\n\nIn the context of an AWS Classic Load Balancer (CLB), we can apply Little’s Law to estimate the average time a TCP request waits in the surge queue:\n\nL: Total length of the surge queue (TotalSurgeQueueLength)\nλ: Arrival rate of TCP requests (RequestCount)\nW: Average time a TCP request waits in the surge queue (AverageTimeInSurgeQueue)\n\nBy rearranging Little’s Law, we can calculate the average waiting time (W):\nAverageTimeInSurgeQueue (W) = TotalSurgeQueueLength (L) / TotalRequestCount (λ)\n\nTo obtain TotalSurgeQueueLength, we need to consider the AverageSurgeQueueLength and SampleCount from CloudWatch metrics, as discussed earlier:\nTotalSurgeQueueLength = AverageSurgeQueueLength * SampleCount\n\nHere, AverageSurgeQueueLength represents the average number of requests in the surge queue across all load balancer nodes during a specific time period, and SampleCount is the number of data points used for computing the average.\nBy using the modified Little’s Law formula, we can estimate the average time a TCP request waits in the surge queue of an AWS CLB, taking into account the queuing behavior and request arrival rate. However, keep in mind that this is still an approximation and may not precisely represent the actual waiting time for every request."},"notes/AI/LLM-Models":{"title":"LLM Models","links":[],"tags":[],"content":"Commercial Models §\n\nGPT-4\nGPT-3.5\n\nOpen Source Models §\n\nDolly\nStableLM\n"},"notes/AI/New-Bing":{"title":"New Bing","links":[],"tags":[],"content":"Based on GPT-4, New Bing is free to use but has a limit of 5 turns per session for unregistered users.\nNew Bing can be accessed from the Edge browser or the Bing app on mobile."},"notes/AI/Poe":{"title":"Poe","links":["notes/AI/ChatGPT"],"tags":[],"content":"poe.com\nPoe is a platform that enables users to access third-party chatbots, including ChatGPT.\nPoe can give suggestions on further questions the user could ask, and highlights terms that could be explained further by clicking on them.\nWithout a subscription, message speed and quality may be reduced."},"notes/CLI/Bash/Here-Document":{"title":"Here Document","links":[],"tags":[],"content":"Parameter expansion §\nQuote the word in here-documents to inhibit any expansion.\n&lt;&lt;&#039;word&#039;\n\n\nIf any part of word is quoted, the delimiter is the result of quote removal on word, and the lines in the here-document are not expanded.\tIf word is unquoted, all lines of the here-document are subjected to parameter expansion, command substitution, and arithmetic expansion, the character sequence \\&lt;newline&gt; is ignored, and \\ must be used to quote the characters \\, $, and `.\n"},"notes/CLI/Bash/Line-Editing":{"title":"Line Editing","links":[],"tags":[],"content":"Read man 3 readline for more information."},"notes/CLI/Convenient-CLI-Tools":{"title":"Convenient CLI Tools","links":["notes/CLI/namei"],"tags":[],"content":"\ndog (DNS resolution)\nxhs, xh (HTTP/S request)\nfd (find)\nrg (grep)\npv (pipe viewer)\ndug (DNS propagation checker)\nffsend (share files securely)\nage (simple file encryption tool)\nrestic (file backup tool)\nrclone (rsync for cloud)\nnamei (check parent directory permissions)\nncdu (TUI disk usage analyzer)\nlft (Layer Four Traceroute)\n"},"notes/CLI/Vim/Removing-Auto-Inserted-Comment-Leader":{"title":"Removing Auto-Inserted Comment Leader","links":[],"tags":[],"content":"After pressing Enter or ‘o’, use CTRL-U to remove the comment leader. See :help fo-table.\no       Automatically insert the current comment leader after hitting &#039;o&#039; or\n        &#039;O&#039; in Normal mode.  In case comment is unwanted in a specific place\n        use CTRL-U to quickly delete it. i_CTRL-U\n"},"notes/CLI/Zsh/Word-Splitting":{"title":"Word Splitting","links":[],"tags":[],"content":"Use $=VAR instead of $VAR to force parameter expansions to be split into separate words before substitution, using IFS as a delimiter.\nThis is done by default in most other shells."},"notes/CLI/curl":{"title":"curl","links":[],"tags":[],"content":"—connect-to §\nSyntax: --connect-to &lt;HOST1:PORT1:HOST2:PORT2&gt;\n\n“HOST1” and “PORT1” may be the empty string, meaning “any host/port”. “HOST2” and “PORT2” may also be the empty string, meaning “use the request’s original host/port”.\nAdded in curl 7.49.0\nExample: curl --connect-to ::&lt;IP&gt; &lt;URL&gt;\n\nCompared to --resolve, it can adapt to the request’s original port."},"notes/CLI/ffmpeg":{"title":"ffmpeg","links":[],"tags":[],"content":"One-Liners §\nDetect encoding errors §\nffmpeg -err_detect explode -i &lt;infile&gt; -f null -\nConvert music file to Opus format §\nfind . -type f -name &#039;*.flac&#039; | parallel &#039;test -f {.}.opus || ffmpeg -i {} -vn -c:a libopus -b:a 192k {.}.opus&#039;\nKeep Display Aspect Ratio when Downsampling §\nffmpeg -i in.mp4 -vf &quot;setsar=&#039;if(sar,sar,1)&#039;,scale=640x480&quot; out.mp4"},"notes/CLI/git/git-status":{"title":"git-status","links":[],"tags":[],"content":"git status taking too long §\nMay affect interactivity of shell prompts (e.g. starship) that check git status.\nTo reduce prompt latency, disable checking untracked files in big repositories.\ngit config status.showUntrackedFiles no"},"notes/CLI/journalctl":{"title":"journalctl","links":[],"tags":[],"content":"Useful Options §\n-b (show logs from the current boot only)\n-e (jump to end in pager mode)\n-x (augment log lines with catalog info, don&#039;t use for bug reports)\n"},"notes/CLI/less":{"title":"less","links":[],"tags":[],"content":"Do not clear the screen on exit §\n       -X or --no-init\n\t      Disables sending the termcap initialization and deinitialization\n\t      strings to the terminal.\tThis is  sometimes  desirable  if  the\n\t      deinitialization\tstring does something unnecessary, like clear-\n\t      ing the screen.\n"},"notes/CLI/lsof":{"title":"lsof","links":["notes/CLI/procstat"],"tags":[],"content":"List open files and sockets. On FreeBSD, use procstat instead."},"notes/CLI/namei":{"title":"namei","links":[],"tags":[],"content":"namei -om displays permissions follows each pathname and prints their modes and owners."},"notes/CLI/nstat":{"title":"nstat","links":[],"tags":[],"content":"nstat is part of the iproute2 collection, replacing the old netstat --statistics command."},"notes/CLI/procstat":{"title":"procstat","links":["notes/Operating-System/FreeBSD"],"tags":[],"content":"On FreeBSD, use procstat -f &lt;PID&gt; to list open files and sockets."},"notes/CLI/systemd-analyze":{"title":"systemd-analyze","links":[],"tags":[],"content":"Subcommands §\n\ntime: analyze startup time\nset-log-level: set systemd log level\n"},"notes/CLI/zfs":{"title":"zfs","links":[],"tags":[],"content":"Snapshots §\n# Create snapshot\nzfs snap &lt;volume&gt;@&lt;snapshot_name&gt;\n# Delete snapshot (dry-run)\nzfs destroy -vn &lt;volume&gt;@&lt;snapshot_name&gt;\n# List snapshots\nzfs list -t snapshot"},"notes/Cloud/AWS/Aurora/Aurora-MySQL":{"title":"Aurora MySQL","links":[],"tags":[],"content":"JDBC Drivers §\nMariaDB Connector/J §\n#failover-and-load-balancing-modes\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeDescriptionauroraThis mode supports connection failover in an Amazon Aurora cluster. This mode does support load-balancing reads on slave instances if the connection is set to read-only before executing the read. The connector performs load-balancing by randomly picking a slave instance to execute read queries for a connection.This mode has been available since MariaDB Connector/J 1.2.0\nCaveats and Gotchas §\naurora-mysql-db-cluser-read-only-error\nSmart driver §\nThe Amazon Aurora DB cluster endpoints propagate DNS record updates automatically, but the process doesn’t happen instantly. This can cause delays in responding to an event that occurred on the database and the event might be handled by the application. A Smart Driver uses the DB cluster topography through the INFORMATION_SCHEMA.REPLICA_HOST_STATUS metadata table, which is in near-real-time. This helps to route connections to the appropriate role, and helps load-balance across the existing replicas. MariaDB Connector/J is an example of a third party Smart Driver that has native support for Aurora MySQL.\nNote: Even Smart Drivers might be affected by excessive DNS Caching.\nDNS caching §\nIf you are not using a smart driver, then you depend on the DNS record updates and propagation after a failover event occurs. Aurora DNS zones use a short time-to-live (TTL) of 5 seconds, so it is important that your network and client configurations don’t further increase this. DNS caching can occur at multiple layers of an architecture, such as the operating system (OS), the network layer and the application container. It is important that you understand how each of these layers is configured. If there is unintended DNS caching beyond the TTL of 5 seconds, it is possible that you will re-connect to the old writer after a failover.\nJava Virtual Machines (JVM) can excessively cache DNS, indefinitely. When the JVM resolves a hostname to an IP address, it caches the IP address for a specified period of time (TTL). On some configurations, the JVM default TTL is set to never refresh DNS entries until the JVM is restarted. This can lead to read-only errors after a failover. In this case, it is important to manually set a small TTL so that it will periodically refresh."},"notes/Cloud/AWS/Availability-Zones":{"title":"Availability Zones","links":[],"tags":[],"content":"Caveats and Gotchas §\nMap Availability Zones across accounts §\nAvailability Zone names don’t map to the same location across accounts. However, you can use AZ IDs to map Availability Zones across accounts:\n\nOpen the AWS Resource Access Manager console.\nIn the navigation bar, select your Region from the Region Selector.\nIn the Your AZ ID pane on the right, review the list of Availability Zone names and their corresponding AZ IDs.\nNote: You can also use the aws ec2 describe-availability-zones --region &quot;region-name&quot; command in the AWS CLI to generate AZ ID information. Be sure to replace &quot;region-name&quot; with the name of your AWS Region.\nIdentify which Availability Zones have the same AZ IDs across your accounts. Availability Zones that have the same AZ IDs map to the same physical location.\n"},"notes/Cloud/AWS/CDK":{"title":"CDK","links":[],"tags":[],"content":"Caveats and Gotchas §\nMake sure that construct IDs are reproducible. It is recommended to run cdk synth -q twice and compare the generated files."},"notes/Cloud/AWS/Cloud9":{"title":"Cloud9","links":[],"tags":[],"content":"Caveats and Gotchas §\nAWS managed temporary credentials §\nSome Cloud9, IAM and STS actions are restricted. See Actions supported by AWS managed temporary credentials.\n\n\nCurrently, if your environment’s EC2 instance is launched into a private subnet, you can’t use AWS managed temporary credentials to allow the EC2 environment to access an AWS service on behalf of an AWS entity (an IAM user, for example).\n\nsecurity-iam.html#auth-and-access-control-temporary-managed-credentials"},"notes/Cloud/AWS/CloudTrail":{"title":"CloudTrail","links":[],"tags":[],"content":"Usage §\nCertain types of CloudTrail events do not associate with related resources. It is recommended to search events with event names, which can be looked up in the Actions page of service API references.\nCaveats and Gotchas §\n\nEvents may take several minutes to show up in CloudTrail.\n"},"notes/Cloud/AWS/ECS":{"title":"ECS","links":[],"tags":[],"content":"Caveats and Gotchas §\n\nThe awsvpc network mode doesn’t provide task ENIs with public IP addresses. Therefore, they can’t make direct use of an internet gateway [and therefore can’t connect to the internet].\n\nnetworking-outbound.html"},"notes/Cloud/AWS/ELB":{"title":"ELB","links":[],"tags":[],"content":"Elastic Load Balancing §\nCaveats and Gotchas §\nSecurity groups §\nsecurity-group-load-balancer\nIf you’re using an Application Load Balancer, follow the instructions at Security groups for your Application Load Balancer.\nIf you’re using a Network Load Balancer, update the security groups for your target instances because Network Load Balancers don’t have associated security groups.\n\nIf your target type is an IP, add a rule to your security group to allow traffic from your load balancer’s IP address to the target IP address.\nIf your target type is an instance, add a rule to your security group to allow traffic from your load balancer’s IP address and clients to the target IP address.\n"},"notes/Cloud/AWS/ELB/CLB-CloudWatch-metrics":{"title":"CLB CloudWatch metrics","links":[],"tags":[],"content":"TCP Listener §\nSampleCount §\n\nSampleCount of RequestCount and Latency metrics are exactly the same as number of requests.\nSampleCount of SurgeQueueLength is roughly number of requests doubled.\nSampleCount of EstimatedProcessedBytes and EstimatedALB* metrics is number of load balancer nodes at 1 minute periods.\nSampleCount of HealthyHostCount and UnHealthyHostCount is a multiple of number of load balancer nodes at 1 minute periods. The multiplier could be one of 18, 60, 66, or some other integer.\n\nConnection Rate §\nWhen TCP request rate is stable and no requests fail, sum of EstimatedALBNewConnectionCount should be twice as big as sum of RequestCount, because connections established with both clients and targets are counted towards EstimatedALBNewConnectionCount.\nAverage Waiting Time in Surge Queue §\n\n\n                  \n                  Warning \n                  \n                \nThis is a thought experiment. It relies on an assumption that is not true.\n\nThe 2 times relationship between SurgeQueueLength samples and number of requests implies that each request triggers 2 samples.\nGiven that the maximum SurgeQueueLength we have observed is the per node limit of 1024, we can assume that each sample only takes the surge queue length of that specific node.\nTherefore, the total SurgeQueueLength of the load balancer should be Avg(SurgeQueueLength) multiplied by the number of nodes, assuming that each node handles the same amount of requests in each period.\nAvg(Latency)s​=Sum(RequestCount)/PeriodsAvg(SurgeQueueLength)∗Number of Nodes​=Sum(RequestCount)/Periods∗PeriodminAvg(SurgeQueueLength)∗SampleCount(EstimatedProcessedBytes)​=60∗Sum(RequestCount)Avg(SurgeQueueLength)∗SampleCount(EstimatedProcessedBytes)​​\nFrom the CloudWatch data we have analyzed, we can assume that ratio of nodes across AZs is the same as ratio of HealthyHostCount in each AZ.\nTherefore, to calculate the average latency per AZ, we could use per-AZ metrics and SampleCount of EstimatedProcessedBytes (which is not split by AZ) multiplied by SampleCount(HealthyHostCount)SampleCount(HealthyHostCounta​)​.\nThe resulting formula is\nAvg(Latencya​)s​=60∗Sum(RequestCounta​)Avg(SurgeQueueLengtha​)∗SampleCount(EstimatedProcessedBytes)​=60∗Sum(RequestCounta​)∗SampleCount(HealthyHostCount)Avg(SurgeQueueLengtha​)∗SampleCount(EstimatedProcessedBytes)∗SampleCount(HealthyHostCounta​)​​\n, with the assumption that requests are evenly distributed across nodes.\nBack to the assumption, from percentile statistics of EstimatedALBNewConnectionCount we can see that the distribution of connections across nodes is not even nor stable. To get the total SurgeQueueLength, each node’s samples must be normalized to even weight, which is not possible from summaries."},"notes/Cloud/AWS/ELB/NLB":{"title":"NLB","links":[],"tags":[],"content":"Caveats and Gotchas §\n\nYou cannot change the health check interval for a target group with the TCP protocol.\n\n\nQuoting AWS Support:\n\nWhen you register a new target to your Network Load Balancer, it is expected to take between 90 and 180 seconds to complete the registration process. After registration is complete, the Network Load Balancer health check systems will begin to send health checks to the target. A newly registered target must pass health checks for the configured interval to enter service and receive traffic. For example, if you configure your health check for a 30 second interval, and require 3 health checks to become healthy, the minimum time a newly registered target could enter service is 90 seconds after a new target passes its first health check.\nSimilarly, when you deregister a target from your Network Load Balancer, it is expected to take 90-180 seconds to process the requested deregistration, after which it will no longer receive new connections. During this time the Elastic Load Balancing API will report the target in ‘draining’ state. The target will continue to receive new connections until the deregistration processing has completed. At the end of the configured deregistration delay, the target will not be included in the describe-target-health response for the Target Group, and will return ‘unused’ with reason ‘Target.NotRegistered’ when querying for the specific target.\n\n67203212 (CC BY-SA 4.0)\n\n\nThere is a limit on the number of ALBs, CLBs and NLBs per region (separately). As of late 2017, the default limit for each is 20 per region. These limits can be easily raised for ALB and CLB, but AWS is quite reluctant to raise the limit on NLBs.\n\nog-aws#load-balancer-gotchas-and-limitations (CC BY-SA 4.0)"},"notes/Cloud/AWS/EMR":{"title":"EMR","links":["notes/Cloud/AWS/Availability-Zones"],"tags":[],"content":"Availability Zones §\nAmazon EMR launches all nodes for a given cluster in the same Amazon EC2 Availability Zones. Running a cluster in the same zone improves performance of the jobs flows."},"notes/Cloud/AWS/Graviton-Processor":{"title":"Graviton Processor","links":[],"tags":[],"content":"AWS Graviton Technical Guide\nProcessor Specifications §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProcessorGraviton2Graviton3(E)InstancesM6gd, C6gn, R6gd, T4g, X2gd, G5g, and Is4genC7gn, M7g, R7g, and HPC7gCoreNeoverse-N1Neoverse-V1Frequency2500MHz2600MHzTurbo supportedNoNoInstruction latenciesInstruction LatenciesInstruction LatenciesInterconnectCMN-600CMN-700Architecture revisionARMv8.2-aARMv8.4-aAdditional  featuresfp16, rcpc, dotprod, cryptosve, rng, bf16, int8, cryptoRecommended -mcpu flagneoverse-n1neoverse-512tvbRNG InstructionsNoYesSIMD instructions2x Neon 128bit vectors4x Neon 128bit vectors / 2x SVE 256bitLSE (atomic mem operations)yesyesPointer AuthenticationnoyesCores6464L1 cache (per core)64KB inst / 64KB data64KB inst / 64KB dataL2 cache (per core)1MB1MBLLC (shared)32MB32MBDRAM8x DDR48x DDR5DDR Encryptionyesyes\nC/C++ on Graviton §\nSource\nOptimal processor features §\nOn arm64 -mcpu= acts as both specifying the appropriate\narchitecture and tuning and it’s generally better to use that vs -march if\nyou’re building for a specific CPU.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPUFlagGCC versionLLVM verisonGraviton2-mcpu=neoverse-n1*GCC-9^Clang/LLVM 10+Graviton3(E)-mcpu=neoverse-512tvb%GCC 11+Clang/LLVM 14+\n^ Also present in Amazon Linux2 GCC-7\n* Requires GCC-9 or later; otherwise we suggest using -mcpu=cortex-a72\n% If your compiler doesn’t support neoverse-512tvb, please use the Graviton2 tuning.\nUsing SVE §\nThe scalable vector extensions (SVE) require both a new enough tool-chain to\nauto-vectorize to SVE (GCC 11+, LLVM 14+) and a 4.15+ kernel that supports SVE.\nOne notable exception is that Amazon Linux 2 with a 4.14 kernel doesn’t support SVE;\nplease upgrade to a 5.4+ AL2 kernel."},"notes/Cloud/AWS/IAM":{"title":"IAM","links":[],"tags":[],"content":"Caveats and Gotchas §\niam-increase-policy-size\nManaged policy limit §\nYou can assign IAM users to up to 10 groups. You can also attach up to 10 managed policies to each group, for a maximum of 120 policies (20 managed policies attached to the IAM user, 10 IAM groups, with 10 policies each).\nInline policy character quota §\nYou can add as many inline policies as you want to an IAM user, role, or group. But the total aggregate policy size (the sum size of all inline policies) per entity cannot exceed the following quotas:\n\nUser policy size cannot exceed 2,048 characters.\nRole policy size cannot exceed 10,240 characters.\nGroup policy size cannot exceed 5,120 characters.\n"},"notes/Cloud/AWS/RDS/RDS-Proxy":{"title":"RDS Proxy","links":[],"tags":[],"content":"Caveats and Gotchas §\n\nYou can’t use RDS Proxy with custom DNS.\n"},"notes/Cloud/AWS/Red-Hat-Knowledge-Base-Access":{"title":"Red Hat Knowledge Base Access","links":[],"tags":[],"content":"Running an on-demand instance of Red Hat Enterprise Linux (RHEL) for at least 24 to 48 hours to get access to the Red Hat knowledge base v.a home."},"notes/Cloud/AWS/S3":{"title":"S3","links":[],"tags":[],"content":"Caveats and Gotchas §\nPartitioning §\ns3-prefix-nested-folders-difference\nA prefix is the complete path in front of the object name, which includes the bucket name. For example, if an object (123.txt) is stored as BucketName/Project/WordFiles/123.txt, the prefix is “BucketName/Project/WordFiles/”. If the 123.txt file is saved in a bucket without a specified path, the prefix value is “BucketName/“.\nA partitioned prefix in a bucket can support 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second. There is no limit to the number of prefixes you can have in a bucket.\nNote: In Amazon S3, there are no partitions for keys or objects. Partitions exist only at the prefix level, and not at the object level. For more information about using prefixes in Amazon S3, see Organizing objects using prefixes."},"notes/Cloud/AWS/VPC/Interface-Endpoint":{"title":"Interface Endpoint","links":[],"tags":[],"content":"Private DNS for interface endpoints §\nIf the private DNS option is enabled, an interface endpoint is created with a hidden private hosted zone attached to the associated VPC. Records in this hosted zone point the default service domain to ENIs of the interface endpoint, which have private IPs that isolated subnets can connect to.\nCaveats and Gotchas §\nDetach of a private hosted zone can take a few minutes, so it is not possible to recover a deleted AWS service interface endpoint immediately.\nAPI error message:\nprivate-dns-enabled cannot be set because there is already a conflicting DNS domain for ssm.&lt;region&gt;.amazonaws.com in the VPC\n"},"notes/Cloud/AWS/VPC/NAT-Gateway":{"title":"NAT Gateway","links":[],"tags":[],"content":"Caveats and Gotchas §\n\nIf a connection that’s using a NAT gateway is idle for 350 seconds or more, the connection times out.\n"},"notes/Economics/Economics-in-China":{"title":"Economics in China","links":[],"tags":[],"content":"2010 §\nExcerpt translated from kkndme_tianya.\nEconomics §\n\nHigh inflation and continuous depreciation of currency are determined by the state-owned economic system, which cannot be changed.\nThe state-owned economic system determines that\n\nGovernment administrative costs are high.\nEconomics is overly reliant on government investment.\nCost of corruption is astonishingly high.\nExport of low value-added products generates foreign exchange earnings.\n\n\n\nDriven by vested interests, the development of society is inevitably shaped by those who hold power. Even if policies go against the interests of the ruling class, they will likely be difficult to enforce, with few consequences for those who fail to comply. In the end, such policies often end up as empty promises that are never fully realized.\n\n“Land Finance” §\n\nValue of real estate in China is composed of the house itself and the right to use the land it is built on.\nIndividuals or organizations only have the right to own properties and use the land through ownership and usage rights.\n\n\nProperty Price §\n\nHousing prices are not determined by the average income as reported by the National Bureau of Statistics, but rather by the average income of the elite, because:\n\nThe pyramid structure of China society implies that even if only 10% of the population resides at the top, it still constitutes an enormous number of over 100 million people, much higher than the sum of most Western countries.\nOnly the relationship between supply and demand determines the price of a commodity.\nCommodity housing in the center of a first-or-second-tier city is sufficiently scarce for the elite population.\n\n\n\n\nProperty Rights §\n\nProperty rights include the right to use (live in) the property and ownership.\n"},"notes/Emacs/Frame":{"title":"Frame","links":[],"tags":[],"content":"Fullscreen by default §\n(add-to-list &#039;default-frame-alist &#039;(fullscreen . maximized))"},"notes/Emacs/doom/Consult":{"title":"Consult","links":[],"tags":[],"content":"Asynchronous Search §\nconsult#asynchronous-search\nDoom Emacs uses the perl splitting style, which splits the input string at a punctuation character and treats each as an Emacs regular expression, so special characters like $ has to be escaped.\nTo avoid such splitting, add a backslash (\\) before spaces."},"notes/Emacs/doom/Dhall":{"title":"Dhall","links":[],"tags":[],"content":"Run dhall-freeze as early as possible, because dhall-mode re-evaluates the buffer on each change."},"notes/Emacs/doom/Doom-CLI":{"title":"Doom CLI","links":[],"tags":[],"content":"Commands §\nWhen you need to force a full byte-compile, for example after upgrading Emacs:\ndoom build\ndoom sync\nWhen your need to reclaim some disk space occupied by .emacs.d:\ndoom purge"},"notes/Emacs/doom/Embark":{"title":"Embark","links":[],"tags":[],"content":"Start with C-;.\nSource §\nembark"},"notes/Emacs/doom/Evil-Collection":{"title":"Evil Collection","links":[],"tags":[],"content":"For additional evil-collection-MODULE:\n(after! mpdel\n  (add-transient-hook! &#039;mpdel-mode\n    (+evil-collection-init &#039;mpdel)))"},"notes/Emacs/doom/Evil":{"title":"Evil","links":[],"tags":[],"content":"yank forward §\ny f SPC does not yank the space character with evil-snipe-override-mode. See 86.\n(remove-hook &#039;doom-first-input-hook #&#039;evil-snipe-override-mode)\nCommands §\nRepeat last change: .\nRepeat last substitute: &amp;\nEx-commands §\nClose buffer: :bd"},"notes/Emacs/doom/Font":{"title":"Font","links":[],"tags":[],"content":"(setq doom-font (font-spec :family &quot;Cascadia Mono PL&quot; :size 12 :weight &#039;semi-light))"},"notes/Emacs/doom/How-to-Bump-a-Package":{"title":"How to Bump a Package","links":[],"tags":[],"content":"How to contribute §\n\nMake the commit hash changes.\nEnter the magit buffer with SPC g g.\nStage the changes.\nSPC : and run doom/bumpify-diff.\nc c and paste the commit message with p.\nTest updates with ~/.emacs.d/bin/doom upgrade -p.\n"},"notes/Emacs/doom/Init":{"title":"Init","links":[],"tags":[],"content":"Enabled Modules §\n:ui\nindent-guides\n\n:emacs\nibuffer\n\n:term\nvterm\n\n:checkers\n(spell +flyspell)\n\n:tools\n(lookup +docsets +dictionary)\nlsp\nmagit\n"},"notes/Emacs/doom/Jumper":{"title":"Jumper","links":[],"tags":[],"content":"Natural jumper §\n;; Add to config.el\n(after! better-jumper\n  (setq better-jumper-add-jump-behavior &#039;replace))"},"notes/Emacs/doom/Leader-Key":{"title":"Leader Key","links":[],"tags":[],"content":"Define new key bindings with leader key prefix §\n(map! :leader\n      (:prefix (&quot;o&quot; . &quot;open&quot;)\n       :desc &quot;elfeed&quot; &quot;e&quot; #&#039;=rss))\nRemap leader keys §\nChange alt key bindings:\n;; Add to config.el\n(setq doom-leader-alt-key &quot;C-SPC&quot;\n      doom-localleader-alt-key &quot;C-SPC m&quot;)"},"notes/Emacs/doom/Lookup":{"title":"Lookup","links":[],"tags":[],"content":"Open URLs §\ng f (+lookup/file) works for URLs too.\nDocsets §\nDash docsets need to be associated with major modes. Most :lang modules have this already.\n(set-docsets! &#039;tcl-mode &quot;Tcl&quot;)"},"notes/Emacs/doom/Magit":{"title":"Magit","links":[],"tags":[],"content":"Navigation §\nmagit-section-backward-sibling ([) and magit-section-forward-sibling (]).\nLine-wrap §\n$ is mapped to magit-process-buffer, so it’s hard to jump to end of line.\nUse SPC t w to enable line wrapping.\nCommit message §\nFinish or cancel editing §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActionKeyFinishC-c C-c, ZZCancelC-c C-k, ZQ\nCommit message ring §\nBoth finished and cancelled messages are saved to a ring that persists until Emacs is closed. Use M-p and M-n to navigate older messages in any edit session."},"notes/Emacs/doom/Mpdel":{"title":"Mpdel","links":[],"tags":[],"content":"(use-package! mpdel\n  :config\n  (+evil-collection-init &#039;mpdel)\n  (map! :leader\n        :desc &quot;MPDel&quot; &quot;Z&quot; mpdel-core-map))"},"notes/Emacs/doom/Nativecomp":{"title":"Nativecomp","links":[],"tags":[],"content":"Enabled automatically if you have Emacs built with --with-native-compilation.\nCaveats §\ndoom build may stuck on native compile if the vterm module is enabled, see 5592.\nThis problem can be avoided by compiling vterm-module in advance."},"notes/Emacs/doom/Org-roam-v2":{"title":"Org-roam v2","links":[],"tags":[],"content":"Config §\n;; in config.el\n(setq org-roam-directory &quot;~/org-roam/&quot;)\n\n(after! org-roam\n  (setq +org-roam-auto-backlinks-buffer t))\norg-roam-protocol §\nFollow the Org-roam user manual to create an AppleScript application. Then save the following as a bookmarklet in Firefox:\njavascript:location.href =\n    &#039;org-protocol://roam-ref?template=r&amp;ref=&#039;\n    + encodeURIComponent(location.href)\n    + &#039;&amp;title=&#039;\n    + encodeURIComponent(document.title)"},"notes/Emacs/doom/ParEdit":{"title":"ParEdit","links":[],"tags":[],"content":"Why doesn’t Doom package ParEdit? §\nFrom Doom’s FAQs\n\nWhy are there no default keybinds for Smartparens (for evil users)?\nDoom only uses smartparens to manage pair “completion” (it does the job better than electric-{pair,quote}-mode or the multitude of other pair-management solutions in the Emacs ecosystem at the time of writing).\nNone of smartparen’s commands have default keybinds for evil users because they are redundant with motions and text-objects provided by evil/vim. If you disagree, I recommend trying the :editor lispy or :editor parinfer modules.\n\nBut if you insist on using ParEdit in Doom, read on.\nInstall §\nAdd following code to packages.el or enable clojure in :lang of init.el.\n(package! paredit :pin &quot;...&quot;)\nConfiguration §\nAdd following code to config.el and restart Emacs.\n(autoload &#039;enable-paredit-mode &quot;paredit&quot; &quot;Turn on pseudo-structural editing of Lisp code.&quot; t)\n(add-hook! &#039;(emacs-lisp-mode-hook clojure-mode-hook) #&#039;enable-paredit-mode)\nKey bindings §\nparedit-{backward,forward}-slurp-sexp (C-( and C-)) pulls new elements in from either side and paredit-{backward,forward}-barf-sexp (C-{ and C-}) pushes elements away."},"notes/Emacs/doom/Project":{"title":"Project","links":["notes/Emacs/doom/wgrep"],"tags":[],"content":"Project-wide actions §\n\nFind file: SPC p f\n\nInvalidate cache: SPC p i\n\n\nSave all: SPC p s\nProject-wide search and replace\n"},"notes/Emacs/doom/Rime":{"title":"Rime","links":[],"tags":[],"content":"Setup §\n;; Add to packages.el\n(package! pyim :disable t)\n(package! rime)\n;; Add to config.el\n(use-package! rime\n  :after-call after-find-file pre-command-hook\n  :init\n  (setq rime-emacs-module-header-root &quot;/Applications/MacPorts/Emacs.app/Contents/Resources/include&quot;\n        rime-librime-root &quot;/opt/local&quot;)\n  :config\n  (setq rime-show-candidate &#039;popup\n        rime-disable-predicates\n        &#039;(rime-predicate-evil-mode-p\n          rime-predicate-prog-in-code-p)\n        default-input-method &quot;rime&quot;))"},"notes/Emacs/doom/Straight":{"title":"Straight","links":[],"tags":[],"content":"\nOut of the box, you can install any package listed on MELPA, GNU ELPA, or Emacsmirror, which is to say any package in existence. (Although MELPA is used as a package listing, packages are installed by cloning their Git repositories rather than by downloading tarballs like package.el does.)\n\nMost of the time you don’t need to specify a :recipe."},"notes/Emacs/doom/Undo":{"title":"Undo","links":[],"tags":[],"content":"Disable undo history persistence §\n;; Add in config.el\n(remove-hook &#039;undo-fu-mode-hook #&#039;global-undo-fu-session-mode)"},"notes/Emacs/doom/Vertico":{"title":"Vertico","links":["notes/Emacs/doom/Consult","notes/Emacs/doom/Embark"],"tags":[],"content":"Key bindings in minibuffer §\nPreview: C-SPC for +vertico/embark-preview\nNext &amp; previous history: M-p and M-n\nPage up &amp; down: C-S-j and C-S-k\nNext &amp; previous group: C-M-j and C-M-k\nNext &amp; previous row: C-j and C-k\nExit with input: M-RET (vertico-exit-input) or RET (exit-minibuffer)\nSearch pattern §\nConsult\nSearch all key bindings §\nUsually SPC h b b, but use C-h b b or &lt;doom-leader-alt-key&gt; instead of SPC when leader key (SPC) is not available, for example in a vertico minibuffer:\nscroll-up-command             &lt;kp-next&gt;, C-S-j\nvertico-scroll-up             &lt;remap&gt; &lt;scroll-up-command&gt;\n\nCopy candidate list §\nC-; E in minibuffer to export candidates to a new buffer with Embark.\ncompleting-read-multiple (crm) §\nPowers multi-select feature in magit.\n\n+vertico/crm-select: TAB\n+vertico/crm-exit: RET\n+vertico/crm-select-keep-input: S-TAB\n"},"notes/Emacs/doom/Window":{"title":"Window","links":[],"tags":[],"content":"SPC w o (doom/window-enlargen)\nSPC w C-o (delete-other-windows)\nSPC w q (evil-quit = Z Q)"},"notes/Emacs/doom/Workspace":{"title":"Workspace","links":[],"tags":[],"content":"Key bingings §\nQuick switch: s-{0..9}"},"notes/Emacs/doom/cc":{"title":"cc","links":[],"tags":[],"content":"LSP (preferred) §\n;; in init.el\n:lang\n(cc +lsp)\n\nIrony-Mode §\nIt does not work out of the box on macOS, but it is possible to put additional header flags to ~/.clang_complete, as long as no intermediate parent directory contains .clang_complete or compile_flags.txt.\n(defun irony-cdb-clang-complete--locate-db ()\n  (when buffer-file-name\n    (catch &#039;fname\n      (locate-dominating-file\necho | clang -x c++ -v -E - 2&gt;&amp;1 | sed -n &#039;/^#include &lt;/,/^End/s|^[^/]*\\([^ ]*/include[^ ]*\\).*$|-I\\1|p&#039; &gt; ~/.clang_complete\nrtags §\nrtags also does not work out of the box on macOS, requiring users to configure ~/.rdmrc.\n--isystem=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1\n--isystem=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/12.0.0/include\n&lt;...&gt;\n"},"notes/Emacs/doom/chinese":{"title":"chinese","links":["notes/Emacs/doom/Rime"],"tags":[],"content":"How to Enable §\n;; Uncomment in init.el\n(doom! :input\n       chinese\n\n       :ui\n       unicode\n       ...)\nInput Methods §\npyim §\nEnabled by default in chinese module.\nemacs-rime §\nRime"},"notes/Emacs/doom/wgrep":{"title":"wgrep","links":[],"tags":[],"content":"SPC s p foo C-; E i :%s/././ RET Z Z\n\nSPC-s-p foo invokes project-wide search\nC-; E exports the results into a wgrep buffer\ni &lt;...buffer edits&gt; RET Z Z makes the buffer editable and saves the changes.\n\ndoom-emacs-search-replace-project"},"notes/Emacs/emacsclient":{"title":"emacsclient","links":[],"tags":[],"content":"Emacs Server on macOS §\n&lt;!-- ~/Library/LaunchAgents/gnu.emacs.daemon.plist --&gt;\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; &quot;http://www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt;\n&lt;plist version=&quot;1.0&quot;&gt;\n&lt;dict&gt;\n    &lt;key&gt;Label&lt;/key&gt;\n    &lt;string&gt;gnu.emacs.daemon&lt;/string&gt;\n \n    &lt;key&gt;ProgramArguments&lt;/key&gt;\n    &lt;array&gt;\n        &lt;string&gt;/Applications/MacPorts/Emacs.app/Contents/MacOS/Emacs&lt;/string&gt;\n        &lt;string&gt;--fg-daemon&lt;/string&gt;\n    &lt;/array&gt;\n \n    &lt;key&gt;RunAtLoad&lt;/key&gt;\n    &lt;true/&gt;\n \n    &lt;key&gt;ProcessType&lt;/key&gt;\n    &lt;string&gt;Interactive&lt;/string&gt;\n&lt;/dict&gt;\n&lt;/plist&gt;\nEmacs Client with AppleScript §\nSave the following in Script Editor.app as an Application.\ndo shell script &quot;/Applications/MacPorts/Emacs.app/Contents/MacOS/bin/emacsclient -c -n&quot;\n\n⚠ Warning: Without -n the applet won’t quit and prevents sleep.\n"},"notes/Emacs/mu4e":{"title":"mu4e","links":[],"tags":[],"content":"M-&lt;up&gt;: move to previous message\nM-&lt;down&gt;: move to next message"},"notes/Emacs/mu4e/Prefer-Plain-Text":{"title":"Prefer Plain Text","links":[],"tags":[],"content":"\nBy default, mu4e tries to display the ’richest’ option, which is the last MIME-part of the alternatives. You can customize this to prefer the text version, if available, with something like the following in your configuration (and see the docstring for mm-discouraged-alternatives for details)\n\n(with-eval-after-load &quot;mm-decode&quot;\n  (add-to-list &#039;mm-discouraged-alternatives &quot;text/html&quot;)\n  (add-to-list &#039;mm-discouraged-alternatives &quot;text/richtext&quot;))\nMSGV-Rich_002dtext-and-images.html#MSGV-Rich_002dtext-and-images"},"notes/HTTP/CORS-Credentials":{"title":"CORS Credentials","links":[],"tags":[],"content":"\nThe CORS request was attempted with the credentials flag set, but the server is configured using the wildcard (”*”) as the value of Access-Control-Allow-Origin, which doesn’t allow the use of credentials.\n\nCORSNotSupportingCredentials"},"notes/HTTP/CORS-Origin":{"title":"CORS Origin","links":[],"tags":[],"content":"Access-Control-Allow-Origin: &lt;origin&gt;\nOnly a single origin can be specified. If the server supports clients from multiple origins, it must return the origin for the specific client making the request."},"notes/HTTP/HTTP-Request-RTTs":{"title":"HTTP Request RTTs","links":[],"tags":[],"content":"\nPhase 1: DNS Lookup\nPhase 2: TCP Handshake (1 RTT)\nPhase 3: TLS Handshake (1 RTT if TLS 1.3)\n\n2 round trips if TLS version &lt; 1.3\n\n\nPhase 4: HTTP (1 RTT + server-side latency)\n"},"notes/HTTP/Preflight-Request":{"title":"Preflight Request","links":[],"tags":[],"content":"It’s an OPTIONS request, using three HTTP request headers: Access-Control-Request-Method, Access-Control-Request-Headers, and the Origin header.\nFor example:\nOPTIONS /resource/foo\nAccess-Control-Request-Method: DELETE\nAccess-Control-Request-Headers: origin, x-requested-with\nOrigin: https://foo.bar.org"},"notes/Health/20-20-20-Rule":{"title":"20-20-20 Rule","links":[],"tags":[],"content":"Every 20 minutes spent using a screen, you should try to look away at something that is 20 feet away from you for a total of 20 seconds."},"notes/Krita":{"title":"Krita","links":[],"tags":[],"content":"Pen Tablet Shortcuts §\n\nShift + drag left/right to resize the brush.\nButton (close to pen tip) + drag to zoom in and out.\nSPACE + drag to move the canvas.\nShift + SPACE + drag to rotate the canvas.\n"},"notes/MacPorts/Fontconfig":{"title":"Fontconfig","links":[],"tags":[],"content":"Font directory list §\n\t&lt;dir&gt;/usr/share/fonts&lt;/dir&gt;\n\t&lt;dir&gt;/usr/X11/lib/X11/fonts&lt;/dir&gt; &lt;dir&gt;/opt/local/share/fonts&lt;/dir&gt; &lt;dir&gt;/Library/Fonts&lt;/dir&gt; &lt;dir&gt;/Network/Library/Fonts&lt;/dir&gt; &lt;dir&gt;/System/Library/Fonts&lt;/dir&gt;\n\t&lt;dir prefix=&quot;xdg&quot;&gt;fonts&lt;/dir&gt;\n\t&lt;!-- the following element will be removed in the future --&gt;\n\t&lt;dir&gt;~/.fonts&lt;/dir&gt;\n\nShould we add ~/Library/Fonts?"},"notes/MacPorts/Ideas":{"title":"Ideas","links":[],"tags":[],"content":"Improvements §\n\n Fix port bump in ports with multiple checksums (e.g. cargo.crates).\n port command blocks on DNS resolution timeout, and is impossible to interrupt safely.\n\nIt’s blocked on a curl call, which already has a reasonable timeout.\nWhy doesn’t Ctrl+C interrupt the call?\n\n\n port reclaim takes too much time checking distfiles.\n\nCache distfile information (done).\n\n\n[/] Dependency resolution is too slow.\n\nCould not make use of libsolv because we have variants and countless combinations out of them.\n\n\n"},"notes/MacPorts/base/Alternative-Compiler":{"title":"Alternative Compiler","links":[],"tags":[],"content":"Beware of MP_PATH_SCAN.\ndnl This macro ensures MP installation prefix paths are NOT in PATH\ndnl for configure to prevent potential problems when base/ code is updated\ndnl and ports are installed that would match needed items.\nAC_DEFUN([MP_PATH_SCAN],[\n    ...\n])\nUse symboic links as a workaround:\nln -s /opt/local/libexec/llvm-13/libexec/intercept-cc ~/.local/bin/\nintercept-build-mp-13 bash -c &#039;./configure &amp;&amp; make -j6&#039;\n"},"notes/MacPorts/base/Anatomy":{"title":"Anatomy","links":[],"tags":[],"content":"Database §\n/* ports table */\n&quot;CREATE TABLE registry.ports (&quot;\n        &quot;id INTEGER PRIMARY KEY&quot; // row ID\n    &quot;, name TEXT COLLATE NOCASE&quot; // name of the port\n    &quot;, portfile TEXT&quot; // path of Portfile copy\n    &quot;, location TEXT&quot; // image (binary archive) location\n    &quot;, epoch INTEGER&quot; // part of version comparison\n    &quot;, version TEXT COLLATE VERSION&quot; // part of version comparison\n    &quot;, revision INTEGER&quot; // part of version comparison\n    &quot;, variants TEXT&quot; // actual variants used to build port\n    &quot;, requested_variants TEXT&quot; // variants explicitly requested by user\n    &quot;, state TEXT&quot; // installed (activated) or imaged (has image)\n    &quot;, date DATETIME&quot; // install date\n    &quot;, installtype TEXT&quot; // unused, always &quot;image&quot;\n    &quot;, archs TEXT&quot; // list of architectures of this port\n    &quot;, requested INTEGER&quot; // is port requested (boolean)\n    &quot;, os_platform TEXT&quot; // operating system (e.g. darwin)\n    &quot;, os_major INTEGER&quot; // OS version\n    &quot;, cxx_stdlib TEXT&quot; // stdlib used for C++ (&quot;libstdc++&quot; or &quot;libc++&quot;)\n    &quot;, cxx_stdlib_overridden INTEGER&quot; // flag to track broken C++ ports\n    &quot;, UNIQUE (name, epoch, version, revision, variants)&quot;\n    &quot;)&quot;,\n&quot;CREATE INDEX registry.port_name ON ports&quot;\n    &quot;(name, epoch, version, revision, variants)&quot;,\n&quot;CREATE INDEX registry.port_state ON ports(state)&quot;,\n \n/* file map */\n&quot;CREATE TABLE registry.files (&quot;\n        &quot;id INTEGER&quot; // row ID\n    &quot;, path TEXT&quot; // file path\n    &quot;, actual_path TEXT&quot; // actual path installed (FS case-sensitivity)\n    &quot;, active INTEGER&quot; // is file installed\n    &quot;, binary BOOL&quot; // is file a binary (rev-upgrade check)\n    &quot;, FOREIGN KEY(id) REFERENCES ports(id))&quot;,\n&quot;CREATE INDEX registry.file_port ON files(id)&quot;,\n&quot;CREATE INDEX registry.file_path ON files(path)&quot;,\n&quot;CREATE INDEX registry.file_actual ON files(actual_path)&quot;,\n&quot;CREATE INDEX registry.file_actual_nocase ON files(actual_path COLLATE NOCASE)&quot;,\n \n/* dependency map */\n&quot;CREATE TABLE registry.dependencies (&quot;\n        &quot;id INTEGER&quot; // row ID\n    &quot;, name TEXT&quot; // name of the port\n    &quot;, variants TEXT&quot; // variants of the port\n    &quot;, FOREIGN KEY(id) REFERENCES ports(id))&quot;, // the dependency\n&quot;CREATE INDEX registry.dep_id ON dependencies(id)&quot;,\n&quot;CREATE INDEX registry.dep_name ON dependencies(name)&quot;,\nVersioning §\nepoch &gt; version &gt; revision\n\nPrefixes §\nMacPorts defaults to /opt/local, but allows alternatives prefixes.\nPhases §\n\nfetch\n\nFetch the ${distfiles} (e.g. source code).\n\nchecksum\n\nCompare ${checksums} specified in a Portfile to the checksums of the fetched ${distfiles}.\n\nextract\n\nExtract the ${distfiles} into working directory for build.\n\npatch\n\nApply optional patch files.\n\nconfigure\n\nExecute ${configure.cmd} in ${worksrcpath}.\n\nbuild\n\nExecute ${build.cmd} in ${worksrcpath}.\n\ndestroot\n\nExecute command to “stages” an installation into an intermediate location ${destroot}.\n\ninstall\n\nArchive (a.k.a. Image) a port’s destrooted files into ${prefix}/var/macports/software.\n\nactivate\n\nExtract the port’s files from the archive in ${prefix}/var/macports/software to their final installed locations, usually in ${prefix}.\nPortGroup §\nRolling release (synced with ports tree) of utility functions to help make Portfiles more concice and declarative.\nVariants §\nMaximum flexibility, but brings a lot of headache. Prefer non-conflicting subports if possible.\nSubports §\nA directive used in Portfile to generate multiple port definitions from a single Portfile.\nC++ stdlib §\nThe default C++ runtime on OS X 10.9 and later is libc++ from LLVM which supports C++11.\nFor older OS X versions, MacPorts will install a functional libc++ toolchain by default to replace the ancient version of libstdc++ OS X shipped.\nOn Linux libstdc++ (The GNU C++ Library) is more ubiquitous."},"notes/MacPorts/base/Registry":{"title":"Registry","links":[],"tags":[],"content":"registry2 package is entirely written in C.\nTcl_PkgProvide(interp, &quot;registry2&quot;, &quot;2.0&quot;)\nregistry2.0/registry.tcl provides registry 1.0, don’t mistaken it with the C part."},"notes/MacPorts/base/Tcl-API":{"title":"Tcl API","links":[],"tags":[],"content":"Tcl_GetIndexFromObjStruct §\nSimple static dictionary lookup.\n// Array of `struct` type, whose first member is a null-terminated string.\n// End of array is marked by a NULL string pointer.\nstatic entry_cmd_type entry_cmds[] = {\n    /* Global commands */\n    { &quot;create&quot;, entry_create },\n    { &quot;delete&quot;, entry_delete },\n    { &quot;open&quot;, entry_open },\n    { &quot;close&quot;, entry_close },\n    { &quot;search&quot;, entry_search },\n    { &quot;exists&quot;, entry_exists },\n    { &quot;imaged&quot;, entry_imaged },\n    { &quot;installed&quot;, entry_installed },\n    { &quot;owner&quot;, entry_owner },\n    { NULL, NULL }\n};\n\n// Search for a string in array, save matching index to `cmd_index` and return TCL_OK.\nif (Tcl_GetIndexFromObjStruct(interp, objv[1], entry_cmds,\n            sizeof(entry_cmd_type), &quot;cmd&quot;, 0, &amp;cmd_index) == TCL_OK) {\n    entry_cmd_type* cmd = &amp;entry_cmds[cmd_index];\n    return cmd-&gt;function(interp, objc, objv);\n}\n"},"notes/Network/SNMPv2-TCP-Statistics":{"title":"SNMPv2 TCP Statistics","links":[],"tags":[],"content":"Source: RFC 2012\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObjectDescriptionRtoAlgorithmThe algorithm used to determine the timeout value used for retransmitting unacknowledged octets.RtoMinThe minimum value permitted by a TCP implementation for the retransmission timeout, measured in milliseconds. More refined semantics for objects of this type depend upon the algorithm used to determine the retransmission timeout. In particular, when the timeout algorithm is rsre(3), an object of this type has the semantics of the LBOUND quantity described in RFC 793.RtoMaxThe maximum value permitted by a TCP implementation for the retransmission timeout, measured in milliseconds. More refined semantics for objects of this type depend upon the algorithm used to determine the retransmission timeout. In particular, when the timeout algorithm is rsre(3), an object of this type has the semantics of the UBOUND quantity described in RFC 793.MaxConnThe limit on the total number of TCP connections the entity can support. In entities where the maximum number of connections is dynamic, this object should contain the value -1.ActiveOpensThe number of times TCP connections have made a direct transition to the SYN-SENT state from the CLOSED state.PassiveOpensThe number of times TCP connections have made a direct transition to the SYN-RCVD state from the LISTEN state.AttemptFailsThe number of times TCP connections have made a direct transition to the CLOSED state from either the SYN-SENT state or the SYN-RCVD state, plus the number of times TCP connections have made a direct transition to the LISTEN state from the SYN-RCVD state.EstabResetsThe number of times TCP connections have made a direct transition to the CLOSED state from either the ESTABLISHED state or the CLOSE-WAIT state.CurrEstabThe number of TCP connections for which the current state is either ESTABLISHED or CLOSE-WAIT.InSegsThe total number of segments received, including those received in error. This count includes segments received on currently established connections.OutSegsThe total number of segments sent, including those on current connections but excluding those containing only retransmitted octets.RetransSegsThe total number of segments retransmitted - that is, the number of TCP segments transmitted containing one or more previously transmitted octets.ConnTableA table containing TCP connection-specific information.InErrsThe total number of segments received in error (for example, bad TCP checksums).OutRstsThe number of TCP segments sent containing the RST flag."},"notes/Nginx":{"title":"Nginx","links":["notes/Nginx/location","notes/Nginx/Byte-Range-Request","notes/Nginx/proxy/Headers","notes/Nginx/proxy/Health-Check","notes/Nginx/proxy/Upstream-Keepalive"],"tags":[],"content":"Nginx is a high-performance Web server, mostly used to serve HTTP requests.\n\nlocation\nByte-Range Request\nProxy\n\nHeaders\nHealth Check\nUpstream Keepalive\n\n\n"},"notes/Nginx/Byte-Range-Request":{"title":"Byte-Range Request","links":[],"tags":[],"content":"Nginx will try to slice the response body when one of the following conditions is met and the request specifies a Range header.\n\nNginx is serving static files directly.\nNginx cache is enabled.\nproxy_force_ranges is set to on.\n\nThis behavior is controlled by allow_ranges in struct ngx_http_request_s."},"notes/Nginx/Nginx-Thundering-Herd-Problem":{"title":"Nginx Thundering Herd Problem","links":["notes/Operating-System/Linux/epoll"],"tags":[],"content":"Solution 1: EPOLLEXCLUSIVE flag §\nIf EPOLLEXCLUSIVE (Linux 4.5, glibc 2.24) is defined when compiling Nginx, Nginx could make use of it to reduce resource usage when volume of new connection is low.\nNginx discards EPOLLRDHUP if EPOLLEXCLUSIVE is enabled, keeping EPOLLIN and EPOLLOUT compatible with the CentOS 7 backport.\n#if (NGX_HAVE_EPOLLEXCLUSIVE &amp;&amp; NGX_HAVE_EPOLLRDHUP)\n    if (flags &amp; NGX_EXCLUSIVE_EVENT) {\n        events &amp;= ~EPOLLRDHUP;\n    }\n#endif\nFor every 16 requests handled, Nginx would re-add the socket in ngx_reorder_accept_events() to balance request across workers.\nSolution 2: listen reuseport §\nSO_REUSEPORT could significantly increase the max latency in a degraded state. See the-sad-state-of-linux-socket-balancing.\nAs a side effect, total length of pending connection queue (backlog) is multiplied by the number of workers in this case."},"notes/Nginx/location":{"title":"location","links":[],"tags":[],"content":"proxy_pass redirecting §\nIf a prefix-matching location ends with a slash (/), and requests are processed by proxy_pass, requests to the exact string minus the trailing slash will be redirected to full URI (domain included) with slash appended.\nCase sensitivity §\nOn case-sensitive operating systems, only ~* matches case-insensitively."},"notes/Nginx/proxy/Headers":{"title":"Headers","links":[],"tags":[],"content":"The default set of headers passed to upstream is defined in ngx_http_proxy_headers.\nstatic ngx_keyval_t  ngx_http_proxy_headers[] = {\n    { ngx_string(&quot;Host&quot;), ngx_string(&quot;$proxy_host&quot;) },\n    { ngx_string(&quot;Connection&quot;), ngx_string(&quot;close&quot;) },\n    { ngx_string(&quot;Content-Length&quot;), ngx_string(&quot;$proxy_internal_body_length&quot;) },\n    { ngx_string(&quot;Transfer-Encoding&quot;), ngx_string(&quot;$proxy_internal_chunked&quot;) },\n    { ngx_string(&quot;TE&quot;), ngx_string(&quot;&quot;) },\n    { ngx_string(&quot;Keep-Alive&quot;), ngx_string(&quot;&quot;) },\n    { ngx_string(&quot;Expect&quot;), ngx_string(&quot;&quot;) },\n    { ngx_string(&quot;Upgrade&quot;), ngx_string(&quot;&quot;) },\n    { ngx_null_string, ngx_null_string }\n};"},"notes/Nginx/proxy/Health-Check":{"title":"Health Check","links":[],"tags":[],"content":"If Nginx upstream points at a load balancer and overloading is not of concern, you can set max_fails to 0 to disable passive health checks."},"notes/Nginx/proxy/Upstream-Keepalive":{"title":"Keepalive","links":["notes/Nginx/proxy/Headers"],"tags":[],"content":"To enable keepalive connections to upstream, the keepalive directive must be included in upstream{} blocks, and in the location{} blocks you need to switch HTTP version to 1.1 and clear the default Connection: close request header set by Nginx, see Headers.\nproxy_http_version 1.1;\nproxy_set_header   &quot;Connection&quot; &quot;&quot;;\n"},"notes/Obsidian":{"title":"Obsidian","links":[],"tags":[],"content":"Setup §\n\nAssign Cmd+; hotkey to “Template: Insert template”.\nIn Files &amp; Links, use absolute path in vault for new links.\n\nRoutine §\n\nCreate a note by linking to it first and then press Cmd+Enter to follow the link.\nPress Cmd+; to populate the headers before writing actual content.\n"},"notes/Operating-System/FreeBSD":{"title":"FreeBSD","links":["notes/Operating-System/FreeBSD/DTrace"],"tags":[],"content":"Features §\n\nDTrace\n"},"notes/Operating-System/FreeBSD/DTrace":{"title":"DTrace","links":["notes/Programming/C/Flame-Graph"],"tags":[],"content":"One-Liners and Scripts §\nOne-Liners\nCapture Stacks §\nFor use with Flame Graph.\n# Capture user-level stack at 197 Hz for 60 seconds\nsudo dtrace -x ustackframes=100 -n &#039;profile-197 /execname == &quot;python3.9&quot;/ { @[ustack()] = count(); } tick-60s { exit(0); }&#039; -o out.stacks\n \n# Kernel stack\nsudo dtrace -x stackframes=100 -n &#039;profile-197 /pid == 12345/ { @[stack()] = count(); } tick-60s { exit(0); }&#039; -o out.kstacks\nCapture Function Argument, Return Value and Latency §\nsudo dtrace -s getaddrinfo.d -p 12345\n#!/usr/sbin/dtrace -s\n/*\n * getaddrinfo.d\n */\n\n#pragma D option quiet\n\ndtrace:::BEGIN\n{\n\tprintf(&quot;%-20s  %-4s %-12s %s\\n&quot;, &quot;TIME&quot;, &quot;RET&quot;, &quot;LATENCY(ms)&quot;, &quot;HOST&quot;);\n}\n\npid$target::getaddrinfo:entry\n{\n\tself-&gt;host = copyinstr(arg0);\n\tself-&gt;start = timestamp;\n}\n\npid$target::getaddrinfo:return\n/self-&gt;start/\n{\n\tthis-&gt;delta = (timestamp - self-&gt;start) / 1000000;\n\tprintf(&quot;%-20Y  %-4d %-12d %s\\n&quot;, walltimestamp, arg1, this-&gt;delta, self-&gt;host);\n\tself-&gt;host = 0;\n\tself-&gt;start = 0;\n}"},"notes/Operating-System/FreeBSD/ps":{"title":"ps","links":[],"tags":[],"content":"Process Tree §\n-d arranges processes into a tree representing parent and child relationships.\nps aux -d"},"notes/Operating-System/Linux/BBR":{"title":"BBR","links":[],"tags":[],"content":"Configuration §\nOnly sysctl net.ipv4.tcp_congestion_control is needed since Linux 4.13.\n\nJust a quick announcement that Eric Dumazet has checked in a nice feature in Linux 4.13-rc1 that implements TCP-level pacing in Linux TCP:\n?id=218af599fa635b107cfe10acf3249c4dfe5e4123\nThat means that the TCP layer itself can handle the pacing requirements of BBR, if the fq qdisc is not in place. In turn, that means when enabling BBR there is no need to change anything in your qdisc setup, if you don’t want to.\n"},"notes/Operating-System/Linux/FirewallD-Specify-Zone-by-IP-Address":{"title":"FirewallD Specify Zone by IP Address","links":[],"tags":[],"content":"\n\n                  \n                  Warning \n                  \n                \nMake sure to add services from the previous zone before adding IP to the new zone.\n\nfirewall-cmd --new-zone=minecraft-access --permanent\n\nfirewall-cmd --zone=minecraft-access --add-service=ssh --permanent\nfirewall-cmd --zone=minecraft-access --add-source=a.b.c.d/32 --permanent\nfirewall-cmd --zone=minecraft-access --add-port=25565/tcp --permanent\nfirewall-cmd --reload\n\nRemove --permanent flags and skip the reload command to make runtime changes that are lost on reboot, which is great for experimenting with unfamiliar changes.\nIf the client can change its IP address, risk of locking yourself out is low."},"notes/Operating-System/Linux/TCP-Kernel-Parameters":{"title":"TCP Kernel Parameters","links":["notes/Operating-System/Linux/BBR"],"tags":[],"content":"net.ipv4.tcp_congestion_control §\nSee BBR.\nnet.ipv4.tcp_rmem §\nContains three values that represent the minimum, default and maximum size of the TCP socket receive buffer.\nThe minimum represents the smallest receive buffer size guaranteed, even under memory pressure. The minimum value defaults to 1 page or 4096 bytes.\nThe default value represents the initial size of a TCP sockets receive buffer. This value supersedes net.core.rmem_default used by other protocols. The default value for this setting is 87380 bytes. It also sets the tcp_adv_win_scale and initializes the TCP window size to 65535 bytes.\nThe maximum represents the largest receive buffer size automatically selected for TCP sockets. This value does not override net.core.rmem_max. The default value for this setting is somewhere between 87380 bytes and 6M bytes based on the amount of memory in the system.\nThe recommendation is to use the maximum value of 16M bytes or higher (kernel level dependent) especially for 10 Gigabit adapters.\nnet.ipv4.tcp_wmem §\nSimilar to the net.ipv4.tcp_rmem this parameter consists of 3 values, a minimum, default, and maximum.\nThe minimum represents the smallest receive buffer size a newly created socket is entitled to as part of its creation. The minimum value defaults to 1 page or 4096 bytes.\nThe default value represents the initial size of a TCP sockets receive buffer. This value supersedes net.core.rmem_default used by other protocols. It is typically set lower than net.core.wmem_default. The default value for this setting is 16K bytes.\nThe maximum represents the largest receive buffer size for auto-tuned send buffers for TCP sockets. This value does not override net.core.rmem_max. The default value for this setting is somewhere between 64K bytes and 4M bytes based on the amount of memory available in the system.\nThe recommendation is to use the maximum value of 16M bytes or higher (kernel level dependent) especially for 10 Gigabit adapters."},"notes/Operating-System/Linux/epoll":{"title":"epoll","links":[],"tags":[],"content":"EPOLLEXCLUSIVE Flag §\nLimitations §\nThe following values may be specified in conjunction with EPOLLEXCLUSIVE: EPOLLIN, EPOLLOUT, EPOLLWAKEUP, and EPOLLET.  EPOLLHUP and EPOLLERR can also be specified, but this is not required.\nCentOS 7 §\nBackport to CentOS 7.3 only supports EPOLLIN and EPOLLOUT events alongside the implicit   EPOLLHUP and EPOLLERR events.\n* Mon May 02 2016 Rafael Aquini &lt;aquini@redhat.com&gt; [3.10.0-386.el7]\n- [fs] epoll: restrict EPOLLEXCLUSIVE to POLLIN and POLLOUT (Hannes Frederic Sowa) [1245628]\n- [fs] epoll: add EPOLLEXCLUSIVE flag (Hannes Frederic Sowa) [1245628]\n\nkernel.spec"},"notes/Operating-System/OpenWRT/WiFi":{"title":"WiFi","links":[],"tags":[],"content":"Client mode §\nconfig wifi-iface &#039;wifinet1&#039;\n\toption device &#039;radio0&#039;\n\toption mode &#039;sta&#039;\n\toption ssid &#039;AP_SSID&#039;\n\toption encryption &#039;psk2&#039;\n\toption bssid &#039;xx:xx:xx:xx:xx:xx&#039;\n\toption key &#039;xxxxxxxx&#039;\n\toption network &#039;wan&#039;\n\nDon’t use AP+STA on one physical radio without travelmate.\n\nA logical combination of AP+STA mode on one physical radio allows most of OpenWrt supported router devices to connect to a wireless hotspot/station (STA) and provide a wireless access point (AP) from that hotspot at the same time. Downside of this solution: whenever the STA interface looses the connection it will go into an active scan cycle which renders the radio unusable for AP mode operation, therefore the AP is taken down if the STA looses its association.\nTo avoid these kind of deadlocks, travelmate will set all station interfaces to an “always off” mode and connects automatically to available/configured hotspots.\n\nREADME.md\nList connections §\n# List devices\niw dev\n# List associated stations\niw dev wlan0 station dump"},"notes/Operating-System/macOS/KeyChain-in-CLI-Tools":{"title":"KeyChain in CLI Tools","links":[],"tags":[],"content":"Email §\nmbsync (isync) §\nUseKeychain yes\n\nAdd the above to mbsync configuration and create the keychain item with:\nsecurity add-internet-password -r imap -s Host -a User -w\nmsmtp §\nUse the exact host and user configured in .msmtprc. The process is automatic if you got these right and have compiled msmtp with macOS Keychain support.\nsecurity add-internet-password -r smtp -s mail.freemail.example -a joe.smith@freemail.example -w"},"notes/Programming":{"title":"Programming","links":[],"tags":[],"content":"Programming is the process of creating software."},"notes/Programming/C/Compile-Database":{"title":"Compile Database","links":[],"tags":[],"content":"Database Generation §\nCMake §\ncmake -DCMAKE_EXPORT_COMPILE_COMMANDS=On ...\n\nMakefile §\ncompiledb\ncompiledb make -j&lt;N&gt;\nOn macOS, you may need to use gmake to workaround SIP (System Integrity Protection)."},"notes/Programming/C/Flame-Graph":{"title":"Flame Graph","links":[],"tags":[],"content":"FlameGraph\nGraph Variants §\n\n--reverse stack-reversed flame graph\n--inverted icicle graph (top to bottom, )\n"},"notes/Programming/C/LLVM/Code-Coverage":{"title":"Code Coverage","links":[],"tags":[],"content":"llvm-cov gcov §\n\nOnce you have generated the coverage data files, run llvm-cov gcov for each main source file where you want to examine the coverage results. This should be run from the same directory where you previously ran the compiler.\n\n./configure \\\n  CFLAGS=&quot;--coverage&quot; \\\n  LDFLAGS=&quot;--coverage&quot;\nmake\ncd src\ngcov *.c\nprofile-instr §\n\nTo use llvm-cov show, you need a program that is compiled with instrumentation to emit profile and coverage data. To build such a program with clang use the -fprofile-instr-generate and -fcoverage-mapping flags. If linking with the clang driver, pass -fprofile-instr-generate to the link stage to make sure the necessary runtime libraries are linked in.\n\n./configure \\\n  CFLAGS=&quot;-fprofile-instr-generate -fcoverage-mapping&quot; \\\n  LDFLAGS=&quot;-fprofile-instr-generate&quot;\nmake\nLLVM_PROFILE_FILE=&quot;$PWD/cov-%p.profraw&quot; make test\nllvm-profdata merge --sparse --output=cov.profdata cov-*.profraw"},"notes/Programming/Go/benchstat":{"title":"benchstat","links":[],"tags":[],"content":"Compare go test -bench=... results and print nice statistics.\nbenchstat\n$ benchstat old.txt new.txt\nname        old time/op  new time/op  delta\nGobEncode   13.6ms ± 1%  11.8ms ± 1%  -13.31% (p=0.016 n=4+5)\nJSONEncode  32.1ms ± 1%  31.8ms ± 1%     ~    (p=0.286 n=4+5)\n"},"notes/Programming/Python/Module-Search-Path":{"title":"Module Search Path","links":[],"tags":[],"content":"sys.path is initialized from these locations:\n\nThe directory containing the input script (or the current directory when no file is specified).\nPYTHONPATH (a list of directory names, with the same syntax as the shell variable PATH).\nThe installation-dependent default (by convention including a site-packages directory, handled by the site module).\n"},"notes/SRE":{"title":"SRE","links":["notes/SRE/On-Call","notes/SRE/Monitoring","notes/SRE/Emergency-Response","notes/SRE/Change-Management","notes/SRE/Capacity-Planning","notes/SRE/Measuring-Service-Risk","notes/SRE/Managing-Service-Risk","notes/SRE/Eliminating-Toil"],"tags":[],"content":"Definition §\nSRE is what happens when you ask a software engineer to design an ops team.\nWe want systems that runs and repairs itself: automatic, not just automated.\nTopics §\n\nOn-Call\nMonitoring\nEmergency Response\nChange Management\nCapacity Planning\nRisk Management\n\nMeasuring Service Risk\nManaging Service Risk\n\n\nEliminating Toil\nSimplicity\n\nSRE teams should\n\nPush back when accidental complexity is introduced into the systems for which they are responsible.\nConstantly strive to eliminate complexity in systems they onboard and for which they assume operational responsibility.\n\n\n\n\nHierarchy of Needs (from most basic to most advanced)\n\nMonitoring\nIncident Response\nPostmertem w/ RCA\nTesting and Release procedures\nCapacity Planning\nDevelopment\nProduct\n\n\n"},"notes/SRE/Alerting":{"title":"Alerting","links":[],"tags":[],"content":"Software should do the interpreting of the alerting domain, and humans should be notified when action is needed.\nThere are three kinds of valid monitoring output:\n\nAlerts (paging): immediate action needed\nTickets and email alerts: action needed, but not immediately\nLogging: recorded for diagnostic or forensic purposes\n\nPaging a human is a quite expensive use of an employee’s time. Effective alerting systems have good signal and very low noise.\nYou should favor a dashboard that monitors all ongoing sub-critical problems for the sort of information that typically ends up in email alerts. A dashboard might also be paired with a log, in order to analyze historical correlations."},"notes/SRE/Blameless-Culture":{"title":"Blameless Culture","links":[],"tags":[],"content":"Definition §\nBlameless culture means\n\nFocus on identifying the contributing causes of the incident without indicating any individual or team for bad or inappropriate behavior.\nAssume that everyone involved in an incident had good intentions and did the right thing with the information they had.\n\nYou can’t “fix” people, but you can fix systems and processes to better support people making the right choices when designing and maintaining complex systems."},"notes/SRE/Capacity-Planning":{"title":"Capacity Planning","links":[],"tags":[],"content":"SRE ultimately controls provisioning. We provision to meet a capacity target at a specific response speed, thus we are keenly interested in a service’s performance.\n\nTake both organic growth and inorganic growth into account.\nRegular load testing to correlate raw capacity to service capacity.\nResource use is a function of demand (load), capacity and software efficiency.\n"},"notes/SRE/Change-Management":{"title":"Change Management","links":["notes/SRE/Mandatory-Review","notes/SRE/Error-Budget"],"tags":[],"content":"Roughly 70% of outages are due to changes in a live system.\nTrio of practices §\n\nImplementing progressive rollouts\nQuickly and accurately detecting problems\nRolling back changes safely when problems arise\n\nAutomation §\nRemove humans from the loop, use automation.\nMandatory Review §\nVelocity §\nFrequency of pushes should be guided by Error Budget.\n\n\n                  \n                  Info \n                  \n                \nPush means any change to a service’s running software or its configuration.\n"},"notes/SRE/Eliminating-Toil":{"title":"Eliminating Toil","links":["notes/SRE/Toil"],"tags":[],"content":"Toil\nTODO: eliminating-toil\n50% Cap §\n“Ops” work like tickets, on-call should be capped to below 50% of each SRE’s time.\nThis ensures a focus on the engineering aspect of SRE.\nOn the other hand, toil doesn’t make everyone unhappy all the time, especially in small amounts. Toil isn’t always and invariably bad, and some amount of toil is unavoidable in the SRE role. That’s why we aim for a cap instead of total elimination of toil."},"notes/SRE/Emergency-Response":{"title":"Emergency Response","links":[],"tags":[],"content":"Playbooks §\nHumans add latency, so thinking through and recording the best practices ahead of time in a “playbook”.\nIt’s also a substitute for smart engineers able to think on the fly, and helps response to a high-stakes or time-sensitive page."},"notes/SRE/Error-Budget":{"title":"Error Budget","links":["notes/SRE/Service-Level-Indicator","notes/SRE/Change-Management","notes/SRE/SLO"],"tags":[],"content":"Purpose §\nError budgets resolve the structural conflict of incentives between development and SRE. If product development wants to skimp on testing or increase push velocity and SRE is resistant, the error budget guides the decision.\nThe goal is not “zero outages”, an outage is not a “bad” thing.\n\nIt is an expected part of the process of innovation.\nAn occurrence that we manage, rather than fear.\nWe aim to spend the error budget getting maximum feature velocity.\n\nExample §\nWhat happens if a network outage or data-center failure reduces the measured Service Level Indicator?\n\nSuch events also eat into the error budget.\nAs a result, the number of new pushes may be reduced for the remainder of the quarter. Change Management\nEveryone shares the responsibility for uptime.\nIf the team is having trouble launching new features, they may elect to loosen the SLO in order to increase innovation.\n\nImplementations §\nSloth is a Prometheus-native way to generate SLO metrics, including error budgets."},"notes/SRE/Managing-Incidents":{"title":"Managing Incidents","links":[],"tags":[],"content":"Recursive Separation of Responsibilities §\nA role leader might delegate system components to colleagues, who report high-level information back up to the leaders.\nSeveral roles that could be delegated:\n\nIncident Command\n\nHold the high-level state about the incident, structure the incident response task force, assigning responsibilities according to need and priority.\nHold all positions that they have not delegated.\nKeep a living incident document.\n\n\nOperational Work\n\nWork with the commander to respond to the incident by applying operational tools.\n\n\nCommunication\n\nThe public face of the incident response task force.\n\n\nPlanning\n\nDeal with longer-term issues, such as filing bugs, ordering dinner, arranging handoffs, and tracking how the system has diverged from the norm, so that it can be reverted later.\n\n\n\nTracking Outages §\nBuild a tracking system where Multiple escalating notifications (“alerts”) can be combined into a single entity (“incident”) with free-form tags like cause:network, bug:1234 and bogus."},"notes/SRE/Managing-Service-Risk":{"title":"Managing Service Risk","links":["notes/SRE/SLO"],"tags":[],"content":"With risk conceptualized as a (nonlinear) continuum, we need to determine where to place individual services on the continuum.\nWe give equal importance to\n\nFigure out how to engineer great reliability into our systems.\nIdentify the appropriate level of tolerance for the services we run.\n\nFor consumer services, product owners are often the product teams (product managers). We often need to do cost/benefit analysis (e.g. reliability vs. revenue) when planning changes.\nThe cost is not only from compute resources. It also includes the opportunity cost, which is the cost of engineering resources spent to diminish risk that could have been devoted to market opportunity (e.g. features for end users).\nFor infrastructure services, they have multiple clients, often with varying needs. The key strategy is to deliver services with explicitly delineated levels of service, so that the client can make the right risk and cost-tradeoffs when building their systems.\nRisk Target §\nUpon setting an availability target, the goal is to explicitly align the risk taken with the risk the business is willing to bear, i.e. make a service reliable enough, but no more reliable than it needs to be, see Choosing SLO Targets for details.\nThe target is often set quarterly and tracked weekly or daily. We might set an external quarterly availability target of 99.9%, and back this target with a stronger internal availability target and a contract that stipulates penalties if we fail to deliver to the external target."},"notes/SRE/Mandatory-Review":{"title":"Mandatory Review","links":[],"tags":[],"content":"All proposed changes (CLs) should be reviewed before being submitted."},"notes/SRE/Measuring-Service-Risk":{"title":"Measuring Service Risk","links":["notes/SRE/Error-Budget","notes/SRE/Managing-Service-Risk"],"tags":[],"content":"The traditional way of measuring unplanned downtime is not suitable for geo-distributed Internet services, because a given service is at least partially “up” at all times.\nIn a typical Internet service, availability can be measured by dividing the number of successful requests with total requests.\nError Budget is often a good indicator of service health. SREs must work with the product owners to turn business goals into explicit objectives that can be measured, see Managing Service Risk for details."},"notes/SRE/Monitoring":{"title":"Monitoring","links":["notes/SRE/Alerting"],"tags":[],"content":"The Four Golden Signals §\n\nLatency\n\nIt’s important to distinguish between the latency of successful requests and the latency of failed requests.\n\nShould we cout client timeouts as successful requests? They are potentially requests that took too long but could have been successful, on the other hand it could also be a client-side early close.\n\n\nA slow error is even worse than a fast error! Therefore, it’s important to track error latency, as opposed to just filtering out errors.\n\n\nTraffic\n\nNormalized request rate, for example HTTP requests per second.\n\n\nErrors\n\nRate of requests that fail, either explicitly (e.g. HTTP 500s), implicitly (e.g. wrong content), or by policy (e.g. missing latency target).\n\n\nSaturation\n\nEmphasize the resources that are most constrained (e.g. memory, I/O, etc.)\nSystems degrade before they achieve 100% utilization, so having a utilization target is essential.\n\n\n\nMetric Aggregation §\nMost metrics are better thought of as distributions rather than averages. A high-order percentile, such as the 99th or 99.9th, shows you a plausible worst-case value, while the 60th percentile emphasizes the typical case.\nFor example, Histogram summaries of CPU usage per second in a minute provides good resolution (granularity) with less collection and retention cost.\nBlack-Box Versus White-Box §\nWe should aim for heavy use of white-box monitoring with modest but critical uses of black-box monitoring. White-box monitoring helps identify the root cause.\nBlack-box monitoring is symptom-oriented and represents active (not predicted) problems. For not-yet-occurring but imminent problems, black-box monitoring is fairly useless.\nAlerting §"},"notes/SRE/On-Call":{"title":"On-Call","links":["notes/SRE/Postmortem","notes/SRE/Emergency-Response","notes/SRE/Troubleshooting"],"tags":[],"content":"Best Practice §\nGoogle recommends a maximum of two events per 8-12-hour on-call shift to have adequate time to handle the event accurately and quickly.\nA Postmortem should be conducted after each event, subject to postmortem criteria of the company.\nHandling an incident §\n\nEmergency Response\nTroubleshooting\n"},"notes/SRE/Postmortem":{"title":"Postmortem","links":["notes/SRE/Blameless-Culture"],"tags":[],"content":"Criteria §\nIt is important to define postmortem criteria before an incident occurs so that everyone knows when a postmortem is necessary.\nBlameless Culture §\nImportance of the blameless culture when conducting a postmortem can not be overstated. It’s the best way to encourage SREs to face incidents without fear."},"notes/SRE/Prodtest":{"title":"Prodtest","links":[],"tags":[],"content":"Prodtest extent a unit test framework to allow for unit testing of real-world services.\nAny time a team encountered a delay due to anthoer team’s unexpected misconfiguration, a bug could be filed to extend their Prodtest. This ensures that a similar problem would be discovered earlier in the future."},"notes/SRE/SLO":{"title":"SLO","links":["notes/SRE/Managing-Service-Risk","notes/SRE/Service-Level-Indicator"],"tags":[],"content":"Purpose §\nManaging Service Risk\nSLOs should be a major driver in prioritizing work for SREs and product developers, because they reflect what users care about. A poorly thought-out SLO can result in wasted work if a team uses heroic efforts to meet an overly aggressive SLO, or a bad product if the SLO is too lax.\nProcess §\n\nIdentify the relevant SLIs.\nChoose and publish SLOs and SLAs.\nKeep tracking and revise as needed.\n\nChoosing and publishing SLOs to users sets expectations about how a service will perform. Without an explicit SLO, users often develop their own beliefs about desired performance which may lead to both over-reliance (users incorrectly believe that a service will be more available than it actually is) and under-reliance (prospective users believe a system is flakier and less reliable than it actually is).\nStart by thinking about what your users care about, not what you can measure. If you start with what’s easy to measure, you’ll end up with less useful SLOs. Sometimes, working from desired objectives backward to specific indicators works better than choosing indicators and then coming up with targets.\nSLOs should specify how they’re measured and the conditions under which they’re valid.\nChoosing Targets §\nHere are several rules to follow when choosing your SLO targets:\nDon’t pick a target based on current performance §\nWhile understanding the merits and limits of a system is essential, adopting values without reflection may lock you into supporting a system that requires heroic efforts to meet its targets, and that cannot be improved without significant redesign.\nKeep it simple §\nComplicated aggregations can obscure changes to system performance, and are also harder to reason about.\nAvoid absolutes §\nNo “infinite” scale or “always” available.\nHave as few SLOs as possible §\nChoose just enough SLOs to provide good coverage of your system’s attributes.\nPerfection can wait §\nYou can always refine SLO definitions and targets over time as you learn about a system’s behavior. It’s better to start with a loose target that you tighten than to choose an overly strict target.\nManaging Expectation §\nPublishing SLOs set expectations for system behavior. Users often want to know what they can expect from a service in order to understand whether it’s appropriate for their use case. There are some tatics you could follow.\nKeep a safety margin §\nUse a tighter internal SLO than the SLO advertised to users to give yourself room to respond to chronic problems before they become visible externally. A buffer allows you to accommodate re-implementations that trade performance for other attributes, such as cost or ease of maintenance.\nDon’t overachieve §\nUsers build on the reality of what you offer, rather than what yo say you’ll supply, particularly for infra services.\nYou can avoid over-dependence by deliberately taking the system offline occasionally, throttling some requests, or designing the system so that it isn’t faster under light loads.\nSLA §\nSRE’s role is to help business and legal teams understand the likelihood and difficulty of meeting the SLOs contained in the SLA (Service Level Agreement).\nIt is wise to be conservative in what you advertise to users."},"notes/SRE/Service-Level-Indicator":{"title":"Service Level Indicator","links":["notes/SRE/Monitoring"],"tags":[],"content":"Be Realistic §\nSometimes only a proxy is available. For example, client-side latency is often the more user-relevant metric, but it might only be possible to measure latency at the server.\nCommon Indicators §\n\nRequest latency\nError rate / Availability (commonly expressed in the number of nines, e.g. 99% is “2 nines”)\nSystem throughput\nData durability\nCorrectness (needless to say, but often not an SRE responsibility)\n\nA few broad categories of services tend to find different SLIs relevant:\n\nUser-facing serving systems generally care about availability, latency and throughput.\nStorage systems often emphasize latency, availability and durability.\nBig data systems tend to care about throughput and end-to-end latency.\n\nAggregation §\nSee Monitoring.\nSLI Templates §\nTo save effort, build a set of reusable SLI templates for each common metric. Define the aggregation interval &amp; regions, measurement frequency, scope, method of measurement, etc in the template."},"notes/SRE/Toil":{"title":"Toil","links":[],"tags":[],"content":"Definition §\nToil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows.\nBalance with Engineering Work §\nEngineering work is novel and intrinsically requires human judgment. It produces a permanent improvement in your service, and is guided by a strategy. It is frequently creative and innovative, taking a design-driven approach to solving a problem—the more generalized, the better. It helps your team or the SRE organization handle a large service, or more services, with the same level of staffing.\nTypical SRE activities involve\n\nSoftware engineering\n\nInvolves writing or modifying code, in addition to any associated design and documentation work.\n\n\nSystems engineering\n\nInvolves configuring production systems, modifying configurations, or documenting systems in a way that produces lasting improvements from a one-time effort.\nExamples include monitoring setup and updates, load balancing configuration, server configuration, tuning of OS parameters, and load balancer setup.\nAlso includes consulting on architecture, design and productionization for developer teams.\n\n\nToil\nOverhead: Administrative work not tied directly to running a service.\n\nToil tends to be spiky, so a steady 50% may not be realistic."},"notes/SRE/Troubleshooting":{"title":"Troubleshooting","links":[],"tags":[],"content":"Monitoring, logging and exposing current state are the three tricks for troubleshooting.\nIt’s really useful to have multiple verbosity levels available, along with a way to increase these levels on the fly."},"notes/Software/Irssi-Config":{"title":"Irssi Config","links":[],"tags":[],"content":"Pin Self-signed Certificate §\n/set use_tls yes\n/set tls_verify no\n/set tls_pinned_cert &quot;XX:XX:...&quot;\n\nCertfp authentication §\nThe pem file should contain both a private key and the corresponding certificate.\n/set tls_cert &quot;~/.irssi/certs/xxx.user.pem&quot;\n"},"notes/Software/Netdata":{"title":"Netdata","links":[],"tags":[],"content":"Disable Telemetry and Cloud Features §\ncat &gt; ~/cloud.conf &lt;&lt; EOF\n[global]\n  enabled = no\nEOF\n \nsudo cp ~/cloud.conf /var/lib/netdata/cloud.d/\nsudo touch /etc/netdata/.opt-out-from-anonymous-statistics\nHow to build RPM packages §\nbuilding-native-packages-locally.md\n:z is needed if SELinux is enabled. Use :Z for additional access protection.\ngit clone https://github.com/netdata/netdata.git\ncd netdata\n \ngit apply &lt;&lt;&#039;EOF&#039;\ndiff --git a/netdata.spec.in b/netdata.spec.in\nindex c2fa7dcab..f70f74c6c 100644\n--- a/netdata.spec.in\n+++ b/netdata.spec.in\n@@ -231,6 +231,7 @@ export CFLAGS=&quot;${CFLAGS} -fPIC&quot; &amp;&amp; ${RPM_BUILD_DIR}/%{name}-%{version}/packaging\n # Conf step\n autoreconf -ivf\n %configure \\\n+\t--disable-cloud \\\n \t%if 0%{!?_have_ebpf}\n \t--disable-ebpf\n \t%endif\nEOF\n \npodman run -it --rm -e VERSION=&lt;1.x.x&gt; -v $PWD:/netdata:z netdata/package-builders:oraclelinux9\n# checking if Cloud functionality should be enabled... no\nCharts v3 §\n\nThese are currently available at Netdata Cloud. At the next Netdata release (v1.40.0), the agent dashboard will be replaced to also use the same charts.\n"},"notes/Software/WeeChat-Config":{"title":"WeeChat Config","links":[],"tags":[],"content":"ZNC Multi-Connect and CertFP §\nConnect to multiple networks by specifying network name in username in username/networkname format.\n/server add networkname IP/port -ssl -username=username/networkname -password=x -autoconnect\n/set irc.server.snoonet.ssl_fingerprint xxxx\n/set irc.server.libera.ssl_cert &quot;${weechat_config_dir}/ssl/client.pem&quot;\nSet irc.server.libera.ssl_fingerprint to verify with fingerprint only if your server has a self-signed certificate.\nLook and Feel §\nSeparate Server Buffers §\n/set irc.look.server_buffer independent\nIRC Smart Filter §\n/filter add irc_smart * irc_smart_filter *\nXDG Directories §\nTable of default directories and paths.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariableDefault valueFallback valueweechat_config_dir$XDG_CONFIG_HOME/weechat$HOME/.config/weechat if $XDG_CONFIG_HOME is not defined or emptyweechat_data_dir$XDG_DATA_HOME/weechat$HOME/.local/share/weechat if $XDG_DATA_HOME is not defined or emptyweechat_cache_dir$XDG_CACHE_HOME/weechat$HOME/.cache/weechat if $XDG_CACHE_HOME is not defined or emptyweechat_runtime_dir$XDG_RUNTIME_DIR/weechatsame as cache directory if $XDG_RUNTIME_DIR is not defined or empty\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptionDefault valuefifo.file.path${weechat_runtime_dir}/weechat_fifo_${info:pid}logger.file.path${weechat_data_dir}/logsrelay.network.ssl_cert_key${weechat_config_dir}/ssl/relay.pemscript.scripts.path${weechat_cache_dir}/scriptweechat.plugin.path${weechat_data_dir}/pluginsxfer.file.download_path${weechat_data_dir}/xfer\nReleaseNotes.adoc#v3.2_xdg_directories"},"notes/Software/ZNC":{"title":"ZNC","links":[],"tags":[],"content":"CertFP with Server §\nOne certificate per user only. Make sure you have a backup before updating it from Web UI.\nCert\nCertFP with Client §\nChat with *certauth to manage keys. It’s not configurable from Web UI.\nCertauth\n/msg *certauth help\n&lt;*certauth&gt; Commands: show, list, add, del [no]\n/msg *certauth show\n&lt;*certauth&gt; Your current public key is: ...\n/msg *certauth list\n&lt;*certauth&gt; No keys set for your user\n/msg *certauth add\n&lt;*certauth&gt; Added your current public key to the list\n/msg *certauth list\n...\n"},"notes/Taking-Notes":{"title":"Taking Notes","links":["notes/Obsidian"],"tags":[],"content":"Organizing Notes §\n\nCapture ideas on the go with Tana Capture.\nTurn them into organized fleet notes.\nKeep a private journal for complex tasks.\nNurture evergreen notes from fleet notes and journals.\nContinously evolve your evergreen notes.\n\nTools §\nObsidian is a good writing tool for Markdown notes."},"notes/Twitter":{"title":"Twitter","links":[],"tags":[],"content":"uBlock Origin rules §\n! Discover more / More Tweets - Conversation view suggestions\ntwitter.com##[aria-label=&quot;Timeline: Conversation&quot;] [data-testid=&quot;cellInnerDiv&quot;] h2:has-text(/^(Discover more|More Tweets)$/):upward([data-testid=&quot;cellInnerDiv&quot;])\ntwitter.com##[aria-label=&quot;Timeline: Conversation&quot;] [data-testid=&quot;cellInnerDiv&quot;] h2:has-text(/^(Discover more|More Tweets)$/):upward([data-testid=&quot;cellInnerDiv&quot;])~div\n"}}