{"index":{"title":"Zero's Garden","links":["maps/Public-Cloud-Map","maps/SRE-Map","notes/Taking-Notes","maps/AI-Productivity-Tool-Map"],"tags":[],"content":"Welcome to my digital garden, where I keep notes of my thoughts and knowledge about topics I’m interested in like computer science.\nNotes are loosely grouped into topics and interconnected, and there is no strict hierarchy. Here are some top-level links to expand on.\n\nPublic Cloud Map\nSRE Map\nTaking Notes\nAI Productivity Tool Map\n\nTo focus on permanent notes and keep the scale of this place under control, fleeting notes and automatically imported notes are stored in a separate Logseq graph inspired by note.xuanwo.io/.\nPersonal Use Notice\nContent on this website is provided “as is” and the author may move or delete content at any time, causing link rot.\nIt is therefore not recommended to directly link to any specific notes on this website, except as required by the CC BY-NC-ND 4.0 license.\nYou may contact the author if you need a stable reference. The author may decide, at their sole discretion, to publish a blog post on l2dy.github.io/blog/.\nLegal Notice\nUnless otherwise stated, content on this website is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License."},"maps/AI-Productivity-Tool-Map":{"title":"AI Productivity Tool Map","links":["notes/AI/ChatGPT","notes/AI/LLM-Models"],"tags":[],"content":"Chatbots\nChatGPT\nCoding Assistant\n\nGitHub Copilot\nAmazon CodeWhisperer\n\nModels\n\nLLM Models\n\nToolkit\n\nLangChain\n"},"maps/AWS-Map":{"title":"AWS Map","links":["notes/Cloud/AWS/AWS-Backbone-Network","notes/Cloud/AWS/AWS-VPC","notes/Cloud/AWS/Hyperplane","notes/Cloud/AWS/Network-Load-Balancer","notes/Cloud/AWS/AWS-Pitfalls"],"tags":[],"content":"Networking\nThe AWS Backbone Network connects AWS Regions, Availability Zones, and the Internet.\nAWS VPC is a SDN built for AWS powering isolated virtual networks.\nHyperplane is an AWS internal network load balancing service, powering other AWS services such as the Network Load Balancer (NLB) and EFS.\nOther Notes\nAWS Pitfalls"},"maps/Computer-Networking-Map":{"title":"Computer Networking Map","links":["notes/Network/NIC-Offload","notes/Network/NIC-Scaling-and-Steering"],"tags":[],"content":"NIC\nNetwork interface controller (NIC) is the hardware component for networking.\n\nNIC Offload\nNIC Scaling and Steering\n\nTCP\nTODOs\n\nCongestion Control Algorithms\nexplicit congestion notification (ECN)\nHyStart++\nSACK loss recovery\nProportional Rate Reduction (RFC6937)\nbandwidth-delay product (BDP)\npacket pacing\nBBRv3 for WAN traffic and BBR.Swift for traffic within a datacenter\n"},"maps/DevOps-Roadmap":{"title":"DevOps Roadmap","links":[],"tags":[],"content":"mindmap\n\troot((DevOps))\n\t\tDevelopment\n\t\t\tProject Management\n\t\t\tPlatform Support\n\t\t\tCoding Skills\n\t\t\tArchitecture\n\t\t\t\tTrade-offs\n\t\t\t\tPerformance\n\t\t\t\tComplexity\n\t\t\t\tExtensibility, e.g. bfe\n\t\t\t\tServicability, e.g. cloud infra that supports VM live migration\n\t\t\tEffort Estimation\n\t\t\tContinuous Integration\n\t\t\t\tUnit &amp; Integration Testing\n\t\t\t\tStatic analysis, e.g. SonarQube, npm audit\n\t\t\t\tDynamic analysis, e.g. fuzzing, ASan\n\t\t\t\tContinuous Delivery\n\t\tOperations\n\t\t\tResilience\n\t\t\t\tDisaster Recovery\n\t\t\t\tChaos Engineering\n\t\t\tElasticity\n\t\t\t\tLoad Balancing\n\t\t\t\tAuto Scaling\n\t\t\tSecurity\n\t\t\t\tWAF\n\t\t\t\tHIDS\n\t\t\t\tZero Trust\n\t\t\tObservability\n\t\t\t\tMetrics and Logs\n\t\t\t\tTraces\n\t\t\t\tContinuous Profiling\n\t\t\t\tAlerting\n\t\t\t\tService Level Objectives\n\t\t\tTroubleshooting\n\t\t\t\tLinux Commands\n\t\t\t\tNetworking\n\t\t\tProcess\n\t\t\t\tChange Management\n\t\t\t\tPlaybooks\n\t\t\t\tAutomation\n\t\t\t\tOn-Call Rotation\n\t\t\tCost &amp; Performance Optimization\n\t\t\t\tCost Data Analysis\n\t\t\t\tTracing and Profiling\n\t\t\tManagement &amp; Culture\n\t\t\t\tBlameless Postmortem or Post-Incident Reviews\n\t\t\t\tTeam Structure\n\t\t\t\tTeamwork\n\t\t\t\tOKR\n\t\t\t\tData-Driven\n"},"maps/FreeBSD-Map":{"title":"FreeBSD Map","links":["notes/Operating-System/FreeBSD/DTrace","notes/Operating-System/FreeBSD/Upgrading-FreeBSD"],"tags":[],"content":"Features\n\nDTrace\n\nNotes\n\nUpgrading FreeBSD\n"},"maps/Google-Cloud-Map":{"title":"Google Cloud Map","links":["notes/Cloud/Google-Cloud/Andromeda","notes/Cloud/Google-Cloud/Maglev"],"tags":[],"content":"Networking\nAndromeda powers VM-to-VM communication.\nMaglev powers regional Network Load Balancers."},"maps/Linux-Map":{"title":"Linux Map","links":["notes/Operating-System/Linux/De-Prioritize-Process","notes/Operating-System/Linux/TCP-Kernel-Parameters","notes/Network/SNMPv2-TCP-Statistics","notes/Operating-System/Linux/BBR","notes/Operating-System/Linux/Firewalld","notes/Operating-System/Linux/nftables","notes/Operating-System/Linux/SysRq-key","notes/Operating-System/Linux/BPF"],"tags":[],"content":"Process\nScheduling\n\nDe-Prioritize Process\n\nNetwork\nTCP\n\nTCP Kernel Parameters\nSNMPv2 TCP Statistics\nBBR\n\nFirewall\n\nFirewalld\nnftables\n\nAdministration\n\nSysRq key\n\nBPF\n\nBPF\n"},"maps/Microsoft-Azure-Map":{"title":"Microsoft Azure Map","links":["notes/Cloud/Azure/Virtual-Filtering-Platform","notes/Cloud/Azure/Azure-Accelerated-Networking","notes/Cloud/Azure/Azure-Load-Balancer","notes/Cloud/Azure/Ananta","notes/Network/Direct-Server-Return","notes/Cloud/Azure/Azure-Blob-Storage"],"tags":[],"content":"Networking\nAzure Host SDN is comprised of Virtual Filtering Platform and the hardware offload counterpart Azure Accelerated Networking (AccelNet) with Azure SmartNIC based on FPGAs.\nAzure Load Balancer implements L4 load balancing powered by Ananta, utilizing DSR and VM-to-VM Fastpath to reduce number of packets processed.\nAzure Blob Storage is an object storage service, equivalent to Amazon S3 on Azure.\nWith stateless fallback on load balancers and host connection limit, the Azure networking stack is hostile to long-lived connections, especially in large quantities."},"maps/NGINX-Map":{"title":"NGINX Map","links":["notes/NGINX/NGINX-location-Directive","notes/NGINX/NGINX-HTTP-Variables-to-Log","notes/NGINX/Byte-Range-Request","notes/NGINX/proxy/Headers-in-NGINX","notes/NGINX/proxy/Health-Checks-in-NGINX","notes/NGINX/proxy/How-NGINX-round-robin-upstream-implements-passive-health-checks","notes/NGINX/proxy/Upstream-Keepalive","notes/NGINX/NGINX-realip-configuration-for-Cloudflare","notes/NGINX/Optimizing-single-box-NGINX-performance","notes/NGINX/NGINX-Proxy-Cache","notes/NGINX/NGINX-IPv6-Rate-Limiting","notes/NGINX/openresty/Customize-error-page-of-ngx.exit","notes/NGINX/NGINX-Code-Walkthrough---Client-Closed-Request","notes/NGINX/NGINX-Code-Walkthrough---Upstream-Connect-Timeout","notes/NGINX/NGINX-Thundering-Herd-Problem"],"tags":[],"content":"NGINX is a high-performance Web server, mostly used to serve HTTP requests.\nConfiguration\n\nNGINX location Directive\nNGINX HTTP Variables to Log\nByte-Range Request\nProxy\n\nHeaders in NGINX\nHealth Checks in NGINX\n\nHow NGINX round-robin upstream implements passive health checks\n\n\nUpstream Keepalive\n\n\nNGINX realip configuration for Cloudflare\n\nOptimization\n\nOptimizing single-box NGINX performance\nNGINX Proxy Cache\n\nAccess Control\n\nNGINX IPv6 Rate Limiting\nCustomize error page of ngx.exit (with Lua)\n\nCode Walkthrough\n\nNGINX Code Walkthrough - Client Closed Request\nNGINX Code Walkthrough - Upstream Connect Timeout\nNGINX Thundering Herd Problem\n\nQuick Notes\n\nNGINX reserves a large amount of RAM if worker_connections is set to an unreasonable value.\nserver_tokens is best turned off to reduce metadata leak and save bandwidth.\nNested locations are a trap where proxy_pass does not get inherited. Avoid them if possible.\nIncrease proxy_buffers and client_body_buffer_size to save a temporary file read and write, but note that it also increases memory pressure.\n"},"maps/Package-Managers-Map":{"title":"Package Managers Map","links":[],"tags":[],"content":"For use in $HOME:\n\ngithub.com/conda-forge/miniforge, with the conda-forge channel set as the default (and only) channel.\n\nFor deterministic and declarative configuration:\n\nNix with nix-installer, and\nhome-mangler for a lightweight package-only alternative to home-manager, or\nnix-darwin for declarative system-wide configuration on macOS.\n\nTraditional package managers:\n\nMacPorts (macOS)\nDNF with RPM\nAPT with dpkg or alternative frontends like oma\n"},"maps/Presentation-Skills-Map":{"title":"Presentation Skills Map","links":["notes/Note-Making/Open-Source-Diagram-Editors"],"tags":[],"content":"Diagrams\nOpen Source Diagram Editors"},"maps/Public-Cloud-Map":{"title":"Public Cloud Map","links":["maps/AWS-Map","maps/Microsoft-Azure-Map","maps/Google-Cloud-Map","notes/Cloud/AWS/AWS-Pitfalls"],"tags":[],"content":"Architecture Exploration\n\nAWS Map\nMicrosoft Azure Map\nGoogle Cloud Map\n\nNitty-Gritty\n\nAWS Pitfalls\n"},"maps/SRE-Map":{"title":"SRE Map","links":["notes/SRE/On-Call","notes/SRE/Monitoring","notes/SRE/Emergency-Response","notes/SRE/Change-Management","notes/SRE/Capacity-Planning","notes/SRE/Measuring-Service-Risk","notes/SRE/Managing-Service-Risk","notes/SRE/Eliminating-Toil"],"tags":["sre"],"content":"Definition\nSRE is what happens when you ask a software engineer to design an ops team.\nWe want systems that runs and repairs itself: automatic, not just automated.\nTopics\n\nOn-Call\nMonitoring\nEmergency Response\nChange Management\nCapacity Planning\nRisk Management\n\nMeasuring Service Risk\nManaging Service Risk\n\n\nEliminating Toil\nSimplicity\n\nSRE teams should\n\nPush back when accidental complexity is introduced into the systems for which they are responsible.\nConstantly strive to eliminate complexity in systems they onboard and for which they assume operational responsibility.\n\n\n\n\nHierarchy of Needs (from most basic to most advanced)\n\nMonitoring\nIncident Response\nPostmertem w/ RCA\nTesting and Release procedures\nCapacity Planning\nDevelopment\nProduct\n\n\n"},"maps/Self-Hosting-Map":{"title":"Self-Hosting Map","links":["notes/Self-Hosting/NAS-Solutions"],"tags":[],"content":"Network\nStorage\nNAS Solutions"},"notes/AI/ChatGPT":{"title":"ChatGPT","links":["notes/AI/New-Bing","notes/AI/Poe"],"tags":[],"content":"Derivatives\n\nNew Bing\nPoe\n"},"notes/AI/GPT-4-Chat-Demo":{"title":"GPT-4 Chat Demo","links":[],"tags":[],"content":"\n\n                  \n                  Attention\n                  \n                \n\nThe chat was saved to demonstrate how to ask GPT-4 questions and provide hints. The formula may be incorrect.\n\n\nChat Text\nCould you estimate the average time a TCP request waits in the surge queue of an AWS CLB from CloudWatch metrics?\nTo estimate the average time a TCP request waits in the surge queue of an Amazon Web Services (AWS) Classic Load Balancer (CLB), you can use the SurgeQueueLength and RequestCount metrics from Amazon CloudWatch.\nSurgeQueueLength is the number of requests in the surge queue, and RequestCount is the total number of requests received by the load balancer.\n\nFirst, get the average SurgeQueueLength for a specific time period. To do this, use the avg statistic for the SurgeQueueLength metric in CloudWatch.\n\nNamespace: AWS/ELB\nMetricName: SurgeQueueLength\nStatistic: Average\nDimensions: LoadBalancerName=&lt;your_load_balancer_name&gt;\n\n\nNext, get the total RequestCount for the same time period. Use the sum statistic for the RequestCount metric in CloudWatch.\n\nNamespace: AWS/ELB\nMetricName: RequestCount\nStatistic: Sum\nDimensions: LoadBalancerName=&lt;your_load_balancer_name&gt;\n\n\nDivide the average SurgeQueueLength by the total RequestCount to get the average time a TCP request waits in the surge queue.\n\nAverageTimeInSurgeQueue = AverageSurgeQueueLength / TotalRequestCount\n\nKeep in mind that this is an estimate, and the actual time a request spends in the surge queue may vary depending on factors such as request rate, backend instance performance, and load balancer configuration.\nAdditionally, you may want to monitor the SpilloverCount metric, which represents the number of requests that have been rejected due to the surge queue being full. A high SpilloverCount could indicate that the load balancer is struggling to handle incoming traffic, and you may need to take action, such as adding more backend instances or increasing the surge queue size.\navg statistics of SurgeQueueLength is averaged across individual load balancer nodes. Should the SampleCount statistics be used to calculate the total length of surge queue?\nYes, you’re correct. Since the avg statistic for SurgeQueueLength is an average value across individual load balancer nodes, it’s not ideal for calculating the total length of the surge queue. Instead, you can use the SampleCount statistic, which gives you the number of data points used for the computation of the average.\nTo calculate the total length of the surge queue for the desired time period, you can multiply the avg statistic by the SampleCount:\nTotalSurgeQueueLength = AverageSurgeQueueLength * SampleCount\n\nNow you can use the TotalSurgeQueueLength to estimate the average time a TCP request waits in the surge queue:\nAverageTimeInSurgeQueue = TotalSurgeQueueLength / TotalRequestCount\n\nThis approach will give you a more accurate estimation of the average time a TCP request waits in the surge queue of an AWS Classic Load Balancer.\nCould you explain the theory behind this formula?\nCertainly! The formula I provided earlier is based on the Little’s Law, which is a principle used in queuing theory. Little’s Law states that the average number of items in a queuing system (L) is equal to the average arrival rate of items (λ) multiplied by the average time an item spends in the system (W):\nL = λ * W\n\nIn the context of an AWS Classic Load Balancer (CLB), we can apply Little’s Law to estimate the average time a TCP request waits in the surge queue:\n\nL: Total length of the surge queue (TotalSurgeQueueLength)\nλ: Arrival rate of TCP requests (RequestCount)\nW: Average time a TCP request waits in the surge queue (AverageTimeInSurgeQueue)\n\nBy rearranging Little’s Law, we can calculate the average waiting time (W):\nAverageTimeInSurgeQueue (W) = TotalSurgeQueueLength (L) / TotalRequestCount (λ)\n\nTo obtain TotalSurgeQueueLength, we need to consider the AverageSurgeQueueLength and SampleCount from CloudWatch metrics, as discussed earlier:\nTotalSurgeQueueLength = AverageSurgeQueueLength * SampleCount\n\nHere, AverageSurgeQueueLength represents the average number of requests in the surge queue across all load balancer nodes during a specific time period, and SampleCount is the number of data points used for computing the average.\nBy using the modified Little’s Law formula, we can estimate the average time a TCP request waits in the surge queue of an AWS CLB, taking into account the queuing behavior and request arrival rate. However, keep in mind that this is still an approximation and may not precisely represent the actual waiting time for every request."},"notes/AI/LLM-Models":{"title":"LLM Models","links":[],"tags":[],"content":"Open source models\nChat\n\nPhi-4\nQwen-2.5\n\nReasoning\n\nQwQ\n\nCommercial models from OpenAI\nChat\n\nGPT-4o\n\nReasoning\n\no3-mini\no1\n"},"notes/AI/New-Bing":{"title":"New Bing","links":[],"tags":[],"content":"Based on GPT-4, New Bing is free to use but has a limit of 5 turns per session for unregistered users.\nYou can use New Bing from the Edge browser or via the Bing app on mobile."},"notes/AI/Poe":{"title":"Poe","links":["notes/AI/ChatGPT"],"tags":[],"content":"poe.com/\nPoe is a platform that enables users to access third-party chatbots, including ChatGPT.\nPoe can give suggestions on further questions the user could ask, and highlights terms that could be explained further by clicking on them.\nWithout a subscription, message speed and quality may be reduced."},"notes/Apple-Music":{"title":"Apple Music","links":[],"tags":[],"content":"List of loved songs\nExport “Apple Media Services information” from privacy.apple.com/ to get a list of loved songs in CSV format.\nAlternatively, create a Smart Playlist on your Mac, but it could only find songs in your library."},"notes/CLI/Bash/Best-way-to-add-path-to-PATH-environment-variable":{"title":"Best way to add path to PATH environment variable","links":[],"tags":[],"content":"Solution\nif [[ ! &quot;$PATH&quot; == *your_path* ]]; then\n  PATH=&quot;${PATH:+${PATH}:}your_path&quot;\nfi\nExplanation\n[[ ! &quot;$PATH&quot; == *your_path* ]] checks if PATH contains your new path already, hence avoiding duplicates. This doesn’t work well for nested bin directories, but that rarely happens.\n${parameter:+word} uses word if parameter is null or unset. Therefore, if PATH is unset or empty, the new PATH contains only your_path and doesn’t contain a leading :.\nReferences\n\nbash(1)\n"},"notes/CLI/Bash/Here-Document":{"title":"Here Document","links":[],"tags":[],"content":"Parameter expansion\nQuote the word in here-documents to inhibit any expansion.\n&lt;&lt;&#039;word&#039;\n\n\nIf any part of word is quoted, the delimiter is the result of quote removal on word, and the lines in the here-document are not expanded.\tIf word is unquoted, all lines of the here-document are subjected to parameter expansion, command substitution, and arithmetic expansion, the character sequence \\&lt;newline&gt; is ignored, and \\ must be used to quote the characters \\, $, and `.\n"},"notes/CLI/Bash/Line-Editing":{"title":"Line Editing","links":[],"tags":[],"content":"Read man 3 readline for more information."},"notes/CLI/Bash/Read-a-block-of-text-into-a-variable":{"title":"Read a block of text into a variable","links":[],"tags":[],"content":"read -r -d &#039;&#039; VAR1 &lt;&lt; EOV\nlorum\nipsum\nEOV\nTo prevent expansions, quote EOV into &#039;EOV&#039;."},"notes/CLI/Convenient-CLI-Tools":{"title":"Convenient CLI Tools","links":["notes/CLI/namei","notes/Software/Zellij"],"tags":[],"content":"\nnamei (directory permission on path)\ndoggo (DNS resolution)\nxhs, xh (HTTP/S request)\nPARI/GP (simple calculator)\n\nprintf(&quot;0x%X&quot;, %): print latest computed object in hexadecimal notation.\n\n\nnumbat (calculator with support for units) Try online!\n\nwarning: numbat persists command history in data_dir()/numbat/history.\n\n\nZellij (tmux)\neza, lsd (ls)\nfd, bfs (find)\nrg (grep)\npv (pipe viewer)\ndug (DNS propagation checker)\ncroc and ffsend (share files securely)\nage (simple file encryption tool)\nrestic (file backup tool)\nrclone (rsync for cloud)\nnamei (check parent directory permissions)\nncdu (TUI disk usage analyzer)\nlft (Layer Four Traceroute)\n"},"notes/CLI/GNU-Sed":{"title":"GNU Sed","links":[],"tags":[],"content":"Append line after match\n\\ (backslash followed by spaces) starts the appended line with all space characters that follows the \\. Without it, sed removes all spaces from the line appended.\nsed -i &#039;/files-2.1/a \\                      caches/modules-2/files-2.1/idea&#039; *.yml"},"notes/CLI/GraphicsMagick":{"title":"GraphicsMagick","links":[],"tags":[],"content":"# Make thumbnail of an image\ngm convert -size 120x120 a.jpg -resize 120x120 +profile &quot;*&quot; thumbnail.jpg\n \n# Report image info\ngm identify -verbose a.jpg"},"notes/CLI/Neovim":{"title":"Neovim","links":[],"tags":[],"content":"History\nNeovim uses shada files located in $HOME/.local/state/nvim/shada instead of .viminfo."},"notes/CLI/Per,-AWK-and-Sed-Tricks-for-Text-File-Manipulation":{"title":"Per, AWK and Sed Tricks for Text File Manipulation","links":[],"tags":[],"content":"Selecting a paragraph with first line known\n/^first line content$/,/^$/ works in both AWK and sed for addressing a paragraph, or any content with unique start and end identifiers.\nsed -i -e &#039;/^Package: angie-module-wasmtime$/,/^$/d&#039; debian/control\nPerl paragraph mode\n-00 is a special value that cause Perl to slurp files in paragraph mode. This is very useful if you need to match text in the middle of a paragraph, which is more difficult for a line-oriented editor like sed.\nperl -i -00 -ne &#039;print unless /wasmtime/&#039; debian/rules\nIn AWK, you could use -v RS= to read inputs in paragraph, but you also need -v ORS=&#039;\\n\\n&#039; to set the output separator.\nNote that both Perl paragraph mode and AWK’s -v RS= rewrites the output separators, so if your file has more than three consecutive newlines, they are merged into an empty line, that is, two newlines."},"notes/CLI/Vim":{"title":"Vim","links":[],"tags":[],"content":"Text objects\n\nThe commands that start with “a” select “a”n object including white space, the commands starting with “i” select an “inner” object without white space, or just the white space. Thus the “inner” commands always select less text than the “a” commands.\n\n\nas: “a sentence”\nis: “inner sentence”\nap: “a paragraph”\nip: “inner paragraph”\n\nAlso w and W for words, t for HTML tags, and any pair of symbols: (&lt;{&quot;&#039;[.\nMacro tricks\nUse va{V to select the outer block line-wise.\nScrolling\nz., zt, zb move &quot;cursor line&quot; into respective position\nH, L, M move cursor to respective position\n\nMarks\nm followed by a letter sets a mark. a lowercase letter is local to the current buffer, but an uppercase letter can jump between files.\n&#039; or `  followed by a letter jumps to the respective mark.\nRemoving auto-inserted comment leader\nAfter pressing Enter or ‘o’, use CTRL-U to remove the comment leader. See :help fo-table.\no       Automatically insert the current comment leader after hitting &#039;o&#039; or\n        &#039;O&#039; in Normal mode.  In case comment is unwanted in a specific place\n        use CTRL-U to quickly delete it. i_CTRL-U\n\nVisual-block edits\nTips: use gv to re-select the previous area in the same mode.\nWith a blockwise selection, A{string} appends the string to end of block on every line of the block, I{string} inserts the string at the start of block on every line of the block.\nTo paste lines of text to the end of a blockwise selection, first make a blockwise selection of the text to copy and yank it, then insert a placeholder character after the end of block on each line, select the placeholder row and press p or P. Both would delete the placeholder column (end of block) and insert text there."},"notes/CLI/Zsh-Word-Splitting":{"title":"Zsh Word Splitting","links":[],"tags":[],"content":"Use $=VAR instead of $VAR to force parameter expansions to split $VAR into separate words before substitution, using the IFS as a delimiter.\nThis is the default in most other shells."},"notes/CLI/Zsh":{"title":"Zsh","links":[],"tags":[],"content":"New user setup\n\nYou should not edit anything between these lines if you intend to run zsh-newuser-install again.  You may, however, edit any other part of the file.\n\n# Lines configured by zsh-newuser-install\nHISTFILE=~/.histfile\nHISTSIZE=1000\nSAVEHIST=1000\nbindkey -e\n# End of lines configured by zsh-newuser-install\n# The following lines were added by compinstall\nzstyle :compinstall filename &#039;/mnt/home/l2dy/.zshrc&#039;\n \nautoload -Uz compinit\ncompinit\n# End of lines added by compinstall\nRun applications in background\nUse &amp;| to run job in background and remove it from the job table in one command.\nIn Fish and Bash, you have to use two commands. For example, firefox &amp;; disown, where disown removes the job from the job table."},"notes/CLI/apt":{"title":"apt","links":[],"tags":[],"content":"Find reverse dependencies\napt-cache rdepends --installed --recurse &lt;package&gt;\nList packages by prefix\napt search --names-only &#039;^unit-&#039;\n"},"notes/CLI/benchstat":{"title":"benchstat","links":[],"tags":[],"content":"Compare go test -bench=... results and print nice statistics.\npkg.go.dev/golang.org/x/perf@v0.0.0-20211012211434-03971e389cd3/cmd/benchstat\n$ benchstat old.txt new.txt\nname        old time/op  new time/op  delta\nGobEncode   13.6ms ± 1%  11.8ms ± 1%  -13.31% (p=0.016 n=4+5)\nJSONEncode  32.1ms ± 1%  31.8ms ± 1%     ~    (p=0.286 n=4+5)\n"},"notes/CLI/col":{"title":"col","links":[],"tags":[],"content":"Convert tabs to spaces\nethtool -l eth0 | col -x converts tabs to spaces with proper indentation."},"notes/CLI/curl":{"title":"curl","links":[],"tags":[],"content":"—connect-to\nSyntax: --connect-to &lt;HOST1:PORT1:HOST2:PORT2&gt;\n\n“HOST1” and “PORT1” may be empty, meaning “match any host/port.” “HOST2” and “PORT2” may also be empty, meaning “use the request’s original host/port.”\nAdded in curl 7.49.0.\nExample: curl --connect-to ::&lt;IP&gt; &lt;URL&gt;\n\nCompared to --resolve, it can adapt to the request’s original port."},"notes/CLI/dig":{"title":"dig","links":[],"tags":[],"content":".digrc\n+noall +answer\n"},"notes/CLI/du":{"title":"du","links":[],"tags":[],"content":"Linux-specific flags\nList top 10 directories with most files (inodes) in the root filesystem.\ndu --inodes -xS / | sort -nr | head\n \n# Less efficient version:\nfind / -xdev -printf &#039;%h\\n&#039; | sort | uniq -c | sort -k1 -n"},"notes/CLI/ffmpeg":{"title":"ffmpeg","links":[],"tags":[],"content":"One-liners\nDetect encoding errors\nffmpeg -err_detect explode -i &lt;infile&gt; -f null -\nConvert music file to Opus format\nfind . -type f -name &#039;*.flac&#039; | parallel &#039;test -f {.}.opus || ffmpeg -i {} -vn -c:a libopus -b:a 192k {.}.opus&#039;\nKeep display aspect ratio when downsampling\nffmpeg -i in.mp4 -vf &quot;setsar=&#039;if(sar,sar,1)&#039;,scale=640x480&quot; out.mp4"},"notes/CLI/git/Git-Partial-Clone":{"title":"Git Partial Clone","links":[],"tags":[],"content":"Blobless clone\ngit clone --filter=blob:none &lt;url&gt; creates a blobless clone. These clones download all reachable commits and trees while fetching blobs on-demand. These clones are best for developers and build environments that span multiple builds.\nGit downloads blobs on demand when you do a git checkout. This includes the first checkout inside the git clone operation.\nblobless clones can perform commands like git merge-base, git log, or even git log -- &lt;path&gt; with the same performance as a full clone.\nCommands like git diff or git blame &lt;path&gt; require the contents of the paths to compute diffs, so these commands trigger blob downloads. However, the good news is that after the first run you have those blobs in your local repository and don’t need to download them again. Most developers only need to run git blame on a small number of files, so this tradeoff of a slightly slower git blame command is worth the faster clone and fetch times.\nOn the other hand, git log -p without a path constraint is risky and could fetch all the blobs in history.\nReferences\n\ngithub.blog/2020-12-21-get-up-to-speed-with-partial-clone-and-shallow-clone/\n"},"notes/CLI/git/git-filter-repo":{"title":"git-filter-repo","links":[],"tags":[],"content":"Remove matching files from a branch\ngit filter-repo --refs &lt;branch&gt; --invert-paths --path-glob &#039;*.res.js&#039;"},"notes/CLI/git/git-status":{"title":"git-status","links":[],"tags":[],"content":"git status taking too long\nMay affect interactivity of custom shell prompts like Starship that check git status.\nTo reduce prompt latency, turn off scanning of untracked files in big repositories.\ngit config status.showUntrackedFiles no\ngit status during a interactive rebase\n“Last commands done” is tail -n 2 of .git/rebase-merge-done, which is an append-only file during a rebase.\nIn case of a merge conflict, the last line of the “done” file is the commit applied that caused the conflict."},"notes/CLI/grep":{"title":"grep","links":[],"tags":[],"content":"You can specify multiple patterns via -e. This is very helpful when you need to grep command output for something and preserve the header.\nps aux | grep -e TLS -e crypto\n"},"notes/CLI/ip":{"title":"ip","links":[],"tags":[],"content":"Global options\n-s for more verbose output (s for statistics). -s -s increases verbosity. Add -h for human readable stats and add -iec to print data size in IEC units (e.g. 1Ki = 1024).\nNote: don’t stack options. For example, ip complains if you attempt to use -hs:\nOption &quot;-hs&quot; is unknown, try &quot;ip -help&quot;.\n"},"notes/CLI/journalctl":{"title":"journalctl","links":[],"tags":[],"content":"Useful options\n-b (show logs from the current boot only)\n-e (jump to end in pager mode)\n-x (augment log lines with catalog info, don&#039;t use for bug reports)\n"},"notes/CLI/less":{"title":"less","links":[],"tags":[],"content":"Keys\n\nu / d: move backward/forward one half-window.\nb / f: move backward/forward one window.\nESC u: toggle search highlighting.\nn / N: repeat last search in normal/reverse direction. Add ESC prefix to search spanning files.\n:n / :p: examine the next/previous file.\n:e [file]: examine a new file.\n\nDon’t clear the screen on exit\n       -X or --no-init\n\t      Disables sending the termcap initialization and deinitialization\n\t      strings to the terminal.\tThis is  sometimes  desirable  if  the\n\t      deinitialization\tstring does something unnecessary, like clear-\n\t      ing the screen.\n\nUsually used with -F for direct output in one screen, and -R to show colors.\n       -F or --quit-if-one-screen\n\t      Causes less to automatically exit if the entire file can be\n\t      displayed on the first screen.\n"},"notes/CLI/lsof":{"title":"lsof","links":["notes/CLI/procstat"],"tags":[],"content":"List open files and sockets. On FreeBSD, use procstat instead."},"notes/CLI/namei":{"title":"namei","links":[],"tags":[],"content":"namei -l &lt;path&gt; displays Unix permissions of each path segment and prints their modes and owners. This is very useful when troubleshooting file permission issues."},"notes/CLI/nstat":{"title":"nstat","links":[],"tags":[],"content":"nstat is part of the iproute2 collection, replacing the old netstat --statistics command."},"notes/CLI/procstat":{"title":"procstat","links":["maps/FreeBSD-Map"],"tags":[],"content":"On FreeBSD Map, use procstat -f &lt;PID&gt; to list open files and sockets."},"notes/CLI/ps":{"title":"ps","links":[],"tags":[],"content":"List processes by user\nShow every process running as root in user format:\nps -U root -u root u\n\n-U is for real ID and -u is for effective ID.\nTo show numeric id in the USER column, add n.\nList processes by PIDs\nShow PID 1 and 2 processes in virtual memory format:\nps -q 1,2 v\n\nProcess tree on FreeBSD\n-d arranges processes into a tree representing parent and child relationships.\nps aux -d"},"notes/CLI/rsync":{"title":"rsync","links":[],"tags":[],"content":"Only copy certain types of files\n(dry-run)\nrsync -rvn --include=&quot;*/&quot; --include=&quot;*.wav&quot; --exclude=&quot;*&quot; from/ to/\n"},"notes/CLI/snmpwalk":{"title":"snmpwalk","links":[],"tags":[],"content":"Perl\nperl -pi.bak -e &#039;$from=&#039;\\&#039;&#039;fixed string&#039;\\&#039;&#039;; $to=&#039;\\&#039;&#039;replacement string&#039;\\&#039;&#039;; s/\\Q$from\\E/$to/g&#039; &lt;file&gt;\n \n# or\n \nperl -s -pi.bak -e &#039;s/\\Q$from\\E/$to/g&#039; -- -from=&#039;fixed string&#039; -to=&#039;replacement string&#039; files/mini_snmpd.init\nperldoc perlrun says that with -s, Perl does rudimentary switch parsing for switches before an argument of -- or any filename arguments like files/mini_snmpd.init. But in practice, you have to add -- before the from and to arguments in this case."},"notes/CLI/ss":{"title":"ss","links":[],"tags":[],"content":"ss -ntl sport 443 # list sockets lisenting on port 443\nss -ntl src :443 # same as above\n"},"notes/CLI/starship":{"title":"starship","links":[],"tags":[],"content":"Configuration\nSet format to skip slow modules, instead of disabling them individually. For example, I’m using\n# ~/.config/starship.toml\nformat = &#039;$username$hostname$directory$git_branch$git_commit$git_state$git_metrics$git_status$hg_branch$character&#039;"},"notes/CLI/systemd-analyze":{"title":"systemd-analyze","links":[],"tags":[],"content":"Subcommands\n\ntime: analyze startup time\nset-log-level: set systemd log level\n"},"notes/CLI/top":{"title":"top","links":[],"tags":[],"content":"Keys\n\ne / E, :Enforce-Task-Memory-Scale and :Enforce-Summary-Memory-Scale. Cycle through the available memory scaling which ranges from KiB through EiB.\nf, :Fields-Management. Display a separate screen to change which fields to display, ordering and the sort field.\no, :Other-Filtering. With COMMAND=gcc you can filter for gcc processes.\n\nCommand-line options\n\n-u &lt;user&gt;, :User-filter-mode. Display only processes with a user id or user name matching that given.\n"},"notes/Cloud/AWS/AWS-Backbone-Network":{"title":"AWS Backbone Network","links":[],"tags":[],"content":"Regions\nEach Region provides AWS services independently.\nAvailability Zones\nAZs are less than 2 ms apart and highly connected with redundant paths inter-AZ and to transit centers.\n\nTransit Center\nTransit centers handle connections to other AWS Regions, Direct Connect private links, and Internet through peering and paid transit.\nReferences\n\nwww.youtube.com/watch\n"},"notes/Cloud/AWS/AWS-Documentation-Excerpts":{"title":"AWS Documentation Excerpts","links":[],"tags":[],"content":"Aurora MySQL\nJDBC drivers\nAmazon Web Services (AWS) JDBC Driver for MySQL\ngithub.com/awslabs/aws-mysql-jdbc\n\nThe AWS JDBC Driver for MySQL supports fast failover for Amazon Aurora with MySQL compatibility.\n\nCaveats and gotchas\nrepost.aws/knowledge-center/aurora-mysql-db-cluser-read-only-error\nSmart driver\nThe Amazon Aurora DB cluster endpoints propagate DNS record updates automatically, but the process doesn’t happen instantly. This can cause delays in responding to an event that occurred on the database and the event might be handled by the application. A Smart Driver uses the DB cluster topography through the INFORMATION_SCHEMA.REPLICA_HOST_STATUS metadata table, which is in near-real-time. This helps to route connections to the appropriate role, and helps load-balance across the existing replicas. MariaDB Connector/J is an example of a third party Smart Driver that has native support for Aurora MySQL.\nNote: even Smart Drivers might be affected by excessive DNS Caching.\nDNS caching\nIf you are not using a smart driver, then you depend on the DNS record updates and propagation after a failover event occurs. Aurora DNS zones use a short time-to-live (TTL) of 5 seconds, so it is important that your network and client configurations don’t further increase this. DNS caching can occur at multiple layers of an architecture, such as the operating system (OS), the network layer and the application container. It is important that you understand how each of these layers is configured. If there is unintended DNS caching beyond the TTL of 5 seconds, it is possible that you will re-connect to the old writer after a failover.\nJava Virtual Machines (JVM) can excessively cache DNS, indefinitely. When the JVM resolves a hostname to an IP address, it caches the IP address for a specified period of time (TTL). On some configurations, the JVM default TTL is set to never refresh DNS entries until the JVM is restarted. This can lead to read-only errors after a failover. In this case, it is important to manually set a small TTL so that it will periodically refresh.\nInterface Endpoint\nPrivate DNS for interface endpoints\nIf the private DNS option is enabled, an interface endpoint is created with a hidden private hosted zone attached to the associated VPC. Records in this hosted zone point the default service domain to ENIs of the interface endpoint, which have private IPs that isolated subnets can connect to.\nDetaching a private hosted zone can take a few minutes, so it is not possible to recreate a deleted AWS service interface endpoint immediately.\nprivate-dns-enabled cannot be set because there is already a conflicting DNS domain for ssm.&lt;region&gt;.amazonaws.com in the VPC\n\nCloud9\nAWS managed temporary credentials\nSome Cloud9, IAM and STS actions are restricted. See Actions supported by AWS managed temporary credentials.\n\n\nCurrently, if your environment’s EC2 instance is launched into a private subnet, you can’t use AWS managed temporary credentials to allow the EC2 environment to access an AWS service on behalf of an AWS entity (an IAM user, for example).\n\ndocs.aws.amazon.com/cloud9/latest/user-guide/security-iam.html#auth-and-access-control-temporary-managed-credentials\nGraviton processor\nAWS Graviton Technical Guide\nProcessor specifications\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProcessorGraviton2Graviton3(E)InstancesM6g/M6gd, C6g/C6gd/C6gn, R6g/R6gd, T4g, X2gd, G5g, and I4g/Im4gn/Is4genC7g/C7gd/C7gn, M7g/M7gd, R7g/R7gd, and HPC7gCoreNeoverse-N1Neoverse-V1Frequency2500MHz2600MHzTurbo supportedNoNoInstruction latenciesInstruction LatenciesInstruction LatenciesInterconnectCMN-600CMN-650Architecture revisionARMv8.2-aARMv8.4-aAdditional  featuresfp16, rcpc, dotprod, cryptosve, rng, bf16, int8, cryptoRecommended -mcpu flagneoverse-n1neoverse-512tvbRNG InstructionsNoYesSIMD instructions2x Neon 128bit vectors4x Neon 128bit vectors / 2x SVE 256bitLSE (atomic mem operations)yesyesPointer AuthenticationnoyesCores6464L1 cache (per core)64KB inst / 64KB data64KB inst / 64KB dataL2 cache (per core)1MB1MBLLC (shared)32MB32MBDRAM8x DDR48x DDR5DDR Encryptionyesyes\nC/C++ on Graviton\nSource\nOptimal processor features\nOn arm64 -mcpu= acts as both specifying the appropriate\narchitecture and tuning and it’s generally better to use that vs -march if\nyou’re building for a specific CPU.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPUFlagGCC versionLLVM verisonGraviton2-mcpu=neoverse-n1*GCC-9^Clang/LLVM 10+Graviton3(E)-mcpu=neoverse-512tvb%GCC 11+Clang/LLVM 14+\n^ Also present in Amazon Linux2 GCC-7\n* Requires GCC-9 or later; otherwise we suggest using -mcpu=cortex-a72\n% If your compiler doesn’t support neoverse-512tvb, please use the Graviton2 tuning.\nUsing SVE\nThe scalable vector extensions (SVE) require both a new enough tool-chain to\nauto-vectorize to SVE (GCC 11+, LLVM 14+) and a 4.15+ kernel that supports SVE.\nOne notable exception is that Amazon Linux 2 with a 4.14 kernel doesn’t support SVE;\nplease upgrade to a 5.4+ AL2 kernel.\nIAM\nPolicy limits\naws.amazon.com/premiumsupport/knowledge-center/iam-increase-policy-size/\nManaged policy limit\nYou can assign IAM users to up to 10 groups. You can also attach up to 10 managed policies to each group, for a maximum of 120 policies (20 managed policies attached to the IAM user, 10 IAM groups, with 10 policies each).\nInline policy character quota\nYou can add as many inline policies as you want to an IAM user, role, or group. But the total aggregate policy size (the sum size of all inline policies) per entity cannot exceed the following quotas:\n\nUser policy size cannot exceed 2,048 characters.\nRole policy size cannot exceed 10,240 characters.\nGroup policy size cannot exceed 5,120 characters.\n\nS3\nPartitioning\naws.amazon.com/premiumsupport/knowledge-center/s3-prefix-nested-folders-difference/\nA prefix is the complete path in front of the object name, which includes the bucket name. For example, if an object (123.txt) is stored as BucketName/Project/WordFiles/123.txt, the prefix is “BucketName/Project/WordFiles/”. If the 123.txt file is saved in a bucket without a specified path, the prefix value is “BucketName/“.\nA partitioned prefix in a bucket can support 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second. There is no limit to the number of prefixes you can have in a bucket.\nNote: in Amazon S3, there are no partitions for keys or objects. Partitions exist only at the prefix level, and not at the object level. For more information about using prefixes in Amazon S3. See Organizing objects using prefixes."},"notes/Cloud/AWS/AWS-Nitro-System":{"title":"AWS Nitro System","links":[],"tags":[],"content":"Nitro Controller\nPacket encapsulation\nNitro cards handle packet encapsulation and decapsulation on VM hosts.\nOn receipt of an encapsulated packet, Nitro checks if the source is valid for the encapsulated VPC packet. If not, Nitro drops the packet and triggers an alarm internally in AWS.\nARP interception\nNitro cards intercept ARP requests, and handle destinations within and outside of the subnet differently.\n\nFor EC2 instances communicating in the same subnet, ARP requests from one VM will get the MAC address of the other VM’s ENI.\nFor EC2 instances sending packets to the gateway (across subnets), ARP returns a “fake” address for the gateway. During decapsulation, packets from a different subnet has its source and destination MAC addresses rewritten to “fake” addresses of the respective gateways.\n"},"notes/Cloud/AWS/AWS-Pitfalls":{"title":"AWS Pitfalls","links":["notes/Network/DNS-Caching"],"tags":[],"content":"Security\nIAM\nNumber of policies attached to an IAM role or user and the size of each policy is limited. With workarounds, at most 120 managed policies and a set of inline policies can be added.\nCloudTrail\n\nCertain types of CloudTrail events aren’t associated with related resources. In this case, filter by event names to reduce number of events to skim through.\nEvents may take several minutes to show up in CloudTrail. Be patient.\n\nNetworking\nAvailability Zones\nAvailability Zone names don’t map to the same location across accounts. Only AZ IDs from Resource Access Manager (RAM) uniquely identify Availability Zones.\nNAT Gateway\nIf a connection that’s using a NAT gateway is idle for 350 seconds or more, the connection times out.\nInterface Endpoint\nDetach of a private hosted zone in a VPC (e.g. ssm.&lt;region&gt;.amazonaws.com) can take a few minutes, so it is not possible to recover a deleted AWS service interface endpoint immediately.\nNetwork Load Balancer\n\nYou cannot change the health check interval for a target group with the TCP protocol.\nRegistration and de-registration of a target in a Network Load Balancer is expected to take between 90 and 180 seconds to complete.\nTarget security groups: NLB does not have associated security groups, so firewall rules should be configured directly on the target instances.\nYou can created at most 20 NLBs per region by default.\n\nPaaS\nAurora MySQL\nTo improve reconciliation of cluster topology changes, use a smart driver like the AWS JDBC Driver and know your DNS Caching behavior.\nRDS Proxy\nYou can’t use RDS Proxy with custom DNS.\nEMR\n\nAn Amazon EMR cluster with multiple primary nodes can reside only in one Availability Zone or subnet.\nIf any two primary nodes fail simultaneously, Amazon EMR can’t recover the cluster.\n\nECS\nIn the awsvpc network mode, each task receives its own ENI. These task ENIs aren’t given public IP addresses, so tasks must be launched in a private subnet to access the internet using a NAT gateway.\nSaaS\nS3\nEach partitioned prefix in a bucket can support 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second. Distribute your files and access pattern across prefixes to avoid this limit.\nCloud9\nSome Cloud9, IAM and STS actions are restricted with the default managed credentials. Managed credentials also does not work on EC2 instances in private subnets."},"notes/Cloud/AWS/AWS-VPC":{"title":"AWS VPC","links":["notes/Cloud/AWS/AWS-Nitro-System","notes/Cloud/Google-Cloud/Andromeda"],"tags":[],"content":"Addressing and routing\nPhysical addresses are separate from VPC addresses, to enable independent scaling of the virtual network and the physical network.\nA proprietary encapsulation format is used to carry VPC information to the destination. It includes 3 layers of headers: IP packet from the payload, VPC encapsulation, and IP on physical network. On VM hosts, encapsulation is handed by the AWS Nitro System.\nMulticast and broadcast routing are not supported in a regular VPC.\nMapping service\nThe mapping service is a distributed web service that handles mappings between (VPC ID, IP address) and physical destinations like target physical host IP.\nThere are two kinds of mappings, /32 host mappings and routes (CIDR mappings) for Blackfoot edge devices.\nMappings are cached, pushed out and pre-loaded to memory, and proactively invalidated when they change.\nMappings from the mapping service are 100% cached on the physical host, and the cache miss path isn’t implemented, that is, drop all cache misses, to ensure predictable performance (especially latency) and mitigate a bunch of availability concerns for the mapping service. This is the Preprogrammed Model described in Andromeda’s paper.\nFor EC2 instances communicating within the same subnet, the fake ARP response from VM hosts will return the actual but virtual MAC address of the other instance. Other packets are sent to the subnet gateway, for which a fake MAC address is assigned and VM host parses the IP header to find the destination for those packets.\nOn receipt of an encapsulated packet, VM hosts check if the source is valid, and an alert is triggered if invalid packets are found, which could be caused by a program error or an attack.\nFlow tracking\nNot all flows are tracked on VM hosts. See docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-connection-tracking.html.\nEdge gateway\nBlackfoot Edge devices handles VPC ingress and egress, including Internet traffic, Direct Connect, VPN and S3 &amp; DynamoDB Endpoints. It’s horizontally scaled, redundant, high available and stateless.\nBlackfoot encapsulates ingress traffic for VPC and decapsulates egress traffic. it also operates NAT in a stateless manner, mapping private and public IPs (EIPs) one-to-one.\nVPC services\nAWS exposes a set of supporting services within customer VPCs at well-known or reserved addresses. These services are traditionally exposed from the IPv4 link-local address range (169.254.0.0/16). For AWS Nitro System instances, AWS also provides these services using IPv6 ULAs.\nServices include\n\nInstance Metadata Service (IMDS)\nRoute 53 DNS resolver\nNetwork Time Protocol server\n\nVPC quotas\nEach VPC can have up to 256,000 NAU units (think of it as IP addresses in use), and the maximum number of NAU units for a VPC and all of its intra-Region peered VPCs is up to 512,000.\nPreviously, only around 50k NAU units are supported. It was likely because of the Preprogrammed Model in local mapping service cache.\nReferences\n\nwww.youtube.com/watch\nwww.youtube.com/watch\ncodezine.jp/article/detail/9790\ndocs.aws.amazon.com/whitepapers/latest/ipv6-on-aws/supporting-amazon-vpc-services.html\ndocs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html\n"},"notes/Cloud/AWS/Accessing-RHKB-via-AWS":{"title":"Accessing RHKB via AWS","links":[],"tags":[],"content":"Running an on-demand instance of Red Hat Enterprise Linux (RHEL) for at least 24 to 48 hours to get access to the Red Hat knowledge base via console.aws.amazon.com/entitlement/rhel/home."},"notes/Cloud/AWS/Amazon-Managed-Service-for-Prometheus":{"title":"Amazon Managed Service for Prometheus","links":[],"tags":[],"content":"Free Tier usage\n40 million metric samples can be ingested for free each month. Given a coefficient of 1.15 and 1 data point per minute, you should keep the number of series below 779.\n? 40000000/1.15/31/24/60\n%1 = 779.18\n"},"notes/Cloud/AWS/Application-Load-Balancer":{"title":"Application Load Balancer","links":[],"tags":[],"content":"Limitations\n\nELB IP address: up to 100\n\nReferences\n\nwww.youtube.com/watch\n"},"notes/Cloud/AWS/EC2-Performance-Benchmarks":{"title":"EC2 Performance Benchmarks","links":[],"tags":[],"content":"Single-core CPU performance\nruns-on.com/reference/benchmarks-ec2-instances/"},"notes/Cloud/AWS/Elastic-Network-Interface":{"title":"Elastic Network Interface","links":[],"tags":[],"content":"MAC Address\nAWS allocates locally administered MAC addresses to each ENI, which means that the second digit in the human-friendly form of MAC addresses is always 2, 6, A, or E.\nThese addresses aren’t globally unique. EC2 instances in different subnets can reuse the same MAC address."},"notes/Cloud/AWS/Hyperplane":{"title":"Hyperplane","links":["notes/Cloud/AWS/Network-Load-Balancer","notes/Network/Direct-Server-Return"],"tags":[],"content":"Architecture overview\nHyperplane is a platform that powers networking services like Network Load Balancer (NLB), NAT gateway (NATGW), PrivateLink, Gateway Load Balancer (GWLB), and Transit Gateway (TGW), and many other AWS services use it internally.\nOne AWS region has many Hyperplane clusters (a.k.a. Hyperplane cells) handling slices of workload. See Design Principles for details.\n\nNodes\nHyperplane nodes are just regular EC2 instances running in a VPC.\nFlow tracker\nFlow tracker is a horizontally scalable database that tracks all flows going through a Hyperplane cluster. It is custom built for flow tracking because no existing database solutions has the desired scale, latency and availability characteristics.\nThe data store maintains 1:1 redundancy of the flow state. When a nodes goes down in case of software deployments or failures, state is replicated to other nodes, repopulating the primary and secondary nodes for each connection involved.\nPacket processor\nPacket processor is a horizontally scalable layer that handles packet processing and forwarding.\nThe packet processing happens in user space.\nControl plane\nControl plane distributes configuration updates to flow trackers and packet processors, and manages Hyperplane capacity dynamically in response to change in demand by provisioning and de-provisioning EC2 nodes.\nContinuous testing\nAWS runs tests in each AZ continuously to monitor latency, availability and functional correctness.\nService monitoring\nHyperplane exports customer-facing metrics to CloudWatch via Kinesis.\nEstablishing a connection\nFirst, let’s zoom in and look at a more detailed architecture of Hyperplane. The packet processors are called Tops internally, and the flow trackers are composed of two components, Flow Masters and Deciders.\nTheir functionality is:\n\nTop layer handles packet forwarding and rewriting.\nFlow Master layer performs connection tracking.\nDecider layer chooses LB targets and assigns NAT source ports.\n\n\nWhen a client on the Internet tries to establish a TCP connection to a public NLB:\n\nThe client sends a TCP SYN packet to establish a connection with a NLB’s public IP address.\nThe packets ends up on a Blackfoot Edge fleet, which rewrites and forwards the packet to a particular Hyperplane cell.\nWhen packets arrives at Hyperplane, the flow is hashed to select a Top. Likely with ECMP or similar protocols.\nThe Top does not have state information about the connection yet, so it asks the primary Flow Master, selected by hashing, and runs through the backend selection process described below.\nThe Top saves connection state in memory, rewrites and delivers the packet to the target selected, keeping the source IP and port.\nThe target receives the packet, and its Nitro controller remembers to redirect packets to the client IP and port back to Hyperplane.\nThe target sends a SYN-ACK packet, which is routed to the same Hyperplane cell via Nitro and VPC networking.\nThe flow is again hashed to select a Top. However, the return path is likely going to hit a different set of nodes in all three layers because the hash ends up differently when you reverse the source and destination. AWS calls it hashing on an ordered flow basis.\nThe Top selected forwards the packet to the client, and the TCP connection is established after 3-way handshake completes.\n\nOnce the flow is established, all Tops involved remember the connection, so only the Top is involved in normal operation. In this mode, the performance penalty is on the orders of tens of microseconds.\nBackend selection\nPackets are chain-replicated between packets processors and flow tracker nodes, such that there is no buffering or retries when packets processors wait for a decision from flow trackers.\n\nThe chain also implements 1:1 redundancy. A pair of secondary decider and flow master is selected with a second set of hashes.\nDesign principles\nThe primary design goals are:\n\nCost efficiency (compared to fleets of commercial load balancers)\nScalability\nAvailability\nHigh throughput\nLong-lasting stateful connections (keep track of connections for years, e.g. for EFS connections)\nFault-tolerant\n\nConstant work\nThe system should be simple and consistent, especially when facing failures. There should be as few modes of operations as possible, to avoid combinatorial explosions of complexity.\nOne example is that cells have a fixed design capacity to test against. When scaling up, more cells are deployed instead of scaling up a particular cell.\nAnother example is the S3 Configuration Loop, as shown below.\nDesign for failures\nFail fast\nNo packet buffering or retries. Rely on TCP retransmission from the clients.\nSelf-healing\nPacket processors sends periodic heartbeats to flow trackers to update flow information, so that a failure in the redundancy of flow trackers can be recovered from and doesn’t break existing connections.\nIn many occasions, automation system can mitigate customer impact quickly in respond to an alarm.\nS3 configuration loop\n\nAWS Hyperplane nodes fetch, process and load configuration files from Amazon S3 every few seconds, even if nothing has changed.\nThe configuration file is sized to its maximum size right from the beginning to ensure the system is always processing and loading the maximum number of configuration changes.\nSuch that,\n\nThe control plane scales independently of the data plane fleet, and a storm of configuration changes does not overload the data plane.\nif AWS Hyperplane nodes are lost, the amount of work in the system goes down, not up.\n\nShuffle sharding\nAWS uses shuffle sharding for heat management, as shown in the diagram below.\n\nWith 100 nodes, there is only 2% chance for 5-node random selections to overlap more than 1 node. AWS also employs algorithmic measures to avoid overlap of more than 2 or half of the nodes between two tenants. This provides good isolation between tenants.\nCell-based\nHyperplane is a cellular and zonal service, which means that Hyperplane has its own control plane and data plane for each availability zone (AZ), and within each AZ Hyperplane operates a series of isolated cells, including a control plane for each cell. This limits blast radius and prevents single point of failure.\nPerformance\nAs of 2018, a single cell of Hyperplane can support terabits per second of throughput, hundreds of millions of connections, and tens of millions of transactions per second.\nSymmetric flows\nFlows have symmetric network path (i.e. no DSR) to enable collecting metrics from traffic in both directions on Hyperplane nodes. In case of NATGW, the packets have to be rewritten in both directions, so all packets has to go through Hyperplane.\nGoing through a Blackfoot or Hyperplane node have almost the same latency, so there is little negative impact.\nSecurity\nVPC mappings are abstracted away from Hyperplane nodes and the mappings are built and enforced in hardware instead. This protects Hyperplane from certain vulnerabilities like spoofing attack.\nCI/CD\nHyperplane’s development model is CI/CD. The CI/CD system extensively tests code as part of the deployment pipeline, and when the monitoring system detects a failure, rolls back the deployment automatically to the last know good version of that code.\nHistory\nBefore Hyperplane, AWS have also built a custom-made load balancer for S3. Hyperplane is based on the architecture of the S3 Load Balancer. S3 did not migrate to Hyperplane, so don’t worry about circular dependency here.\nThe chronological order of services built on top of Hyperplane is as follows:\n\nEFS (2015)\nNATGW (2016)\nNLB (2017)\nPrivateLink (2017)\nand more\n\nReferences\n\naws.amazon.com/builders-library/avoiding-overload-in-distributed-systems-by-putting-the-smaller-service-in-control/\naws.amazon.com/builders-library/reliability-and-constant-work/\nwww.youtube.com/watch (2017)\nwww.facebook.com/atscaleevents/videos/networking-scale-2018-load-balancing-at-hyperscale/2090077214598705/ (2018)\nAWS EC2N Hyperplane: A Deep Dive www.youtube.com/watch (2021)\n"},"notes/Cloud/AWS/Network-Load-Balancer":{"title":"Network Load Balancer","links":["notes/Cloud/AWS/Hyperplane"],"tags":[],"content":"Overview\nNetwork Load Balancer (NLB) is a L4 load balancer built on Hyperplane that is more elastic than other ELB products from AWS.\nClient IP preservation\nUnlike a typical load balancer that terminates incoming connections and initiates outbound connections to targets, Hyperplane operates in a transparent mode and the target resource sees IP information about the client directly from the packets received.\nThere are several considerations I want to highlight when client IP preservation is enabled:\n\nNAT loopback, also known as hairpinning, is not supported when client IP preservation is enabled. If an instance is a client of a load balancer that it’s registered with, and it has client IP preservation enabled, the connection succeeds only if the request is routed to a different instance.\nYou might encounter TCP/IP connection limitations related to observed socket reuse on the targets. These connection limitations can occur when a client, or a NAT device in front of the client, uses the same source IP address and source port when connecting to multiple load balancer nodes (e.g. client connects to one IP in each AZ) simultaneously. If the load balancer routes these connections to the same target, the connections appear to the target as if they come from the same source socket, which results in connection errors.\nWhen client IP preservation is enabled, targets must be in the same VPC as the Network Load Balancer, and traffic must flow directly from the Network Load Balancer to the target.\nClient IP preservation is not supported when a target group contains AWS PrivateLink ENIs, or the ENI of another Network Load Balancer. This will cause loss of communication to those targets.\nClient IP preservation can’t be disabled for instance and IP type target groups with UDP and TCP_UDP protocols.\n\nHealth checks\nNLB uses active and passive health checks to determine whether a target is available to handle requests. Other AWS load balancers does not support passive health checks, so this is a feature unique to NLB and Hyperplane, on a scale of tens of milliseconds for a failover to happen.\nFail open\nIf all targets fail health checks at the same time in all enabled Availability Zones, the load balancer fails open. The effect of the fail open is to allow traffic to all targets in all enabled Availability Zones, regardless of their health status.\nUDP\nUDP health checks are not supported. For a UDP service, target availability can be tested using non-UDP health checks on your target group.\nTCP RST packets\nIf no data is sent through a TCP connection by either the client or the target for longer than the idle timeout, the connection is closed. If a client or a target sends data after the idle timeout period elapses, it receives a TCP RST packet to indicate that the connection is no longer valid.\nThe idle timeout value is 350 seconds for TCP flows, and 120 seconds for UDP flows.\nAdditionally, if a target becomes unhealthy, the load balancer sends a TCP RST for packets received on the client connections associated with the target, unless the unhealthy target triggers the load balancer to fail open. The behavior can be switched off with the target_health_state.unhealthy.connection_termination.enabled target group attribute, but connections are still subject to idle timeout.\nNumber of TCP RST packets generated per NLB can be tracked with the TCP_ELB_Reset_Count metric.\nAvailability Zone isolation\nBy default, each load balancer node routes requests only to the healthy targets in its Availability Zone. If you enable cross-zone load balancing, each load balancer node routes requests to the healthy targets in all enabled Availability Zones. See Cross-zone load balancing for an example of traffic distribution comparison.\nAvailability Zone DNS affinity\nWhen using the default client routing policy, client connections are distributed across the load balancer AZs.\nIf you are using the Route 53 resolver, you can configure the percentage of zonal affinity, favoring load balancer IP addresses from the same AZ.\nIf you are not using the Route 53 resolver, you could still implement this in your application with AZ-specific DNS names [az].[name]-[id].elb.[region].amazonaws.com, for example:\nus-east-2b.my-load-balancer-1234567890abcdef.elb.us-east-2.amazonaws.com\nNote that when using Availability Zone DNS affinity, cross-zone load balancing should be turned off, otherwise traffic from load balancer nodes to targets are still\nReferences\n\ndocs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#client-ip-preservation\ndocs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html\n"},"notes/Cloud/Azure/Acceleration-Mechanisms-on-NVIDIA-ConnectX-5-NICs":{"title":"Acceleration Mechanisms on NVIDIA ConnectX-5 NICs","links":[],"tags":[],"content":"List of mechanisms with dedicated counters\n\nTSO (TCP Segmentation Offload) - increasing outbound throughput and reducing CPU utilization by allowing the kernel to buffer multiple packets in a single large buffer. The NIC split the buffer into packet and transmits it\nLRO (Large Receive Offload) - increasing inbound throughput and reducing CPU utilization by aggregation of o multiple incoming packet of a single stream to a single buffer\n\nThere is also GRO (Generic Receive Offload), which is considered less broken according to a blog post (2009).\n\n\nCHECKSUM (Checksum) – calculation of TCP checksum (by the NIC). The following CSUM offload are available (refer to skbuff.h for detailed explanation)\n\nCHECKSUM_UNNECESSARY\nCHECKSUM_NONE – no CSUM acceleration was used\nCHECKSUM_COMPLETE – Device provided CSUM on the entire packet\nCHECKSUM_PARTIAL – Device provided CSUM\n\n\nCQE Compress – compression of Completion Queue Events (CQE) used for sparing bandwidth on PCIe and hence achieve better performance.\n\nLRO\nRequirements\n\nLRO is only enabled in Striding RQ (a.k.a. Multi-Packet Rx Queue, or MPRQ)\nMinimal hardware version is ConnectX-5\n\nHow to enable Striding RQ\n# ethtool --set-priv-flags eno0 rx_striding_rq on\n# ethtool --show-priv-flags eno0\nPrivate flags for eno0:\n...\nrx_striding_rq : on\n... \nHow to enable LRO\nLRO on a new kernel can be configured using ethtool commands:\n\nTo check LRO config, run:\n\n# ethtool -k ens801f1 | grep large-receive-offload\nlarge-receive-offload: off\n\nTo enable LRO, run:\n\n# ethtool -K eth1 lro on\n\nTo disable LRO, run:\n\n# ethtool -K eth1 lro off\n\nTo verify LRO configuration, run:\n\n# ethtool -k ens801f1 | grep large-receive-offload\nlarge-receive-offload: on\nTSO\nRequirements\n\nMinimal hardware version is ConnectX-4\n\nHow to enable TSO\n\nTo check TSO config, run:\n\n# ethtool -k ens801f1 | grep tcp-segmentation-offload\ntcp-segmentation-offload: on\nFollowing steps are similar to LRO.\nethtool configuration persistence\nRHEL 9\n\nTo set persistent LRO configuration in the ens801f1 connection profile, run:\n\nnmcli con modify ens801f1 ethtool.feature-rx on\n\nTo remove persistent LRO configuration, run:\n\nnmcli con modify ens801f1 ethtool.feature-rx &quot;&quot;\nRHEL 8\nTo persist LRO configuration on RHEL 8, add the following to /etc/sysconfig/network-scripts/ifcfg-* (replace * with interface name):\nETHTOOL_OPTS=&quot;-K ${DEVICE} lro on&quot;\n\nReferences\n\ngithub.com/torvalds/linux/commit/6c3a823e1e9c645f30c5b03fefe87fea8881060b\ndoc.dpdk.org/guides/nics/mlx5.html\nwww.ibm.com/docs/en/linux-on-systems#d7439e143\nenterprise-support.nvidia.com/s/article/how-to-enable-large-receive-offload—lro-x\nenterprise-support.nvidia.com/s/article/understanding-mlx5-ethtool-counters\nwww.redhat.com/en/blog/rhel-9-networking-say-goodbye-ifcfg-files-and-hello-keyfiles\naccess.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/configuring-ip-networking-with-ifcfg-files_configuring-and-managing-networking\naccess.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_networking/configuring-ethtool-settings-in-networkmanager-connection-profiles_configuring-and-managing-networking\n"},"notes/Cloud/Azure/Ananta":{"title":"Ananta","links":["notes/Network/Direct-Server-Return","notes/Cloud/Azure/Azure-Load-Balancer"],"tags":[],"content":"Architecture\n\nEach instance of Ananta is comprised of three main components, Ananta Manager (AM), Multiplexer (Mux) and Host Agent (HA).\nA set of Annata instances could broadcast the same /21 prefix to reduce impact of single instance failures.\nAnanta Manager\nAnanta Manager (AM) implements the control plane of Ananta. It achieves HA using the Paxos consensus protocol with 5 replicas per instance.\nMux Pool\nAn instance of Ananta has one or more sets of Muxes called Mux Pools.\nLost or addition of Muxes in a Mux Pool will cause ongoing connections to be redistributed among the currently live Muxes based on router’s ECMP implementation. When this happens, these connections may get misdirected, subject to mapping entry changes.\nMux\nMux only handles incoming traffic. The other direction is handled by DSR.\nMux is responsible for receiving traffic for all configured VIPs from the router and forwarding it to appropriate DIPs.\nMux makes use of consistent hashing of the five-tuple (source IP, destination IP, IP Protocol, source port and destination port) to balance new flows across DIPs.\nMux maintains a mapping table, where each entry maps a VIP endpoint to a list of DIPs.\nEach Mux node has its own memory quota for trusted and untrusted flows. Once memory quota for trusted flow queue is exhausted, new flow states will not be created, and Mux falls back to using the hash to lookup DIP. This provides a degraded service, during which untracked connections may get misdirected if a change was made in the mapping entry (VIP, IP Protocol, port) → DIPs.\nDo not confuse untracked connections with stateless mapping entries. Stateless mapping entries are used for SNAT and is a stateless rule that maps a port on VIP to the requesting DIP, which can be reused for multiple connections, that is, port reuse.\nHost Agent\nHost Agent (HA) is responsible for Inbound NAT, Source NAT and checking health of DIPs.\n\nInbound NAT: decapsulate packets from Mux.\nSource NAT: See Outbound Rules (SNAT).\nDIP Health Monitoring: Run health probes for Azure Load Balancer and push status updates to AM, which relays them to all related Muxes.\n\nDesign Principles\nScale\nAnanta needs to configure HAs and Muxes during each VIP configuration change, and its programming time could be delayed due to data plane overload, network interruptions, and other real-world failure patterns. The resulting latency generally fits in Azure’s API SLAs, but in case of failure it’s prone to get worse.\nThe data plane relies on industry-standard ECMP at the routers to spread load across Muxes, and RSS at the NICs to spread load across multiple CPU cores.\nHandling Failures\nEach Mux is a BGP speaker. If a Mux fails or shuts down unexpectedly, the router detects the failure via the BGP protocol and automatically drops routes to that Mux."},"notes/Cloud/Azure/Azure-Accelerated-Networking":{"title":"Azure Accelerated Networking","links":["notes/Cloud/Azure/Virtual-Filtering-Platform","notes/Cloud/Azure/Receive-Segment-Coalescing","notes/Cloud/Azure/Microsoft-Azure-Network-Adapter"],"tags":["networking"],"content":"Overview\nBenefits\n\nMore deterministic performance, that is, reduced jitter and latency inconsistency\nDecreased CPU utilization\nHigher packets per second (pps) on the datapath\n\nLatency improvement is very minimal when VMs communicate across virtual networks or connect to on-premises and the Internet.\nLimitations\n\nThe Azure platform does not update the Mellanox NIC drivers in the VM. The driver is known to have bugs on old kernels.\n\nTechnical details\nAzure SmartNIC\nBased on FPGAs, AccelNet offloads packet forwarding to SmartNIC hardware with Generic Flow Tables. GFT is a match-action language that defines specific operations on packets for one specific network flow.\nIf GFT does not match a given packet, the SmartNIC hardware will send the packet to the software layer (VFP) as an Exception Packet, which are most common on the first packet of each flow. After the first packet of each flow, all forwarding can be offloaded, providing the full performance of a native SR-IOV hardware solution.\nServiceability\nAccelNet is transparent to user space applications via NetVSC service in VM, which switches I/O back to synthetic vNICs during a service event to maintain connectivity.\n\nSupported VM instances\nSee Linux VM sizes, where each “series” has its own page describing Accelerated Networking support among other information.\nAccelerated Networking is required and turned on by default on some series, including all v5 and above Intel processor general purpose sizes.\nOther networking acceleration mechanisms on Azure\n\nReceive Segment Coalescing\nThe next-generation MANA\n\nReferences\n\nwww.microsoft.com/en-us/research/uploads/prod/2018/03/Azure_SmartNIC_NSDI_2018.pdf\nlearn.microsoft.com/en-us/azure/site-recovery/azure-vm-disaster-recovery-with-accelerated-networking#supported-vm-instances-1\n"},"notes/Cloud/Azure/Azure-Blob-Storage":{"title":"Azure Blob Storage","links":[],"tags":[],"content":"Partitioning\n\nBlob storage uses a range-based partitioning scheme for scaling and load balancing. Each blob has a partition key comprised of the full blob name (account+container+blob). The partition key is used to partition blob data into ranges. The ranges are then load-balanced across Blob storage.\n\n\nIf your naming scheme uses timestamps or numerical identifiers, it can lead to excessive traffic to one partition. It prevents the system from effectively load balancing. For instance, if you have daily operations that use a blob object with a timestamp, such as yyyy-mm-dd, all the traffic for that operation goes to a single partition server. Instead, prefix the name with a three-digit hash.\n\nConsistency\n\nThe actions of writing a single block or page are atomic, but operations that span blocks, pages, or blobs aren’t. If you need to ensure consistency when write operations are performed across blocks, pages, and blobs, take out a write lock by using a blob lease.\n\nReferences\n\nlearn.microsoft.com/en-us/azure/well-architected/reliability/partition-data#partition-in-azure-blob-storage\nlearn.microsoft.com/en-us/azure/storage/blobs/storage-performance-checklist#partitioning\nsigops.org/s/conferences/sosp/2011/current/2011-Cascais/printable/11-calder.pdf\n"},"notes/Cloud/Azure/Azure-Linux-Guest-Agent":{"title":"Azure Linux Guest Agent","links":["notes/Cloud/Azure/Virtual-Filtering-Platform"],"tags":[],"content":"Conntrack\nWALinuxAgent enables connection tracking for a security feature. You may need to adjust Linux OS configuration to support a large number of connections. Check dmesg output if you faced connectivity issues on Azure VMs and have a lot of connections open. But if you have more than 500k connections, it could be a Azure networking stack limit from Virtual Filtering Platform.\nReferences\n\nlearn.microsoft.com/en-us/azure/aks/custom-node-configuration (net.netfilter.nf_conntrack_max)\ngithub.com/Azure/WALinuxAgent/blob/28345a55f9b21dae89472111635fd6e41809d958/azurelinuxagent/common/utils/networkutil.py#L186-L189\n"},"notes/Cloud/Azure/Azure-Load-Balancer":{"title":"Azure Load Balancer","links":["notes/Cloud/Azure/Ananta","notes/Network/Direct-Server-Return","notes/Network/Strong-and-Weak-Host-Model"],"tags":["networking"],"content":"Ananta\nAzure Load Balancer is based on the Ananta design.\nFastpath\nIntra-DC traffic seen on internal load balancers is mostly offloaded to end systems. Packets are delivered directly to the DIP, bypassing Mux in both directions, thereby enabling communication at full network capacity.\nDirect Server Return (DSR)\nSee Direct Server Return for comparison with other cloud providers.\nFaireness\nIf a flow attempts to steal more than its fair share of bandwidth, Mux starts to drop its packets with a probability directly proportional to the excess bandwidth it’s using.\nIf there is packet drop due to overload, Mux creates a black hole for the offending VIP by withdraw BGP advertisements, and traffic for the VIP may be routed to DoS protection services in several minutes.\nMultiple frontends\nAzure Load Balancer does not allow reusing backend ports by default. Each rule must produce flows with an exclusive combination of destination IP address and destination port. Multiple load balancing rules can deliver flows to the same backend instance IP on different ports by varying the destination port of the flow.\nIf you want to reuse a backend port across multiple rules, you must enable Floating IP in the load balancing rule definition. Floating IP is Azure’s terminology for a portion of what is known as Direct Server Return (DSR). Note that Floating IP for IPv6 doesn’t work for Internal Load Balancers. Floating IP also relies on the weak host model.\nLimitations\n\nLoad Balancer backend pool can’t consist of a Private Endpoint.\nOutbound flow from a backend VM to a frontend of an internal Load Balancer fails silently.\nA load balancer rule can’t span two virtual networks. All load balancer frontends and their backend instances must be in a single virtual network.\nYou can only have one NIC-based Public Load Balancer and one NIC-based internal Load Balancer per availability set. Note that this constraint doesn’t apply to IP-based load balancers.\n\nAvailability set is a special feature around fault domain placement and provides fault isolation to some degree.\n\n\n\nOutbound rules (SNAT)\nAnanta also implements a distributed NAT for outbound connections, besides processing inbound connections. This allows backend instances to use the public IPs of a load balancer to provide outbound internet connectivity.\nAM allocates an externally routable (VIP, port) tuple and configures each Mux in the associated Mux Pool with this allocation. This ensures that returning packets sent to Mux can be routed back to DIP.\n\nReferences\n\nlearn.microsoft.com/en-us/azure/load-balancer/load-balancer-multivip-overview\nlearn.microsoft.com/en-us/azure/load-balancer/outbound-rules\nconferences.sigcomm.org/sigcomm/2013/papers/sigcomm/p207.pdf\n"},"notes/Cloud/Azure/Azure-NAT-Gateway":{"title":"Azure NAT Gateway","links":[],"tags":[],"content":"Azure’s NAT gateway (NATGW)\n\nLimitations\nEach NAT gateway can provide up to 50 Gbps of throughput.\nA NAT gateway can support up to 50,000 concurrent connections per public IP address to the same destination endpoint over the internet for TCP and UDP. The NAT gateway can process 1M packets per second and scale up to 5M packets per second."},"notes/Cloud/Azure/Azure-Network-Idle-Timeout":{"title":"Azure Network Idle Timeout","links":[],"tags":[],"content":"\npublic IP: 4-30 minutes adjustable inbound, 4 minutes fixed outbound.\nLB: adjustable, 4-100 minutes. Default is 4 minutes.\n\nIf you set idle timeout to a large value, you risk reaching the VM connection limit. On the other hand, a very short idle time out could drop valid connections.\nFor SSH, Azure recommends ClientAliveInterval 180, which corresponds to 75% of default idle timeout, or 30-235 if there are special requirements.\nReferences\n\nlearn.microsoft.com/en-us/azure/load-balancer/load-balancer-tcp-idle-timeout\nlearn.microsoft.com/en-us/azure/virtual-network/ip-services/public-ip-addresses\nlearn.microsoft.com/en-us/azure/virtual-machines/linux/redhat-create-upload-vhd\nlearn.microsoft.com/en-us/partner-center/marketplace/azure-vm-certification-faq\n"},"notes/Cloud/Azure/Azure-VM-size-series-naming":{"title":"Azure VM size series naming","links":[],"tags":[],"content":"Dpsv6 is the base name of the Azure Cobalt 100 processor series.\n\nThe D family of VM sizes are one of Azure’s general purpose VM sizes. The standard general purpose VMs offer 4 GiB of memory per vCPU and come without local disk.\nLetter p represents the processor series. Here it’s Azure’s first-generation Cobalt 100 processor.\nLetter s represents Premium SSD or Ultra Disk storage support.\nv6 represents the sixth generation.\n\nDplsv6-series offer 2 GiB of memory per vCPU. l for less memory?\nDpdsv6-series offer local NVMe temporary storage. d for local disk?"},"notes/Cloud/Azure/Hyper-V-Guest-Features":{"title":"Hyper-V Guest Features","links":["notes/Cloud/Azure/Receive-Side-Scaling-on-Azure","notes/Cloud/Azure/Acceleration-Mechanisms-on-NVIDIA-ConnectX-5-NICs","notes/Cloud/Azure/Azure-Accelerated-Networking"],"tags":[],"content":"Networking\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureDescriptionJumbo framesWith this feature, an administrator can increase the size of network frames beyond 1500 bytes, which leads to a significant increase in network performance.VLAN tagging and trunkingThis feature allows you to configure virtual LAN (VLAN) traffic for virtual machines.Live MigrationWith this feature, you can migrate a virtual machine from one host to another host.Static IP InjectionWith this feature, you can replicate the static IP address of a virtual machine after it has been failed over to its replica on a different host. Such IP replication ensures that network workloads continue to work seamlessly after a failover event.vRSS (Virtual Receive Side Scaling)Spreads the load from a virtual network adapter across multiple virtual processors in a virtual machine. For more information, see Receive Side Scaling on Azure.TCP Segmentation and Checksum OffloadsTransfers segmentation and checksum work from the guest CPU to the host virtual switch or network adapter during network data transfers. For more information, see Acceleration Mechanisms on NVIDIA ConnectX-5 NICs.Large Receive Offload (LRO)Increases inbound throughput of high-bandwidth connections by aggregating multiple packets into a larger buffer, decreasing CPU overhead. The feature is enabled by default on VMs running on Windows Server 2019 and later. For more information, see Acceleration Mechanisms on NVIDIA ConnectX-5 NICs.SR-IOVSingle Root I/O devices use DDA to allow guests access to portions of specific NIC cards allowing for reduced latency and increased throughput. SR-IOV requires up to date physical function (PF) drivers on the host and virtual function (VF) drivers on the guest. For more information, see also Azure Accelerated Networking.\nReferences\n\nlearn.microsoft.com/en-us/windows-server/virtualization/hyper-v/feature-descriptions-for-linux-and-freebsd-virtual-machines-on-hyper-v\ndocs.kernel.org/networking/device_drivers/ethernet/microsoft/netvsc.html\n"},"notes/Cloud/Azure/Linux-Integration-Services":{"title":"Linux Integration Services","links":["notes/Cloud/Azure/Hyper-V-Guest-Features","notes/Operating-System/Linux/List-Out-of-Tree-Modules-in-Linux"],"tags":["deprecated"],"content":"Overview\nLinux Integration Services is a set of drivers that enable synthetic device support in Linux VMs under Hyper-V.\nYou can install the stand-alone LIS package as an upgrade in older Linux distributions with LIS built-in, but the package hasn’t been updated since March 2020 and development is mostly stalled. If you are experiencing problems on newer distribution versions, update the kernel instead.\nSupported Linux distributions\n\nRHEL 5.2 to 7.6\nCentOS 5.2 to 7.6\nOracle Linux 6.4 to 7.6, with Red Hat Compatible Kernel\n\nFeatures\n\nSRIOV (for example, Azure Accelerated Networking) capability for Red Hat Enterprise Linux, CentOS, and Oracle Linux with the Red Hat Compatible Kernel versions 6.7-6.10 (64-bit only)\nUpdated mlx4 and mlx5 drivers for Red Hat Enterprise Linux, CentOS, and Oracle Linux with the Red Hat Compatible Kernel versions 7.3-7.4.\nLinux Integration Services uses the Hyper-V TimeSync protocol to implement a Precision Time Protocol service with this time source.\n\nSee Hyper-V Guest Features for recent feature development that might not be available in the LIS package.\nUsage\nValidate installation\nLinux Integration Services kernel modules can be listed with the modinfo command.\n# modinfo hv_vmbus\nfilename:    /lib/modules/2.6.32-71.el6.x86_64/extra/microsoft-hyper-v/hv_vmbus.ko\nversion:     4.3.5\n...\nThis command can be repeated for all kernel modules (hv_vmbus, hv_netvsc, hv_storvsc, hv_blkvsc, and hv_utils), but only out-of-tree modules have useful version information. See also List Out-of-Tree Modules in Linux.\nTo verify that all subcomponents are running in the kernel, execute the following command:\n/sbin/lsmod | egrep -i &quot;^hv|hyperv&quot;\nTimeSync PTP service\nTo verify the device is the TimeSync device, check the clock name:\n$ ls /sys/class/ptp\nptp0  ptp1\n$ cat /sys/class/ptp/ptp0/clock_name\nhyperv\nTo switch to the host clock when using chronyd, enable TimeSync as a source in /etc/chrony.conf:\nrefclock PHC /dev/ptp0 poll 3 dpoll -2 offset 0\n\nLatest documentation for TimeSync is available at learn.microsoft.com/en-us/azure/virtual-machines/linux/time-sync#tools-and-resources. Note the udev rule requirement and stratum 2 configuration.\nReferences\n\nLinux Integration Services v4-3-5.pdf from www.microsoft.com/en-us/download/details.aspx\n"},"notes/Cloud/Azure/Microsoft-Azure-Network-Adapter":{"title":"Microsoft Azure Network Adapter","links":[],"tags":[],"content":"Driver\nThe driver supports Linux and Windows only.\nOn Linux\nLinux v6.6+ is recommended. Example of patches required:\n\ngithub.com/torvalds/linux/commit/a7dfeda6fdeccab4c7c3dce9a72c4262b9530c80 for robustness\ngithub.com/torvalds/linux/commit/b1d13f7a3b5396503e6869ed627bb4eeab9b524f for performance\n\nSource:\n\ngithub.com/microsoft/azurelinux/blob/2.0/SPECS/kernel-azure/kernel-azure.spec\nPatches backported to the CBL-Mariner Linux kernel in 2024\n\nCompatibility\n\nMANA maintains feature-parity with previous Azure networking features. VMs run on hardware with both Mellanox and MANA NICs, so existing ‘mlx4’ and ‘mlx5’ support still need to be present.\n\n\nThe next generation of Azure Boost will introduce the Microsoft Azure Network Adapter (MANA).\n\nSee learn.microsoft.com/en-us/azure/azure-boost/overview#current-availability for the list of compatible VM size families, but beware that compatible VMs may run on Mellanox NICs too.\nCheck traffic flowing through the MANA adapter\nYou can get packet and byte count of the MANA Virtual Function (VF) from the routable ethN interface:\n$ ethtool -S eth0 | grep -E &quot;^[ \\t]+vf&quot;\n     vf_rx_packets: 226418\n     vf_rx_bytes: 99557501\n     vf_tx_packets: 300422\n     vf_tx_bytes: 76231291\n     vf_tx_dropped: 0"},"notes/Cloud/Azure/Receive-Segment-Coalescing":{"title":"Receive Segment Coalescing","links":[],"tags":[],"content":"When receiving data, the miniport driver, NDIS, and TCP/IP must all look at each PDU’s header information separately. When the network stack receives a large amount of data, this is a lot of overhead. Receive segment coalescing (RSC) reduces the overhead by coalescing a sequence of received segments and passing them to the host TCP/IP stack in one operation, so that NDIS and TCP/IP need only look at one header for the entire sequence.\nRSC supports coalescing in a way that:\n\n\nDoesn’t interfere with the normal operation of TCP’s congestion and flow control mechanisms.\n\n\nCoalesces packets without discarding information used by the TCP stack.\n\n\nRSC-capable miniport drivers for network cards must:\n\n\nFollow a standard set of rules when coalescing segments.\n\n\nProvide certain out-of-band information to the host TCP/IP stack.\n\n\nAt a high level, the NIC and miniport driver must handle the receipt of a TCP segment over the wire as follows:\n\n\nCheck the incoming segment for an exception as follows:\n\n\nIf no exception was encountered, check whether the segment can be coalesced with the last segment that was received for the same TCP connection per the rules.\n\n\nIf the segment triggered an exception, or if coalescing it with the previously received segment is not possible, then indicate the segment individually.\n\n\n\n\nThe NIC and miniport driver must not indicate coalesced segments until the protocol driver enables RSC.\n\n\nFor a given TCP connection, a data indication from the miniport adapter to the host TCP/IP stack may consist of one or more coalesced segments, separated by one or more individual segments that could not be coalesced.\n\n\nThe NIC and miniport driver must not delay the indication of TCP segments, whether coalesced or not. Specifically, the NIC and miniport driver must not delay the indication of segments from one deferred procedure call (DPC) to the next in order to attempt to coalesce the segments.\n\n\nThe NIC and miniport driver may use timers to determine the end of coalescing. However, the handling of latency sensitive workloads must be as effective as the DPC boundary requirement.\n\n\nReferences\n\nlearn.microsoft.com/en-us/windows-hardware/drivers/network/overview-of-receive-segment-coalescing\n"},"notes/Cloud/Azure/Receive-Side-Scaling-on-Azure":{"title":"Receive Side Scaling on Azure","links":[],"tags":[],"content":"\nHyper-V supports receive side scaling. For TCP &amp; UDP, packets can be distributed among available queues based on IP address and port number.\nFor TCP &amp; UDP, we can switch hash level between L3 and L4 by ethtool command. TCP/UDP over IPv4 and v6 can be set differently. The default hash level is L4. We currently only allow switching TX hash level from within the guests.\nOn Azure, fragmented UDP packets have high loss rate with L4 hashing. Using L3 hashing is recommended in this case.\n\nFor example, for UDP over IPv4 on eth0:\nTo include UDP port numbers in hashing:\nethtool -N eth0 rx-flow-hash udp4 sdfn\nTo exclude UDP port numbers in hashing:\nethtool -N eth0 rx-flow-hash udp4 sd\nTo show UDP hash level:\nethtool -n eth0 rx-flow-hash udp4\nReferences\n\ndocs.kernel.org/networking/device_drivers/ethernet/microsoft/netvsc.html\n"},"notes/Cloud/Azure/Traceroute-on-Azure":{"title":"Traceroute on Azure","links":["notes/Network/Path-MTU-Discovery"],"tags":[],"content":"github.com/catchpoint/Networking.traceroute\nThis version of traceroute has an “Loose match” mode that allows traceroute to run properly in Azure environments.\nHowever, ICMP isn’t supported on NAT gateways and fails to go through them. But what about Path MTU Discovery?\nReferences\n\nfosdem.org/2024/schedule/event/fosdem-2024-2929—where-the-are-the-packets-going-/\n"},"notes/Cloud/Azure/VL2":{"title":"VL2","links":["notes/Network/Valiant-Load-Balancing","notes/Cloud/Google-Cloud/Andromeda"],"tags":[],"content":"Valiant Load Balancing\nAzure adopts VLB to cope with traffic volatility. Traffic patterns not exceeding line card speeds don’t cause any interference in the network.\nVL2 agent selects an intermediate switch via one or a set of anycast addresses, each representing a pool of intermediate switches and relies on ECMP to split traffic in equal ratios.\nAddressing\nSeparating AAs (application-specific Addresses) from LAs (location-specific addresses, which identify the ToR switch to which servers are connected) reduces complexity on the intermediate and aggregate switches that implement VLB. These switches only need to know where to forward packets carrying LAs, and LAs are bound to locations, so routes could be aggregated into bigger CIDR blocks, resulting in fewer routes.\nIt also limits the number of AAs a ToR switch needs to know to only servers under it, also reducing number of routes.\nDirectory System\nThe VL2 directory system stores the mappings of AAs to LAs.\nThe directory system replaces ARP broadcast packets with unicast query to the VL2 directory system. The mapping is also cached, just like a host’s ARP cache.\nThe VL2 agent at each server traps packets from the host and encapsulates them with the LA address of the destination ToR. The destination ToR switch then decapsulates the packet and delivers it to the destination AA.\nLayer-2 semantics\nARP is replaced by the directory system, and DHCP messages are intercepted and unicast forwarded to DHCP servers.\nOther general layer-2 broadcast traffic is handled via IP multicast using service-specific multicast address. General broadcast traffic is rate-limited to prevent storms.\nCache invalidation\nA stale host mapping is only corrected when it’s used to deliver traffic. The destination ToR forwards a sample of non-deliverable packets to a directory server, which sends corrections to the source server in samples. This reflects that VL2 follows the On Demand Model described in Andromeda’s paper.\nReferences\n\nwww.microsoft.com/en-us/research/wp-content/uploads/2016/02/vl2-sigcomm09-final.pdf\n"},"notes/Cloud/Azure/Virtual-Filtering-Platform":{"title":"Virtual Filtering Platform","links":["notes/Operating-System/FreeBSD/PF-Firewall","notes/Cloud/Azure/Azure-Accelerated-Networking"],"tags":["networking"],"content":"Programming model\nPorts\nEach port maps to a VM or VNIC (virtual NIC). VFP filters traffic from and to a VNIC by attaching a set of policies to each port.\nEach port can scale up to 500k concurrent TCP connections, after which state tracking is considered prohibitively expensive.\nLayers\nVFP divides a port’s policy into layers.\n\nPackets traverse layers in the opposite order when inbound than when outbound. This implements an address space boundary, with all packets above “VIP→DIP NAT” in “DIP Space,” and packets below it in “VIP Space.”\nGroups\nGroups are the atomic unit of policy in VFP, which means transactional update of policy happens at this level.\nGroups can have global conditions, which let packets pass through, traversing further groups.\n\nGroups are processed sequentially within a layer. Usually the last group matched is selected for processing. However, if a rule is marked “terminating,” it’s applied immediately without traversing further groups.\nThis is similar to PF Firewall’s rule evaluation, with the exception that within each group only the highest priority rule that matches the packet is selected.\nUnified Flow Table\nThe action for a UFID (identifier for a unique flow) is relatively stable over the lifetime of a flow, so the UFID can be cached with the resulting HT (Header Transposition). The resulting flow table is the Unified Flow Table (UFT).\nWith the UFT, only the first packet of a TCP flow has to go through the slow path of running the transposition engine. Subsequent packets have the cached HT action applied directly.\nUFT dramatically improves VFP performance, especially with Azure Accelerated Networking offload.\nReconciliation\nEach UF (unified flow) is tagged with the current generation number of the port at creation time. If a matching UF whose generation number is less than the port’s current number, lazy reconciliation is performed.\nReferences\n\nwww.usenix.org/system/files/conference/nsdi17/nsdi17-firestone.pdf\n"},"notes/Cloud/Free-Domain-Name-Providers":{"title":"Free Domain Name Providers","links":[],"tags":[],"content":"\nnic.us.kg/ (.us.kg)\n"},"notes/Cloud/GitHub-personal-access-token-to-perform-Git-operations-over-HTTPS":{"title":"GitHub personal access token to perform Git operations over HTTPS","links":[],"tags":[],"content":"\nCreate a fine-grained personal access token with “Contents” permission to Repositories you have selected.\nConfigure Git to store credentials on disk: git config --global credential.helper store. This stores credentials in plaintext on disk!\nAccess the repo via https with your username and the personal access token as password.\n\n$ git clone github.com/USERNAME/REPO.git\nUsername: YOUR-USERNAME\nPassword: YOUR-PERSONAL-ACCESS-TOKEN\n\nRotate the token before it expires.\n\nReferences\n\nUsing a personal access token on the command line\nRepository permissions for “Contents”\ndocs.github.com/en/get-started/getting-started-with-git/caching-your-github-credentials-in-git\n"},"notes/Cloud/Google-Cloud/Andromeda":{"title":"Andromeda","links":[],"tags":[],"content":"Control plane\nScoped control planes\nThe Andromeda control plane is split into a regionally aware control plane (RACP) and a globally aware control plane (GACP). The RACP programs all intra-region network connectivity, while the GACP manages inter-region connectivity.\nThis approach ensures that intra-region networking in each region is a separate failure domain.\nHoverboard model\nOn the VM host, packets without a matching route goes to Hoverboard gateways, which have forwarding information for all virtual networks.\nOnce a flow exceeds a usage threshold, the Andromeda control plane programs offload flows, which are direct host-to-host flows that bypass the gateways.\n\nUsing gateways for the long tail of low bandwidth flows significantly reduces the number of routes to program on each host, improving scalability by 50x.\nVM host dataplane\n\nFast path\nThe first path in the dataplane is the Fast Path, which is a user-space packet processing path with a per-packet CPU budget of 300 ns.\nThe Fast Path directly accesses both VM virtual NIC and host physical NIC queues via shared memory, bypassing the VMM and host OS to achieve minimal overhead.\nThe Fast Path busy pulls on a dedicated logical CPU. The other logical CPU on the same physical core runs low-CPU control plane work, leaving most of the physical core for Fast Path use. The Fast Path can be scaled to multiple CPUs using multi-queue NICs.\nFast path flow table (FT)\nAll packets pass through the FT for both routing and per-flow packet operations.\nThe FT is a cache of the full vswitchd flow tables. If no match is found, a packet is sent to the Flow Miss Coprocessor, which sends it to vswitchd.\nVM coprocessor\nCPU-intensive work is split to per-VM coprocessors, which enables feature growth without impacting Fast Path performance.\nCoprocessor stages include encryption, DoS and abuse detection, and WAN traffic shaping. CPU time on the coprocessor is attributed to the container of the corresponding VM, providing fairness and isolation between VMs.\nServiceability\nGoogle opted for VM Live Migration to facilitate maintenance, upgrades and placement optimization.\nDuring the migration blackout phase, the VM is paused for a median duration of 65 ms and p99 of 388 ms. After blackout, the VM continues execution and network connections aren’t interrupted.\nReferences\n\nwww.usenix.org/system/files/conference/nsdi18/nsdi18-dalton.pdf\n"},"notes/Cloud/Google-Cloud/GCP-VPC":{"title":"GCP VPC","links":[],"tags":[],"content":"Default firewall actions\n\nEvery network has an implied deny firewall rule for ingress traffic.\nUnless overridden by a higher priority rule, the implied allow rule for egress traffic permits outbound traffic from all instances.\n\nICMP firewall rules\n\nICMP_PROTOCOL: the ICMP protocol type. Specify ICMPv4 by using the protocol name icmp or protocol number 1. Specify ICMPv6 by using protocol number 58.\n\nReferences\n\ncloud.google.com/vpc/docs/vpc#firewall_rules\n"},"notes/Cloud/Google-Cloud/Google-Cloud-Free-Tier":{"title":"Google Cloud Free Tier","links":[],"tags":[],"content":"Creating a Free Tier VM\n\nCreate a VPC and subnet with IPv6 enabled.\nConfigure VPC Firewall.\nCreate Instance Template for reuse.\n\nSelect Debian amd64 image, which avoids Snap from Ubuntu and OOM problems with dnf.\nChange disk type to “standard persistent disk.”\nSelect e2-micro instance type and one of the Free Tier-eligible regions.\n\n\nCreate instance from template.\n\nSelect “No service account.”\nUnselect custom device name of boot disk.\n\n\nEnable “block project-wide SSH keys” and “deletion protection” if you want to.\n\nInstance template\n\nSelect e2-micro machine type.\nChange disk type to standard and size to 30 GB.\nSelect the OS image you want.\nSelect VPC and subnet.\nSelect “IPv4 and IPv6” IP stack.\nIf you prefer not to log in via the console, add SSH keys in Security with one word “comment” as the username.\n\nKEY_VALUE USERNAME\n\nNetwork egress\nAvoid China and Australia destinations. For example, derp5-all.tailscale.com is located in Australia.\nSwap\n\n\n                  \n                  Caution\n                  \n                \n\nIt isn’t recommended to use RHEL-compatible Linux distributions on an e2-micro instance.\n\n\n1 GB memory isn’t enough for dnf upgrade with many repos (per Bug 1907030). Set up swap space as follows.\nsudo -i\n \numask 077\ndd if=/dev/urandom of=/.swapfile count=4096 bs=1MiB\nmkswap /.swapfile\necho &quot;/.swapfile\tnone\tswap\tsw\t0\t0&quot; &gt;&gt; /etc/fstab\nswapon -a\n \necho &quot;vm.swappiness = 10&quot; &gt; /etc/sysctl.d/90-vm.conf\nsysctl -p /etc/sysctl.d/90-vm.conf\nBefore dnf upgrade, remove unused packages.\ndnf config-manager --disable google-cloud-sdk\ndnf install -y tmux\ntmux\n \nrpm -e google-cloud-sdk"},"notes/Cloud/Google-Cloud/Google-Kubernetes-Engine":{"title":"Google Kubernetes Engine","links":[],"tags":[],"content":"Network egress\nBesides container registries, a minimal GKE cluster have the following services that connects to the Internet.\n\nnode-problem-detector.service\ngoogle-guest-agent.service\n\nBootstrap images\nMinimal set of bootstrap images with Dataplane V2.\nImage:         gke.gcr.io/addon-resizer:1.8.18-gke.0\nImage:         gke.gcr.io/cilium/cilium:v1.12.10-gke.25\nImage:         gke.gcr.io/cluster-proportional-autoscaler:1.8.5-gke.0\nImage:         gke.gcr.io/csi-node-driver-registrar:v2.8.0-gke.4\nImage:         gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.10.7-gke.0\nImage:         gke.gcr.io/k8s-dns-dnsmasq-nanny:1.22.22-gke.0\nImage:         gke.gcr.io/k8s-dns-kube-dns:1.22.22-gke.0\nImage:         gke.gcr.io/k8s-dns-sidecar:1.22.22-gke.0\nImage:         gke.gcr.io/metrics-server:v0.5.2-gke.3\nImage:         gke.gcr.io/netd-init:v0.6.19-gke-v0.2.5-gke.0\nImage:         gke.gcr.io/netd:v0.6.19-gke-v0.2.5-gke.0\nImage:         gke.gcr.io/proxy-agent:v0.1.3-gke.0\n\nRestrictions\nOn-premises nodes can not join a GKE cluster. Consider GKE on-prem instead."},"notes/Cloud/Google-Cloud/Maglev":{"title":"Maglev","links":["notes/Network/Direct-Server-Return"],"tags":[],"content":"Architecture overview\nECMP distributes packets for a VIP evenly to Maglev forwarders. Maglev then implements the return path with Direct Server Return, directly sending packets from service endpoint to the edge routers.\nA Maglev cluster can be sharded by serving different sets of VIPs on each shard, which improves scalability and performance isolation between tenants.\n\nMaglev configuration\nMaglev services retrieve configuration objects from local files or remote RPC.\nIn the diagram below, BP stands for backend pool, and represents a set of service endpoints (generally TCP or UDP servers). BPs can include other BPs to simplify configuration of a common set of backends.\nMaglev services\nMaglev runs on commodity Linux servers. It achieves 10Gbps line-rate throughput via kernel bypass, share-nothing architecture and other optimization techniques.\n\nEach Maglev machine contains a controller and a forwarder.\nMaglev controller\nA Maglev controller announces VIPs defined in its config objects, and withdraws all VIP announcements when the forwarder becomes unhealthy. This ensures that routers only forward packets to healthy machines.\nMaglev forwarder\nHealth checking\nEach BP is associated with one or more health checking methods, and health checks are deduplicated by IP to reduce overhead. Only healthy backends are considered for consistent hashing.\nPacket flow\n\nPackets are read by the steering module from the shared packet pool, and distributed to packet (rewriter) threads with 5-tuple hashing.\nPackets are rewritten to be GRE-encapsulated, with the outer IP header destined to the selected backend. Selection is done via connection tracking and consistent hashing.\nRewritten packets are send to TX queues, which are polled by the muxing module and passed to the NIC via the shared packet pool.\n\n\nThe shared packet pool between the NIC and the forwarder means that packets do not need to be copied for transmit or receive. A ring queue of pointers also makes it possible to process packets in batches, further improving efficiency.\n\nConsistent hashing\nMaglev favors even load balancing over minimal disruption during backend changes in its consistent hashing algorithm. Maglev achieves an over-provision factor of less than 1.2 over 60% of time in a production cluster.\nConfig updates are committed atomically, so that consistent hashing input (set of backends) only changes once. Configuration of different machines may be temporarily out of sync, due to the nature of distributed systems.\nConnection tracking\nConnection tracking state is stored locally on each thread of a Maglev machine. It helps when the set of backends change, but is insufficient for the following cases.\n\nWhen the set of Maglev machines changes, the router in front might not provide connection affinity. This behavior is vendor-specific and out of Maglev’s control.\nDuring heavy load or SYN flood attacks, the connection tracking table may be filled and could not track new connections.\n\nIn such cases, consistent hashing is used as a fallback, which is sufficient if backends do not change in the meantime.\nThread optimizations\n\nEach thread is pinned to a dedicated CPU core to achieve maximum performance.\nHash is recomputed on each packet thread to avoid cross-thread synchronization.\n\nReferences\n\nstatic.googleusercontent.com/media/research.google.com/en//pubs/archive/44824.pdf\n"},"notes/Cloud/Google-Cloud/Unexpected-Costs-on-GCP":{"title":"Unexpected Costs on GCP","links":["notes/Cloud/Google-Cloud/Google-Cloud-Free-Tier","notes/Cloud/Google-Cloud/Google-Kubernetes-Engine"],"tags":[],"content":"Cost analysis\nReports\nGroup by SKU, exclude “Promotions”-type credits, and sort by Subtotal.\nBigQuery\nEnable standard usage cost export to BigQuery, and run the following queries.\n/* List billing data from yesterday excluding those using discounts */\nSELECT *\n  FROM\n    `&lt;project&gt;.&lt;dataset_name&gt;.gcp_billing_export_v1_***` CROSS JOIN UNNEST(credits) AS credit\n  WHERE\n    DATE(_PARTITIONTIME) = DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY) AND\n    cost &gt; 0 AND\n    credit.type &lt;&gt; &quot;DISCOUNT&quot;\n  LIMIT 1000;\n \n/* List *discounted* SKUs and type of discount on a specific day */\nSELECT DISTINCT sku.id, sku.description, credit.type, credit.id, credit.name\n  FROM\n    `&lt;project&gt;.&lt;dataset_name&gt;.gcp_billing_export_v1_***` CROSS JOIN UNNEST(credits) AS credit\n  WHERE\n    TIMESTAMP_TRUNC(_PARTITIONTIME, DAY) = TIMESTAMP(&quot;2023-11-**&quot;) AND\n    cost &gt; 0 AND\n    credit.type = &quot;DISCOUNT&quot;\n  LIMIT 1000;\n \n/* Filter for a specific SKU, return original records */\nSELECT service, sku, usage_start_time, usage_end_time, cost, credits\n  FROM\n    `&lt;project&gt;.&lt;dataset_name&gt;.gcp_billing_export_v1_***`\n  WHERE\n    TIMESTAMP_TRUNC(_PARTITIONTIME, DAY) = TIMESTAMP(&quot;2023-11-**&quot;) AND\n    cost &gt; 0 AND\n    sku.id = &quot;CF4E-A0C7-E3BF&quot;\n  LIMIT 1000;\n \n/* Filter for a specific SKU in a month, return cost per day */\nSELECT TIMESTAMP_TRUNC(_PARTITIONTIME, DAY) AS ts, service.description, sku.description, sum(cost) AS cost\n  FROM\n    `billing-management-400904.all_billing_data.gcp_billing_export_v1_01D2EE_85D75C_0DBFA9`\n  WHERE\n    TIMESTAMP_TRUNC(_PARTITIONTIME, MONTH) = TIMESTAMP(&quot;2023-11-01&quot;) AND\n    sku.id = &quot;D34A-7997-2D03&quot;\n  GROUP BY ts, service.description, sku.description\n  HAVING cost &gt; 0\n  ORDER BY ts ASC\n  LIMIT 1000;\nThe second query shows discounted costs, including the Free Tier discount.\nSee cloud.google.com/bigquery/docs/reference/standard-sql/operators for more SQL operators supported by BigQuery.\nUnexpected costs\nFree Tier\n\nGoogle Kubernetes Engine\n\nZonal Kubernetes Clusters\n\n\nCompute Engine\n\nE2 Instance Core running in XXX\nE2 Instance Ram running in XXX\n\n\n\nCompute Engine’s free tier is applied via transactions with negative cost that cancel out the real transactions.\nNetworking\nPrivate Service Connect (GKE Cluster Endpoint)\n\nNetworking Private Service Connect Partner Select End Point (6882-BFB0-B7E5)\n\n\n100% discount for Networking Private Service Connect Partner Select End Point.\n\n\nNetworking Service Directory Registered Resource (D34A-7997-2D03), 0.10 USD per month\n\n\nAccess services in another VPC network by using Private Service Connect endpoints. You can connect to your own services, or those provided by other service producers, including by Google.\n\nRun gcloud services enable servicedirectory.googleapis.com to enable the required API to inspect the resource.\nService directory registration by GKE is handled regardless of the status of the API. Service name is gk3-*-pe, type is Private Service Connect, namespace is the default goog-psc-default, and endpoint IP is the internal endpoint, which means it is immutable.\nNetwork Intelligence Center\n\nNetwork Intelligence Center Network Analyzer Resource Hours\nNetwork Intelligence Center Internet to Google Cloud Performance Resource Hours\nNetwork Intelligence Center Topology and Google Cloud Performance Resource Hours\n\n\nThe Network Topology and Performance Dashboard modules are available to all users for 100% discount. The cost of these modules will be shown in your billing details, but you will not be charged for them. Any changes to the discount structure will be effective with a 90 days prior notice. The updates to the discount rates will appear in your billing information only after you choose to re-enable the modules.\n\nReferences\n\ncloud.google.com/network-intelligence-center/pricing#network-topology-pricing-details\nwww.googlecloudcommunity.com/gc/Cloud-Hub/How-to-stop-Network-Intelligence-Center-related-SKUs/m-p/515742/highlight/true#M1782\ncloud.google.com/vpc/docs/configure-private-service-connect-services\ncloud.google.com/skus/sku-groups/global-skus\ncloud.google.com/free/docs/free-cloud-features#compute\n"},"notes/Cloud/Oracle-Cloud/OCI-IPv6-Address-Fix":{"title":"OCI IPv6 Address Fix","links":[],"tags":[],"content":"Issue\nOn boot, Linux instances can get its IPv6 address, but after lease expires, it is possible to lose the address.\nSolution\nThe Stupid but workable way\nUse a oneshot systemd service to add a static address to the interface.\n[Unit]\nDescription=ensure static IPv6 address\nAfter=network.target\n\n[Service]\nType=oneshot\nExecStart=-/usr/sbin/ip -6 addr add 2603:c024:4506:d46f:c7ad:ddf2:a87b:89dc dev enp0s3\nProtectHome=yes\n\n[Install]\nWantedBy=multi-user.target\n\n[Unit]\nDescription=Run ensure-ipv6-addr daily and on boot\n\n[Timer]\nOnBootSec=15min\nOnUnitActiveSec=1d\n\n[Install]\nWantedBy=timers.target\n\nThe hard way and iSCSI Networking\n\niSCSI boot volumes use the 169.254.0.2/32 address and block volumes use the 169.254.2.0/24 network.\n\nThe default kernel cmdline on Oracle Linux 9 contains netroot=iscsi:169.254.0.2:::1:iqn.2015-02.oracle.boot:uefi, which enables DHCP during early boot.\nFrom journald logs, we can see that the connection is refused on our instance.\ndracut-initqueue[950]: iscsiadm: cannot make connection to 169.254.0.2: Connection refused\ndracut-initqueue[950]: iscsiadm: cannot make connection to 169.254.0.2: Connection refused\ndracut-initqueue[950]: iscsiadm: connection login retries (reopen_max) 5 exceeded\ndracut-initqueue[950]: iscsiadm: Could not perform SendTargets discovery: iSCSI PDU timed out\ndracut-initqueue[887]: Warning: Target discovery to 169.254.0.2:3260 failed with status 0\nThe reason is that the attachement type (Boot volume type) is Paravirtualized, but in this case, IOPS performance is worse than iSCSI attachments. The benefit is that netroot from the cmdline can be removed, and that the VM supports live migration for best availability.\nFor a configuration in /etc/NetworkManager/system-connections to be effective, it must not overlap with configurations in /etc/sysconfig/network-scripts according to default config precedence on Oracle Linux 9, and NetworkManager should not be activated when running from the initial ramdisk.\nA workaround for the latter is to configure allowed-connections for the device, see gitlab.freedesktop.org/NetworkManager/NetworkManager/-/commit/bace14fe1f374db26e49e4e7d61d2fbfce4241cc.\nBy removing netroot from the cmdline, we can regain control of NetworkManager configuration, and set a static IP with nmtui.\nThe formal way\nA workaround was implemented in systemd for this issue.\n\ngithub.com/systemd/systemd/issues/28183\ngithub.com/systemd/systemd/pull/28138\n\nAlso note that we need to have dhcpv6-client in the list of services for the public zone if we are using firewalld. This can be checked with firewall-cmd --list-all, and added with the following commands.\nsudo firewall-cmd --add-service=dhcpv6-client --permanent\nsudo firewall-cmd --reload"},"notes/Cloud/Oracle-Cloud/OCI-Link-Local-Addresses":{"title":"OCI Link-Local Addresses","links":[],"tags":[],"content":"\nInstances use link local addresses to access the instance metadata service (169.254.169.254:80), DNS (169.254.169.254:53), NTP (169.254.169.254:123), kernel updates (169.254.0.3), and iSCSI connections to boot volumes (169.254.0.2:3260, 169.254.2.0/24:3260). You can use host-based firewalls, such as iptables, to ensure that only the root user is authorized to access these IPs. Ensure that these operating system firewall rules are not altered.\n\niSCSI\nOracle Linux configures iscsi.service with /var/lib/iscsi/nodes/iqn.2015-02.oracle.boot:uefi/169.254.0.2,3260,1/default.\niscsi then triggers start of iscsid via iscsid.socket.\nReferences\n\ndocs.oracle.com/en-us/iaas/Content/Security/Reference/compute_security.htm\n"},"notes/Cloud/Oracle-Cloud/OCI-Network-Security-Rules":{"title":"OCI Network Security Rules","links":[],"tags":[],"content":"Security lists\nSecurity lists let you define a set of security rules that applies to all the VNICs in an entire subnet.\nNetwork Security Groups\nNetwork security groups (NSGs) let you define a set of security rules that applies to a group of VNICs of your choice."},"notes/Cloud/Oracle-Cloud/OCI-Public-IP-Ranges":{"title":"OCI Public IP Ranges","links":[],"tags":[],"content":"curl -s docs.oracle.com/en-us/iaas/tools/public_ip_ranges.json | jq -r &#039;.regions[] | select( .region == &quot;xxx&quot;) | .cidrs[].cidr&#039;\n \n# Replace xxx with Region Identifier\n\ndocs.oracle.com/en-us/iaas/Content/General/Concepts/regions.htm\ndocs.oracle.com/en-us/iaas/Content/General/Concepts/addressranges.htm\n"},"notes/Computer-Science/AWS-CLB-Surge-Queue-and-Little's-Law":{"title":"AWS CLB Surge Queue and Little's Law","links":["notes/AI/GPT-4-Chat-Demo"],"tags":[],"content":"Metrics from a TCP Listener\nSampleCount\n\nSampleCount of RequestCount and Latency metrics are exactly the same as number of requests.\nSampleCount of SurgeQueueLength is roughly number of requests doubled.\nSampleCount of EstimatedProcessedBytes and EstimatedALB* metrics is number of load balancer nodes at 1 minute periods.\nSampleCount of HealthyHostCount and UnHealthyHostCount is a multiple of number of load balancer nodes at 1 minute periods. The multiplier could be one of 18, 60, 66, or some other integer.\n\nConnection Rate\nWhen TCP request rate is stable and no requests fail, sum of EstimatedALBNewConnectionCount should be twice as big as sum of RequestCount, because connections established with both clients and targets are counted towards EstimatedALBNewConnectionCount.\nAverage Waiting Time in Surge Queue\n\n\n                  \n                  Warning\n                  \n                \n\nThis is a thought experiment with GPT-4 Chat Demo. The resulting formula is based on a false assumption.\n\n\nThe 2 times relationship between SurgeQueueLength samples and number of requests implies that each request triggers 2 samples.\nGiven that the maximum SurgeQueueLength we have observed is the per node limit of 1024, we can assume that each sample only takes the surge queue length of that specific node.\nTherefore, the total SurgeQueueLength of the load balancer should be Avg(SurgeQueueLength) multiplied by the number of nodes, assuming that each node handles the same amount of requests in each period.\nApplying Little’s law to the surge queue, we get\n\\begin{aligned}\nAvg(Latency)\\,s\n&amp;= \\frac{Avg(SurgeQueueLength) * Number\\ of\\ Nodes}{Sum(RequestCount) / Period\\,s}\\\\\n&amp;= \\frac{Avg(SurgeQueueLength) * SampleCount(EstimatedProcessedBytes)}{ Sum(RequestCount) / Period\\,s * Period\\,min}\\\\\n&amp;= \\frac{Avg(SurgeQueueLength) * SampleCount(EstimatedProcessedBytes)}{60 * Sum(RequestCount)}\n\\end{aligned}\nFrom the CloudWatch data we have analyzed, we can assume that ratio of nodes across Availability Zones (AZs) is the same as ratio of HealthyHostCount in each AZ.\nTherefore, to calculate the average latency per AZ, we could use per-AZ metrics and SampleCount of EstimatedProcessedBytes (which is not split by AZ) multiplied by \\frac{SampleCount(HealthyHostCount_{a})}{SampleCount(HealthyHostCount)}.\nThe resulting formula is\n\\begin{aligned}\nAvg(Latency_{a})\\,s\n&amp;= \\frac{Avg(SurgeQueueLength_{a}) * SampleCount(EstimatedProcessedBytes)}{60 * Sum(RequestCount_{a})}\\\\\n&amp;= \\frac{Avg(SurgeQueueLength_{a}) * SampleCount(EstimatedProcessedBytes) * SampleCount(HealthyHostCount_{a})}{60 * Sum(RequestCount_{a}) * SampleCount(HealthyHostCount)}\n\\end{aligned}\n, with the assumption that requests are evenly distributed across nodes.\nThe Real Situation\nRegarding the assumption in the above section, we can see from percentile statistics of EstimatedALBNewConnectionCount that the distribution of connections across nodes is not even nor stable. To get the total SurgeQueueLength, each node’s samples must be normalized to the same weight, which is not possible from summary metrics.\nTo get accurate numbers, you need to enable access logs for your CLB and crunch the *_processing_time numbers."},"notes/Database/MariaDB-Grant-Privileges-via-Unix-socket":{"title":"MariaDB Grant Privileges via Unix socket","links":[],"tags":[],"content":"CREATE USER &#039;phabricator&#039;@&#039;localhost&#039; IDENTIFIED VIA unix_socket;\nGRANT ALL PRIVILEGES ON `phabricator\\_%`.* TO &#039;phabricator&#039;@&#039;localhost&#039;;"},"notes/Database/MongoDB-Shell":{"title":"MongoDB Shell","links":[],"tags":[],"content":"show dbs\nuse &lt;database&gt;\nshow collections\nconfig.set(&quot;displayBatchSize&quot;, 3) // limit the number of items returned by a cursor\ndb.&lt;collection&gt;.find()\nReferences\n\nwww.mongodb.com/docs/mongodb-shell/reference/configure-shell-settings-api/#update-number-of-items-returned-by-a-cursor\n"},"notes/Database/PgBouncer":{"title":"PgBouncer","links":[],"tags":[],"content":"Authentication\n\nThe passwords or secrets stored in the authentication file serve two purposes. First, they are used to verify the passwords of incoming client connections, if a password-based authentication method is configured. Second, they are used as the passwords for outgoing connections to the backend server, if the backend server requires password-based authentication (unless the password is specified directly in the database’s connection string). The latter works if the password is stored in plain text or MD5-hashed. SCRAM secrets can only be used for logging into a server if the client authentication also uses SCRAM, the PgBouncer database definition does not specify a user name, and the SCRAM secrets are identical in PgBouncer and the PostgreSQL server (same salt and iterations, not merely the same password). This is due to an inherent security property of SCRAM: The stored SCRAM secret cannot by itself be used for deriving login credentials.\n\nPrepared statements in transaction pooling mode\nPgBouncer 1.21.0 added support for protocol-level named prepared statements.\nSee github.com/pgbouncer/pgbouncer/pull/845 and github.com/postgresml/pgcat/pull/474 (PgCat).\nMonitoring\ngithub.com/prometheus-community/pgbouncer_exporter\nConnect to the reserved pgbouncer database, that is, the admin console, as the stat_collector user with configured password or TLS authentication credentials.\nRequired configuration\nstats_users = stat_collector\nignore_startup_parameters = extra_float_digits\nDefault connection string\npostgres://postgres:@localhost:6543/pgbouncer?sslmode=disable\n"},"notes/Database/PostgreSQL":{"title":"PostgreSQL","links":[],"tags":[],"content":"Create user and table\nCREATE USER salesapp;\nCREATE DATABASE sales OWNER salesapp;\n"},"notes/Database/psql-Verify-TLS-Certificate":{"title":"psql Verify TLS Certificate","links":[],"tags":[],"content":"Append ssl* arguments to the connection string like so.\npsql &#039;postgres://...?sslmode=verify-full&amp;sslrootcert=system&#039;\n"},"notes/Economics/Economics-in-China":{"title":"Economics in China","links":[],"tags":[],"content":"2010\nExcerpt translated from github.com/shengcaishizhan/kkndme_tianya.\nEconomics\n\nHigh inflation and continuous depreciation of currency are determined by the state-owned economic system, which cannot be changed in a short time frame.\nThe state-owned economic system determines that\n\nGovernment administrative costs are high.\nEconomics is overly reliant on government investment.\nCost of corruption is astonishingly high.\nExport of low value-added products generates foreign exchange earnings, generating large amounts of cash flowing into market, causing inflation.\n\n\nHolding idle cash is risky, more than holding any preservable goods or assets.\nDemand for assets like bicycles, cars and houses absorbs excessive cash, which ensures commoners has to keep working hard to fight inflation, ensuring stability and prosperity of the society.\nCurrency is designed to exploit commoners via inflation.\nThe government expects property price to rise slowly and steadily, absorbing inflation pressure.\n\nInflation\n\nEmployee wages within the system grows with inflation. Those out of the system are dependent on profitability of their industry and company.\n\nMaintaining Stability\n\nThe bottom line is to ensure food security, i.e. ensuring availability of affordable food.\nTo control food pricing, cash must flow into other assets, especially property.\nPoor people buying properties with mortgage imposes risks to the financial system.\n\nProperty Rights\n\nProperty rights include the right to use (live in) the property and ownership.\nCost of living is reflected in rental prices, not in ownership.\nIt’s the ownership price that absorbs excessive cash, not rental price. Therefore rent-to-sale ratio scales with amount of excessive cash. With large amount of cash issued, rent-to-sale ratio becomes lower.\nResidential land has a lease of 70 years. Houses with 40 or 50 year lease has many disadvantages, including more expensive infrastructure cost (water, electricity, etc.)\n\n”Land Finance”\n\nValue of real estate in China is composed of the house itself and the right to use the land it is built on.\nIndividuals or organizations only have the right to own properties and use the land through ownership and usage rights.\n\nProperty Price\n\nHousing prices are not determined by the average income (e.g. as reported by the National Bureau of Statistics), but rather by the average income of the elite, because:\n\nThe pyramid structure of China society implies that even if only 10% of the population resides at the top, it still constitutes an enormous number of over 100 million people, much higher than the sum of most Western countries.\nOnly the relationship between supply and demand determines the price of a commodity.\nCommodity housing in the center of a first-or-second-tier city is sufficiently scarce for the elite population. For example, some elites in Beijing could easily invest millions in cash to buy houses.\n\n\nValue of property is determined by population, cash (demand) and its position (limited supply).\nEven if property price “falls”, it will be falling relative to other commodity price, not on its absolute value.\nDon’t worry about price fluctuation of properties bought for living. Buy it if you can.\nLong-term investment of property is relatively secure. Short-term trading still has high risk.\nWhen inflation outruns mortgage interest, long-term mortgage is profitable.\nHong Kong has the highest property pricing in China, because land there is very scarce.\n\nProperty Tax\n\nCost of property will eventually be passed on to tenants, including any tax imposed.\n\nDemolition\n\nEventually, most old houses will be demolished to build new ones.\nDemolition brings city growth.\nLow-density, short buildings are more likely to be demolished.\nTwo ways to compensate demolition: Cash compensation and Resettlement.\n\nPolitics\n\nDriven by vested interests, the development of society is inevitably shaped by those who hold power. Even if policies go against the interests of the ruling class, they will likely be difficult to enforce, with few consequences for those who fail to comply. In the end, such policies often end up as empty promises that are never fully realized.\nPRC is ruled by people, not laws. Therefore, it’s not a capitalism country.\n\nCorruption\nFrom a made-up magazine story:\n\n宇文泰以治国之道问苏绰，二人闭门密谈。\n宇文泰问曰：国何以立？\n苏绰曰：具官。\n问：何为具官？\n曰：用贪官，反贪官。\n问：既是贪官，如何能用？\n曰：为臣者，以忠为大。臣忠则君安。然，臣无利则臣不忠。但官多财寡，奈何？\n问：奈何？\n曰：君授权与之官，使官以权谋利，官必喜。\n问：善。虽官得其利，然寡人所得何在？\n曰：官之利，乃君权所授，权之所在，利之所在也，是以官必忠。官忠则江山万世可期。\n叹曰：善！然则，既用贪官，又罢贪官，何故？\n曰：贪官必用，又必弃之，此乃权术之密奥也。\n宇文泰移席，谦恭求教曰：先生教我！\n苏绰大笑：天下无不贪之官。贪，何所惧？所惧者不忠也。凡不忠者，必为异己，以罢贪官之名，排除异己，则内可安枕，外得民心，何乐而不为？此其一。其二，官若贪，君必知之，君既知，则官必恐，官愈恐则愈忠，是以罢弃贪官，乃驭官之术也。若不用贪官，何以弃贪官？是以必用又必弃之也。倘若国中皆清廉之官，民必喜，则君必危矣。\n问：何故？\n曰：清官以清廉为恃，直言强项，犯上非忠，君以何名罢弃之？罢弃清官，则民不喜，不喜则生怨，生怨则国危，是以清官不可用也。\n宇文泰大喜。\n苏绰厉声曰：君尚有问乎？\n宇文泰大惊，曰：尚……尚有乎？\n苏绰复厉色问曰：所用者皆为贪官，民怨沸腾，何如？\n宇文泰汗下，再移席，匍匐问计。\n苏绰笑曰：下旨斥之可也。一而再，再而三，斥其贪婪，恨其无状，使朝野皆知君之恨，使草民皆知君之明，坏法度者，贪官也，国之不国，非君之过，乃贪官之过也，如此则民怨可消。\n又问：果有大贪，且民怨愤极者，何如？\n曰：杀之可也。抄其家，没其财，如是则民怨息，颂声起，收贿财，又何乐而不为？要而言之：用贪官，以结其忠；罢贪官，以排异己；杀大贪，以平民愤；没其财，以充宫用。此乃千古帝王之术也。\n"},"notes/Emacs/CIDER":{"title":"CIDER","links":[],"tags":["clojure","emacs"],"content":"Starting the REPL\nStart the REPL with cider-jack-in, before evaluating any Clojure expressions.\nBasic evaluation\nLoad the buffer with cider-load-buffer (C-c C-k), and execute the top level form under point with cider-eval-defun-at-point (C-c C-c).\nC-x C-e evaluates the form preceding point. You can also use C-c C-e which binds to the same command cider-eval-last-sexp.\nC-u C-c C-c prefixes the form with #dbg, recursively inserting breakpoints to everything inside it. #break instead just places one breakpoint in front of the form.\nOnce you drop into the CIDER debugger, check out docs.cider.mx/cider/debugging/debugger.html#keys for the commands and keys available. Notably, there are:\n\nn, i and o. Step next, in, or out.\nh. Skip all sexps up to “here” (current position).\nc. Continue till next breakpoint.\nC. Continue without stopping.\nq. Quit the debugger.\ne. Eval code in current context.\np, P and l. Inspect the current value, an arbitrary expression, or local variables.\n\nInspector\nTo inspect the eval result, run cider-inspect-last-result (SPC m i r in Doom).\nIn the inspector view, use cider-inspector-pop (L in evil mode) to return to last view, and cider-inspector-next-inspectable-object (type ] ] or g j in evil mode) to jump to next inspectable object. Use cider-inspector-next-page (C-j) to jump between pages if there are too many items.\nCheatsheet\n\nM-x cider-cheatsheet-select opens minibuffer to search the cheatsheet.\n\nRefactoring (Hydra shortcut)\nhydra-cljr-help-menu (SPC m R in Doom) is a Hydra menu for quick consecutive refactoring.\n\nn am adds missing libspec. It searches cljr-magic-require-namespaces for aliases.\np ap adds project dependency. Requires existing :deps key in deps.edn.\n\n{:deps {}}\nReferences\n\ngithub.com/clojure-emacs/clj-refactor.el/blob/dc1bbc8cdaa723bdbb6669ea7d280625c370755d/clj-refactor.el#L93-L101\ndocs.cider.mx/cider/usage/misc_features.html\n"},"notes/Emacs/Emacs-Features":{"title":"Emacs Features","links":[],"tags":[],"content":"\n\nPut a call to provide at the end of each separate Lisp file.\n\n\n\nprovide and require are an alternative to autoload for loading files automatically. They work in terms of named features. Autoloading is triggered by calling a specific function, but a feature is loaded the first time another program asks for it by name.\n\nUse of require ensures that the file is only be loaded once.\nEmacs Lisp files are searched from the load path.\nReferences\n\nwww.gnu.org/software/emacs/manual/html_node/elisp/Named-Features.html\nwww.gnu.org/software/emacs/manual/html_node/elisp/Coding-Conventions.html\nwww.gnu.org/software/emacs/manual/html_node/emacs/Lisp-Libraries.html\n"},"notes/Emacs/Emacs-Frame-Setup":{"title":"Emacs Frame Setup","links":[],"tags":["emacs"],"content":"Fullscreen by default\n(add-to-list &#039;default-frame-alist &#039;(fullscreen . maximized))"},"notes/Emacs/Telegra":{"title":"Telegra","links":[],"tags":[],"content":"Cheatsheet\nEvil mode keybindings\nRoot buffer:\n\ns, telega-filter-map. u for unread, m for mention, / for reset, f for folder, t for type, s for search,\n\na for Interactively select a Chat filter to add to active filter,\ne for Edit and reapply filters list.\ni for “important”. Defaults to mentions and unmuted chats with unread messages or reactions.\n\n\nC, telega-chat-create. Interactively create new chat.\n\nChat buffer, also known as chatbuf:\n\nZa, telega-chatbuf-attach. Attach anything, including stickers.\nZf, telega-chatbuf-attach-media. Attach FILENAME as media.\nZv, telega-chatbuf-attach-clipboard. Attach clipboard to the chatbuf as photo. Requires an image in clipboard.\nzz, telega-chatbuf-recenter-1. Recenter message at point.\nM-x telega-msg-remove-text-spoiler. Remove text spoiler. Sometimes you can use &lt;RET&gt;.\n\nImage view in chatbuf:\n\n{, telega-image-prev. Show previous image in chat.\n}, telega-image-next. Show next image in chat.\n\ng A for chatbuf navigation:\n\nu, telega-chatbuf-next-unread. Goto next unread message in the chatbuf.\n&gt; or r, telega-chatbuf-read-all. Jump to the last message and mark all messages as read.\n@ or m, telega-chatbuf-next-unread-mention. Goto next unread mention or search for most recent mention before message at point.\n!, telega-chatbuf-next-unread-reaction. Goto next unread reaction in chatbuf.\no, telega-chatbuf-next-outgoing. Goto next outgoing (sent by you) message or search for last outgoing message.\n^ or P, telega-chatbuf-goto-pinned-message. Go backwards to last pinned message for the chatbuffer. If not found, go to the latest pinned message. Note: it remembers the last pinned message viewed and searches from there, outliving the chatbuf’s lifetime.\ns, telega-chatbuf-inplace-search. Search backward in the chatbuf. If C-u is given, then search forward instead.\n\ng a for Telegra prefix map:\n\nt, telega. Switch to root buffer.\nb, telega-switch-buffer. Interactively switch to an opened CHAT’s buffer.\nc, telega-chat-with. Start messaging with CHAT-OR-USER.\ns, telega-saved-messages. Switch to “Saved Messages” chatbuf.\nu, telega-switch-unread-chat. Switch to next unread message in next unread CHAT. Press again to highlight the unread message.\n\nOn message at point:\n\n!, telega-msg-add-reaction. Add reaction to MSG.\ni, telega-msg-edit. Start editing the MSG.\n&lt;tab&gt;, telega-button-forward. Move forward or backward (&lt;backtab&gt;) to next visible/active button.\nT, telega-msg-open-thread-or-topic.\n\nMSG could be a channel post, in this case open thread in the discussion group.\nOr MSG could be in supergroup, then filter messages to the corresponding thread or topic.\n\n\n\nCommon keys:\n\ng?, telega-describe-*. Show info about various items, e.g. USER, CHAT and MSG.\nq. Quit window.\n\nIn chatbuf, you may need to use C-u q to force close window.\nThis is because after a\n\npop-to-buffer (telega-chat--pop-to-buffer and telega-msg-open-thread-or-topic) or\npop-to-buffer-same-window (telega-webpage--instant-view and telega-image-view-file) call\n\n\nand returning from the pop up,\nthe quit-restore window parameter is reset to nil from (same (#&lt;buffer *Telega Root*&gt; 1 #&lt;marker at 582 in *Telega Root*&gt; 198) #&lt;window 3 on ◀ [redacted]&gt; #&lt;buffer ◀ [redacted]&gt;), where #&lt;buffer *Telega Root*&gt; is the prev-buffer that quit-window was supposed to display.\nTo workaround this, bind q to a workaround function in both maps of telega-chat-mode.\nHow to modify key bindings in chatbuf:\n\nevil-collection-set-readonly-bindings binds q in telega-msg-button-map to #&#039;quit-window.\ntelega-chat-mode’s keybindings are a combination of telega-msg-button-map and telega-chat-mode-map. The former takes precedence when a chat message is focused.\nTherefore, both maps needs to be modified.\n\n\n\n\n\nMessage bindings (cursor on message):\n\\\\{telega-msg-button-map}\nGlobal chat bindings:\n\\\\{telega-chat-mode-map}&quot;\n\nCompletion\n\ntelega-company-username, backend to complete username. Start with @.\ntelega-company-hashtag, backend to complete hashtag. Start with #.\ntelega-company-emoji, backend to complete emojis. For stickers, use Za.\ntelega-company-botcmd, backend to complete bot commands. Only completes in a bot chatbuf.\n\nDirectories\n\ntelega-cache-dir, Directory for telegram downloads.\n\nGetting Started\nInstalling Telega on macOS\n\nInstall the Symbola font and configure unicode-fonts with Apple Color Emoji.\n\n(after! unicode-fonts (push &quot;Apple Color Emoji&quot; (cadr (assoc &quot;Supplemental Symbols and Pictographs&quot; unicode-fonts-block-font-mapping))))\n\nInstall dependencies with MacPorts. Optional dependencies are\n\nqrencode for log in.\npngpaste for pasting image from clipboard.\n\n\n\nsudo port install tdlib qrencode pngpaste\n\nConfigure telega-server library prefix for tdlib.\n\n(setq telega-server-libs-prefix &quot;/opt/local&quot;)"},"notes/Emacs/autoload":{"title":"autoload","links":[],"tags":[],"content":"Magic autoload comment\n\nA magic autoload comment (often called an autoload cookie) consists of ;;;###autoload, on a line by itself, just before the real definition of the function in its autoload-able source file. The function loaddefs-generate writes a corresponding autoload call into loaddefs.el. Building Emacs loads loaddefs.el and thus calls autoload.\n\nExample:\n;;;###autoload\n(define-derived-mode ini-mode prog-mode &quot;ini&quot;\n  &quot;Major mode for editing Windows-style ini files.&quot;\n  (setq font-lock-defaults &#039;(ini-font-lock-keywords nil)))\nSame line magic\n\nYou can also use a magic comment to execute a form at build time without executing it when the file itself is loaded. To do this, write the form on the same line as the magic comment. Since it is in a comment, it does nothing when you load the source file; but loaddefs-generate copies it to loaddefs.el, where it is executed while building Emacs.\n\nExample:\n;;;###autoload(add-to-list &#039;auto-mode-alist &#039;(&quot;\\\\.ini\\\\&#039;&quot; . ini-mode))\nautoload function\nOutside of Emacs itself, Doom modules and package.el-managed packages, the magic comment ;;;###autoload doesn’t work by default.\nIn your local Emacs Lisp files, you should use the autoload function directly, or import it with use-package by configuring :load-path to load a local package and use :commands (including :hook, :bind, :mode, etc.) to let use-package handle autoload for you.\nReferences\n\nwww.gnu.org/software/emacs/manual/html_node/elisp/Autoload.html\ngithub.com/doomemacs/doomemacs/issues/1213#issuecomment-468970403\ngithub.com/emacs-mirror/emacs/blob/e81e625ab895f1bd3c5263f5b66251db0fd38bd6/lisp/emacs-lisp/package.el#L813\ngithub.com/doomemacs/doomemacs/blob/986398504d09e585c7d1a8d73a6394024fe6f164/lisp/lib/autoloads.el#L160\n"},"notes/Emacs/doom/Consult":{"title":"Consult","links":[],"tags":[],"content":"Asynchronous search\ngithub.com/minad/consult#asynchronous-search\nDoom Emacs uses the perl splitting style, which splits the input string at a punctuation character and treats each as an Emacs regular expression, so special characters like $ has to be escaped.\nTo avoid such splitting, add a backslash (\\) before spaces.\nSecond filter\n\nConsult splits the input string into two parts, if the first character is a punctuation character, like #. For example #regexps#filter-string, is split at the second #.\nThe filter-string is passed to the fast Emacs filtering to further narrow down the list of matches.\n\nNegation operator\n40 !404 searches for 40, but excludes candidates containing 403.\nSearch in file type\nArguments after -- is passed to rg. For example,\n#regexps -- -g *.go -g !*_test.go\n\nWorkaround searching for whitespace character\n#use[\\ ] -- -g *.php\n"},"notes/Emacs/doom/Dhall":{"title":"Dhall","links":[],"tags":[],"content":"Run dhall-freeze as early as possible, because dhall-mode re-evaluates the buffer on each change."},"notes/Emacs/doom/Doom-CLI":{"title":"Doom CLI","links":[],"tags":[],"content":"Commands\nWhen you need to force a full byte-compile, for example after upgrading Emacs:\ndoom sync --rebuild\nWhen your need to reclaim some disk space occupied by .emacs.d:\ndoom gc"},"notes/Emacs/doom/Eglot":{"title":"Eglot","links":[],"tags":[],"content":"Debugging\nDoom does not support Debug Adapter Protocol (DAP) debuggers with +eglot yet.\nYou can use github.com/svaante/dape instead."},"notes/Emacs/doom/Embark":{"title":"Embark","links":[],"tags":[],"content":"Start with SPC a or C-;.\nSource\ngithub.com/oantolin/embark"},"notes/Emacs/doom/Evil-Collection":{"title":"Evil Collection","links":["notes/Emacs/doom/Magit"],"tags":[],"content":"Magit key bindings\nevil-collection-magit-use-y-for-yank\n\nreplace “y” for magit-show-refs with “yy” for evil-collection-magit-yank-whole-line, “ys” for magit-copy-section-value, “yb” for magit-copy-buffer-revision and “yr” for magit-show-refs. This keeps “y” for magit-show-refs, in the help popup (magit-dispatch).\n\nevil-collection-magit-use-z-for-folds\nWhen non nil (default in Doom), use “z” as a prefix for common vim fold commands, such as\n\nz1 Reset visibility to level 1 for all sections\nz2 Reset visibility to level 2 for all sections\nz3 Reset visibility to level 3 for all sections\nz4 Reset visibility to level 4 for all sections\nza Toggle a section\nzo Show section\nzO Show sections recursively\nzc Hide section\nzC Hide sections recursively\nzr Same as z4.\n\nActivate additional evil-collection-MODULE\n(after! mpdel\n  (add-transient-hook! &#039;mpdel-mode\n    (+evil-collection-init &#039;mpdel)))"},"notes/Emacs/doom/Evil":{"title":"Evil","links":["notes/CLI/Vim"],"tags":[],"content":"yank forward\ny f SPC doesn’t yank the space character with evil-snipe-override-mode. See github.com/hlissner/evil-snipe/issues/86.\n(remove-hook &#039;doom-first-input-hook #&#039;evil-snipe-override-mode)\nCommands\nRepeat last change: .\nRepeat last substitute: &amp;\nComment lines: g c N j (comment or uncomment N lines below, defined by motion N j)\nAdd comment at end of line with space: M-; (press a afterwards to start writing)\nText objects\nSee Text Objects.\nEx-commands\nClose buffer: :bd (or use C-w d)\nKey bindings\nQuick jump in current line:\n\nIn motion state, type s and the two characters to search for (S searches backwards, and type s repeatedly to jump to next match in visible buffer)\nIn operator state, that is, waiting state after typing y, d, etc.: z and Z\n\nEasyMotion:\n\ng s s and two characters: highlight all occurrences of the two letters as jump locations.\ng s / or g s SPC and any characters combo quickly: when the idle timer expires a list of jump locations matching the combo are shown.\ng s a / g s A: jump forward or backward in the same lexical scope, such as a block.\ng s * or g s #: search for the word under cursor, forward or backward.\n\nEmbrace:\n\nS in visual state. For example, S {: make selection a new {} block.\ny s i s {: quote sentence with {, i w is a Text Objects. Use } to not insert spaces surrounding the text object.\nd s &quot;: remove &quot; quotes.\nc s &quot; &#039;: change &quot; quoted string to &#039;. As a special case, tag &lt;q&gt; can be typed as a replacement.\n"},"notes/Emacs/doom/Font-Face-Attribute-Override":{"title":"Font Face Attribute Override","links":[],"tags":[],"content":"Override the default set by tree-sitter.\n(after! tree-sitter\n  (set-face-attribute &#039;tree-sitter-hl-face:property nil :slant &#039;normal))\nReferences\n\ngithub.com/emacs-tree-sitter/elisp-tree-sitter/discussions/130\n"},"notes/Emacs/doom/Font":{"title":"Font","links":[],"tags":[],"content":"(setq doom-font (font-spec :family &quot;Cascadia Mono PL&quot; :size 12 :weight &#039;semi-light))"},"notes/Emacs/doom/How-to-Bump-a-Package":{"title":"How to Bump a Package","links":[],"tags":[],"content":"Automatic bump functions\ndoom/bump-package: bump by package name.\ndoom/bump-module: bump whole module, can select entire category or individual modules.\ndoom/commit-bumps: pre-fill magit commit for staged changes. You need to stage your changes first.\nHow to contribute (manual)\n\nMake the commit hash changes.\nEnter the magit buffer with SPC g g.\nStage the changes.\nSPC : and run doom/bumpify-diff. Alternatively, run doom/commit-bumps instead of step 4 and 5.\nc c and paste the commit message with p.\nTest updates with ~/.emacs.d/bin/doom upgrade -p.\n"},"notes/Emacs/doom/Init":{"title":"Init","links":[],"tags":[],"content":"Enabled Modules\n:ui\nindent-guides\n\n:emacs\nibuffer\n\n:term\nvterm\n\n:checkers\n(spell +flyspell)\n\n:tools\n(lookup +docsets +dictionary)\nlsp\nmagit\n"},"notes/Emacs/doom/Jumper":{"title":"Jumper","links":[],"tags":[],"content":"Natural jumper\n;; Add to config.el\n(after! better-jumper\n  (setq better-jumper-add-jump-behavior &#039;replace))"},"notes/Emacs/doom/Leader-Key":{"title":"Leader Key","links":[],"tags":[],"content":"Define new key bindings with leader key prefix\n(map! :leader\n      (:prefix (&quot;o&quot; . &quot;open&quot;)\n       :desc &quot;elfeed&quot; &quot;e&quot; #&#039;=rss))\nRemap leader keys\nChange alt key bindings:\n;; Add to config.el\n(setq doom-leader-alt-key &quot;C-SPC&quot;\n      doom-localleader-alt-key &quot;C-SPC m&quot;)\nTop-level key bindings\nPop up scratch buffer: SPC x"},"notes/Emacs/doom/Lookup":{"title":"Lookup","links":[],"tags":[],"content":"Open URLs\ng f (+lookup/file) works for URLs too.\nDocsets\nDash docsets need to be associated with major modes. Most :lang modules have this already.\n(set-docsets! &#039;tcl-mode &quot;Tcl&quot;)"},"notes/Emacs/doom/Magit":{"title":"Magit","links":[],"tags":[],"content":"Navigation\nmagit-section-backward-sibling ([) and magit-section-forward-sibling (]).\nLine-wrap\n$ is mapped to magit-process-buffer, so it’s hard to jump to end of line.\nUse SPC t w to enable line wrapping.\nCommit message\nFinish or cancel editing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActionKeyFinishC-c C-c, ZZCancelC-c C-k, ZQ\nCommit message ring\nBoth finished and cancelled messages are saved to a ring that persists until Emacs is closed. Use M-p and M-n to navigate older messages in any edit session."},"notes/Emacs/doom/Mpdel":{"title":"Mpdel","links":[],"tags":[],"content":"(use-package! mpdel\n  :config\n  (+evil-collection-init &#039;mpdel)\n  (map! :leader\n        :desc &quot;MPDel&quot; &quot;Z&quot; mpdel-core-map))"},"notes/Emacs/doom/Org-roam-v2":{"title":"Org-roam v2","links":[],"tags":[],"content":"Config\n;; in config.el\n(setq org-roam-directory &quot;~/org-roam/&quot;)\n \n(after! org-roam\n  (setq +org-roam-auto-backlinks-buffer t))\norg-roam-protocol\nFollow the Org-roam user manual to create an AppleScript application. Then save the following as a bookmarklet in Firefox:\njavascript:location.href =\n    &#039;org-protocol://roam-ref?template=r&amp;ref=&#039;\n    + encodeURIComponent(location.href)\n    + &#039;&amp;title=&#039;\n    + encodeURIComponent(document.title)"},"notes/Emacs/doom/ParEdit":{"title":"ParEdit","links":[],"tags":[],"content":"Why doesn’t Doom package ParEdit?\nFrom Doom’s FAQs\n\nWhy are there no default keybinds for Smartparens (for evil users)?\nDoom only uses smartparens to manage pair “completion” (it does the job better than electric-{pair,quote}-mode or the multitude of other pair-management solutions in the Emacs ecosystem at the time of writing).\nNone of smartparen’s commands have default keybinds for evil users because they are redundant with motions and text-objects provided by evil/vim. If you disagree, I recommend trying the :editor lispy or :editor parinfer modules.\n\nBut if you insist on using ParEdit in Doom, read on.\nInstall\nAdd following code to packages.el or enable clojure in :lang of init.el.\n(package! paredit :pin &quot;...&quot;)\nConfiguration\nAdd following code to config.el and restart Emacs.\n(autoload &#039;enable-paredit-mode &quot;paredit&quot; &quot;Turn on pseudo-structural editing of Lisp code.&quot; t)\n(add-hook! &#039;(emacs-lisp-mode-hook clojure-mode-hook) #&#039;enable-paredit-mode)\nKey bindings\nparedit-{backward,forward}-slurp-sexp (C-( and C-)) pulls new elements in from either side and paredit-{backward,forward}-barf-sexp (C-{ and C-}) pushes elements away."},"notes/Emacs/doom/Projectile":{"title":"Projectile","links":["notes/Emacs/doom/wgrep"],"tags":[],"content":"Projects\nBroadly speaking, Projectile identifies projects like this:\n\nDirectories that contain the special .projectile file\nDirectories under version control (e.g. a Git repo)\nDirectories that contain some project description file (e.g. a Gemfile for Ruby projects or pom.xml for Java maven-based projects)\n\nProject-wide actions\n\nFind file: SPC p f\n\nInvalidate cache: SPC p i\n\n\nSave all: SPC p s\nProject-wide search and replace\n"},"notes/Emacs/doom/Rime":{"title":"Rime","links":[],"tags":[],"content":"Setup\n;; Add to packages.el\n(package! pyim :disable t)\n(package! rime)\n;; Add to config.el\n(use-package! rime\n  :after-call after-find-file pre-command-hook\n  :init\n  (setq rime-emacs-module-header-root &quot;/Applications/MacPorts/Emacs.app/Contents/Resources/include&quot;\n        rime-librime-root &quot;/opt/local&quot;)\n  :config\n  (setq rime-show-candidate &#039;popup\n        rime-disable-predicates\n        &#039;(rime-predicate-evil-mode-p\n          rime-predicate-prog-in-code-p)\n        default-input-method &quot;rime&quot;))"},"notes/Emacs/doom/Straight":{"title":"Straight","links":[],"tags":[],"content":"\nOut of the box, you can install any package listed on MELPA, GNU ELPA, or Emacsmirror, which is to say any package in existence. (Although MELPA is used as a package listing, packages are installed by cloning their Git repositories rather than by downloading tarballs like package.el does.)\n\nMost of the time you don’t need to specify a :recipe."},"notes/Emacs/doom/Undo":{"title":"Undo","links":[],"tags":[],"content":"Disable undo history persistence\n;; Add in config.el\n(remove-hook &#039;undo-fu-mode-hook #&#039;global-undo-fu-session-mode)"},"notes/Emacs/doom/Vertico":{"title":"Vertico","links":["notes/Emacs/doom/Consult","notes/Emacs/doom/Embark"],"tags":[],"content":"Key bindings in minibuffer\nPreview: C-SPC for +vertico/embark-preview\nNext &amp; previous history: M-p and M-n\nPage up &amp; down: C-S-j and C-S-k (C-S- does not work in tty)\nNext &amp; previous group: C-M-j (also TAB) and C-M-k\nNext &amp; previous row: C-j and C-k\nExit with input: M-RET (vertico-exit-input) or RET (exit-minibuffer)\nKey bindings in minibuffer input\nInsert selection to minibuffer (eval expression, M-x, or ex-command, i.e. colon-commands): TAB\nSearch pattern\nConsult\nSearch in all key bindings\nUsually SPC h b b, but use C-h b b or &lt;doom-leader-alt-key&gt; instead of SPC when leader key (SPC) is not available, for example in a vertico minibuffer:\nscroll-up-command             &lt;kp-next&gt;, C-S-j\nvertico-scroll-up             &lt;remap&gt; &lt;scroll-up-command&gt;\n\nCopy candidate list\nC-; E or C-c C-; in minibuffer to export candidates to a new buffer with Embark.\ncompleting-read-multiple (crm)\nPowers multi-select feature in magit.\n\n+vertico/crm-select: TAB\n+vertico/crm-exit: RET\n+vertico/crm-select-keep-input: S-TAB\n"},"notes/Emacs/doom/Window":{"title":"Window","links":[],"tags":[],"content":"SPC w o (doom/window-enlargen)\nSPC w C-o (delete-other-windows)\nSPC w d (+workspace/close-window-or-workspace), also s-w and SPC w c\nSPC w q (evil-quit = Z Q)\nNote: SPC w and C-w are interchangeable."},"notes/Emacs/doom/Workspace":{"title":"Workspace","links":[],"tags":[],"content":"Key bingings\nQuick switch: s-{0..9}"},"notes/Emacs/doom/cc":{"title":"cc","links":[],"tags":[],"content":"LSP (preferred)\n;; in init.el\n:lang\n(cc +lsp)\n\nIrony-Mode\nIt does not work out of the box on macOS, but it is possible to put additional header flags to ~/.clang_complete, as long as no intermediate parent directory contains .clang_complete or compile_flags.txt.\n(defun irony-cdb-clang-complete--locate-db ()\n  (when buffer-file-name\n    (catch &#039;fname\n      (locate-dominating-file\necho | clang -x c++ -v -E - 2&gt;&amp;1 | sed -n &#039;/^#include &lt;/,/^End/s|^[^/]*\\([^ ]*/include[^ ]*\\).*$|-I\\1|p&#039; &gt; ~/.clang_complete\nrtags\nrtags also does not work out of the box on macOS, requiring users to configure ~/.rdmrc.\n--isystem=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1\n--isystem=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/12.0.0/include\n&lt;...&gt;\n"},"notes/Emacs/doom/chinese":{"title":"chinese","links":["notes/Emacs/doom/Rime"],"tags":[],"content":"How to enable\n;; Uncomment in init.el\n(doom! :input\n       chinese\n \n       :ui\n       unicode\n       ...)\nInput methods\npyim\nEnabled by default in chinese module.\nemacs-rime\nRime"},"notes/Emacs/doom/defcustom":{"title":"defcustom","links":[],"tags":[],"content":"Use setq in (after! ...) to set variables for a packge.\nIf a variable is defined with defcustom and has an associated custom-set form, use setq! instead of setq in doom to execute it."},"notes/Emacs/doom/lispy":{"title":"lispy","links":[],"tags":[],"content":"Recommendation\nLispy overrides some keys in insert mode if the cursor is immediately before ( or after ).\nIf you are proficient in Vim, maybe you don’t need this module.\nKey bindings\nMovement: h j k l as usual, f to step into and b to move back in history\nSlurp: &gt;\nBarf: &lt;\nClone: c\nForward: ]\nBackward: [\nJump to matching pair: d\nSplit: M-j\nJoin: +\nTyping &quot; inside of a string inserts \\&quot;\\&quot;, so the split command is very useful for string templating.\nInsert\nParens: (\nBraces: {\nBrackets: }\nQuotes: &quot;\nLispyVille\nThe default key theme:\n  :init\n  (setq lispyville-key-theme\n        &#039;((operators normal)\n          c-w\n          (prettify insert)\n          (atom-movement t)\n          slurp/barf-lispy\n          additional\n          additional-insert))\n\nEach  (key-theme modes) is a pair of a LispyVille key theme and modes in which to bind them. For example, (escape insert) binds the “escape” theme in just the insert state. Without (), the default is to bind normal and visual states.\nAs described in github.com/abo-abo/lispy/issues/534, for arrow keys to work in xterm, M-O must be free from bindings in Emacs.\nHowever, package lispyville binds both M-o and M-O keys to its functions with evil-define-key*. Unbind them with the following config.\n(map! :after lispyville\n      :map lispyville-mode-map\n      :n &quot;M-O&quot; nil\n      :n &quot;M-o&quot; nil)\nGiven the order Doom Emacs’ config gets loaded,\n\nprivate/&lt;user-login-name&gt;/config.el is loaded automatically, after all other modules. Keeping :private &lt;user-login-name&gt; in init.el is harmless, but does nothing.\nprivate/&lt;user-login-name&gt;/init.el is loaded before all modules.\nIf your needs are simple, use after! to reconfigure packages.\n\nwe could also configure lispyville-key-theme in config.el, but remember to put it in at top-level, not within after!.\n(setq lispyville-key-theme\n      &#039;((operators normal)\n        c-w\n        (prettify insert)\n        (atom-movement t)\n        slurp/barf-lispy\n        additional\n        ;; additional-insert ; disabled for tty\n        ))\nReferences\n\ngithub.com/doomemacs/doomemacs/issues/88#issuecomment-306222820\n"},"notes/Emacs/doom/lsp-mode":{"title":"lsp-mode","links":["notes/Emacs/doom/Projectile"],"tags":[],"content":"Multi root servers\nMulti-root means that one server can handle multiple projects, but the default behavior of lsp-mode is to load all projects for this language server at once.\nTo make it load folders on demand, add the following in (after! lsp-mode ...).\n(advice-add &#039;lsp :before (lambda (&amp;rest _args) (eval &#039;(setf (lsp-session-server-id-&gt;folders (lsp-session)) (ht)))))\nEven if all projects are loaded, cross-project references does not work. Move related projects into a workspace folder and create the special .projectile file to let Projectile treat it as a whole.\nState files\nSession information is stored in the file pointed to by lsp-session-file.\nTo remove a project from history, run lsp-workspace-folders-remove.\nTo list all current projects, run lsp-describe-session.\nDebugging\ndap-mode is another package hosted under the emacs-lsp organization. It is integrated with and depends on lsp-mode."},"notes/Emacs/doom/rust":{"title":"rust","links":[],"tags":[],"content":"Apheleia\n:editor format wraps Apheleia, which does not support cargo fmt yet. Directly calling rustfmt means the edition specified in Cargo.toml is ignored, and rustfmt may throw an error if unsupported syntax is used.\nA workaround is to add a rustfmt.toml file, as described in github.com/radian-software/apheleia/issues/278.\n# rustfmt.toml\nedition = &quot;2021&quot;\nEglot\nWith rust-analyzer, hyperlinks from markdown hover text (get-text-property (point) &#039;help-echo) are not clickable.\nThis is supposedly fixed in Emacs (Eglot) commit cb562118cb14, but does not work with rustic specifically.\nIssue description is similar to github.com/joaotavora/eglot/issues/865, but it’s rustic’s problem here."},"notes/Emacs/doom/tree-sitter":{"title":"tree-sitter","links":["notes/Emacs/doom/Straight"],"tags":[],"content":"Use existing *-ts-mode package\nFirst, install packages in package.el.\n(package! tree-sitter-langs\n  :pin &quot;3a3ad0527d5f8c7768678878eb5cfe399bedf703&quot;)\n \n(package! typst-ts-mode\n  :recipe (:host sourcehut :repo &quot;l2dy/typst-ts-mode&quot;)\n  :pin &quot;39a9e63c019bd5498c8f0e5a7ee8cb9d6bb53fd0&quot;)\nIf package depends on treesit instead of tree-sitter, it’s required to copy and rename parser library files.\n;; config.el\n \n(defun treesit-copy-langs-lib-if-newer (lang)\n  &quot;Copy treesit libraries from tree-sitter-langs&quot;\n  (require &#039;tree-sitter-langs)\n  (let* ((ts-lib-path (car (file-expand-wildcards (concat (tree-sitter-langs--bin-dir) lang &quot;.*&quot;))))\n         (ts-lib-name (file-name-nondirectory ts-lib-path))\n         (ts-lib-user-dir (concat user-emacs-directory (file-name-as-directory &quot;tree-sitter&quot;)))\n         (ts-lib-user-path (concat ts-lib-user-dir &quot;libtree-sitter-&quot; ts-lib-name)))\n    (when (or (not (file-exists-p ts-lib-user-path))\n              (time-less-p (file-attribute-modification-time (file-attributes ts-lib-user-path))\n                           (file-attribute-modification-time (file-attributes ts-lib-path))))\n      (make-directory ts-lib-user-dir t)\n      (delete-file ts-lib-user-path)\n      (copy-file ts-lib-path ts-lib-user-path nil t))))\n \n(use-package! typst-ts-mode\n  :mode &quot;\\\\.typ\\\\&#039;&quot;\n  :config\n  (treesit-copy-langs-lib-if-newer &quot;typst&quot;))\nUsing pre-compiled language grammars\n\nYou can find them on his Github releases page. You can also download the tree-sitter-langs package from MELPA, but I recommend you just download the shared libs directly instead, as you’ll in any event have to rename them and place the grammar libraries somewhere else.\nThe names of the files are &lt;LANGUAGE&gt;.so (or with your platform’s equivalent extension) which is not in keeping with the expected naming style in Emacs. You must first rename them so they’re named libtree-sitter-&lt;LANGUAGE&gt;.so. This is as good a time as any to learn how to bulk rename them with Emacs’s M-x dired and the editable dired buffers feature.\n\nDefine your own new major mode for highlighting\nSupported languages: github.com/emacs-tree-sitter/tree-sitter-langs/tree/master/queries\n\nEnsure (package! tree-sitter-langs ...) pins to the right commit (or a later commit) in packages.el and tree-sitter module is enabled in init.el.\nRun doom sync if pin is updated, and verify that updates are applied to local Straight directories.\nDefine the major mode in config.el, or a custom Doom module with define-derived-mode split into autoload.el.\n\n;;;###autoload\n(define-derived-mode typst-mode text-mode &quot;Typst&quot;\n  &quot;Major mode for editing Typst documents.&quot;\n  (set-tree-sitter-lang! &#039;typst-mode &#039;typst) ;; only required if not in tree-sitter-langs--init-major-mode-alist\n  (add-hook &#039;typst-mode-local-vars-hook #&#039;tree-sitter! &#039;append))\n \n(add-to-list &#039;auto-mode-alist &#039;(&quot;\\\\.typ\\\\&#039;&quot; . typst-mode))\nTo enable LSP instead, replace #&#039;tree-sitter! with #&#039;lsp!.\n\nFor Elisp gurus: Doom provides MAJOR-MODE-local-vars-hook, and we use it instead of MAJOR-MODE-hook because it runs later in the mode’s startup process (giving other functionality or packages — like direnv — time to configure the LSP client).\n\nLazy loading\nCheck if tsc-dyn-get-ensure is defined on start to verify lazy loading.\nLinux AArch64 support\nSee also github.com/emacs-tree-sitter/elisp-tree-sitter/issues/166#issuecomment-1522766887.\nAdd the following to config.el, which is equivalent to use-package’s :init.\n;; No binaries are provided for tree-sitter on AArch64 Linux\n(setq tsc-dyn-get-from &#039;(:compilation))\n\nThis does not fix missing binaries of tree-sitter-langs, so you need to build them yourself.\nReferences\n\ngithub.com/doomemacs/doomemacs/issues/1213#issuecomment-468970403\nwww.masteringemacs.org/article/how-to-get-started-tree-sitter\ngithub.com/doomemacs/doomemacs/blob/master/docs/faq.org#turn-doom-emacs-into-a-insert-language-here-ide-with-lsp\n"},"notes/Emacs/doom/tty":{"title":"tty","links":[],"tags":[],"content":"+osc\nIn Mosh, SSH_TTY is left as is and points to a dead TTY. unset SSH_TTY in .zprofile or read tty from tmux display-message -p &#039;#{pane_tty}&#039; in a nested tmux session."},"notes/Emacs/doom/wgrep":{"title":"wgrep","links":[],"tags":[],"content":"SPC s p foo C-; E i :%s/././ RET Z Z\n\nSPC-s-p foo invokes project-wide search\nC-; E exports the results into a wgrep buffer\ni &lt;...buffer edits&gt; RET Z Z makes the buffer editable and saves the changes.\n\nhungyi.net/posts/doom-emacs-search-replace-project/"},"notes/Emacs/emacsclient-on-macOS":{"title":"emacsclient on macOS","links":[],"tags":["emacs","macos"],"content":"Starting Emacs server automatically\nConfigure a macOS launch agent with the plist file and load it with launchctl.\nlaunchctl load -w ~/Library/LaunchAgents/gnu.emacs.daemon.plist\n\n&lt;!-- Put following in ~/Library/LaunchAgents/gnu.emacs.daemon.plist --&gt;\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; &quot;www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt;\n&lt;plist version=&quot;1.0&quot;&gt;\n&lt;dict&gt;\n    &lt;key&gt;Label&lt;/key&gt;\n    &lt;string&gt;gnu.emacs.daemon&lt;/string&gt;\n \n    &lt;key&gt;ProgramArguments&lt;/key&gt;\n    &lt;array&gt;\n        &lt;string&gt;/Applications/MacPorts/Emacs.app/Contents/MacOS/Emacs&lt;/string&gt;\n        &lt;string&gt;--fg-daemon&lt;/string&gt;\n    &lt;/array&gt;\n \n    &lt;key&gt;RunAtLoad&lt;/key&gt;\n    &lt;true/&gt;\n \n    &lt;key&gt;ProcessType&lt;/key&gt;\n    &lt;string&gt;Interactive&lt;/string&gt;\n&lt;/dict&gt;\n&lt;/plist&gt;\nEmacs client with AppleScript\nSave the following in Script Editor.app as an Application.\ndo shell script &quot;/Applications/MacPorts/Emacs.app/Contents/MacOS/bin/emacsclient -c -n -q&quot;\n\n⚠ Warning: Without -n the applet won’t quit and prevents system sleep.\n"},"notes/Emacs/mu4e/Prefer-Plain-Text-in-mu4e":{"title":"Prefer Plain Text in mu4e","links":[],"tags":["emacs","mu4e"],"content":"\nBy default, mu4e tries to display the ’richest’ option, which is the last MIME-part of the alternatives. You can customize this to prefer the text version, if available, with something like the following in your configuration (and see the docstring for mm-discouraged-alternatives for details)\n\n(with-eval-after-load &quot;mm-decode&quot;\n  (add-to-list &#039;mm-discouraged-alternatives &quot;text/html&quot;)\n  (add-to-list &#039;mm-discouraged-alternatives &quot;text/richtext&quot;))\nwww.djcbsoftware.nl/code/mu/mu4e/MSGV-Rich_002dtext-and-images.html#MSGV-Rich_002dtext-and-images"},"notes/Emacs/mu4e/mu4e-Hotkeys":{"title":"mu4e Hotkeys","links":[],"tags":["emacs","mu4e"],"content":"M-&lt;up&gt;: move to previous message\nM-&lt;down&gt;: move to next message"},"notes/English-Terms/Conference-Talks":{"title":"Conference Talks","links":[],"tags":[],"content":"During Q&amp;A, if you didn’t hear a question clearly, say “excuse me” to politely ask the questioner to repeat."},"notes/English-Terms/Data-Center-Terms":{"title":"Data Center Terms","links":[],"tags":[],"content":"Architecture\n\nredundant and independent network connections\nactive-active redundant\ndisaster recovery facility\n\nMaintenance\n\nincident\nevents\n\nPower\n\nunplanned maintenance event affecting one of xxx\nindependent power feeds\nutility power feeds\nutility company\npower up generators\nsupplement the feed that was down\nfail over to generator power\nsource of power\nground fault\ntransformer\n"},"notes/HTTP/CORS-Credentials":{"title":"CORS Credentials","links":[],"tags":[],"content":"\nThe CORS request was attempted with the credentials flag set, but the server is configured using the wildcard (”*”) as the value of Access-Control-Allow-Origin, which doesn’t allow the use of credentials.\n\ndeveloper.mozilla.org/en-US/docs/Web/HTTP/CORS/Errors/CORSNotSupportingCredentials"},"notes/HTTP/CORS-Origin":{"title":"CORS Origin","links":[],"tags":[],"content":"Access-Control-Allow-Origin: &lt;origin&gt;\nYou can only allow one specific origin. If the server needs to support clients from multiple origins, it must return the origin for the specific client making the request."},"notes/HTTP/HTTP-Request-RTTs":{"title":"HTTP Request RTTs","links":[],"tags":[],"content":"\nPhase 1: DNS Lookup\nPhase 2: TCP Handshake (1 RTT)\nPhase 3: TLS Handshake (1 RTT if TLS 1.3)\n\n2 round trips if TLS version &lt; 1.3\n\n\nPhase 4: HTTP (1 RTT + server-side latency)\n"},"notes/HTTP/Preflight-Request":{"title":"Preflight Request","links":[],"tags":[],"content":"It’s an OPTIONS request, using three HTTP request headers: Access-Control-Request-Method, Access-Control-Request-Headers, and the Origin header.\nFor example:\nOPTIONS /resource/foo\nAccess-Control-Request-Method: DELETE\nAccess-Control-Request-Headers: origin, x-requested-with\nOrigin: foo.bar.org"},"notes/Health/20-20-20-Rule":{"title":"20-20-20 Rule","links":[],"tags":[],"content":"Every 20 minutes spent using a screen, you should try to look away at something that is 20 feet away from you for a total of 20 seconds."},"notes/Kubernetes/CRI-Logging-Format":{"title":"CRI Logging Format","links":[],"tags":[],"content":"\nThe runtime should decorate each log entry with a RFC 3339Nano timestamp prefix, the stream type (i.e., “stdout” or “stderr”), the tags of the log entry, the log content that ends with a newline.\nThe tags fields can support multiple tags, delimited by :. Currently, only one tag is defined in CRI to support multi-line log entries: partial or full. Partial (P) is used when a log entry is split into multiple lines by the runtime, and the entry has not ended yet. Full (F) indicates that the log entry is completed — it is either a single-line entry, or this is the last line of the multiple-line entry.\nFor example,\n2016-10-06T00:17:09.669794202Z stdout F The content of the log entry 1\n2016-10-06T00:17:09.669794202Z stdout P First line of log entry 2\n2016-10-06T00:17:09.669794202Z stdout P Second line of the log entry 2\n2016-10-06T00:17:10.113242941Z stderr F Last line of the log entry 2\n\nWith the knowledge, kubelet can parse the logs and serve them for kubectl logs requests. This meets requirement (3). Note that the format is defined deliberately simple to provide only information necessary to serve the requests. We do not intend for kubelet to host various logging plugins. It is also worth mentioning again that the scope of this proposal is restricted to stdout/stderr streams of the container, and we impose no restriction to the logging format of arbitrary container logs.\n\nReferences\n\ngithub.com/kubernetes/design-proposals-archive/blob/acc25e14ca83dfda4f66d8cb1f1b491f26e78ffe/node/kubelet-cri-logging.md#proposed-solution\n"},"notes/Kubernetes/Flux-Helm-Release-Troubleshooting":{"title":"Flux Helm Release Troubleshooting","links":[],"tags":[],"content":"Release stuck in pending-upgrade\nYour best option is to wait till timeout passes, reconciliation will sort it out.\nIf the current release revision is invalid and you can’t wait, rollback to a known good one, and let it reconcile again.\nhelm rollback -n flux-system &lt;RELEASE&gt; &lt;known good REVISION&gt;\n\nDeleting a HelmRelease\nKustomization in kustomize.toolkit.fluxcd.io/v1 defines the timeout and retryInterval of reconciliation.\nA deleted HelmRelease during reconciliation is considered a timeout failure and gets re-applied by the controlling Kustomization.\nerror Kustomization/infra-controllers.flux-system - Reconciliation failed after 5m1.877215018s, next try in 1m0s Health check failed after 5m0.030168483s: timeout waiting for: [HelmRelease/flux-system/victoria-metrics-alert status: &#039;NotFound&#039;]\ninfo Kustomization/infra-controllers.flux-system - server-side apply for cluster definitions completed\ninfo HelmRelease/victoria-metrics-alert.flux-system - HelmChart &#039;flux-system/flux-system-victoria-metrics-alert&#039; is not ready\n\nHelmChart not ready\nRun flux reconcile source helm &lt;repo&gt; to fetch latest charts from repo.\nHelmChart &#039;xxx&#039; is not ready\n\nForce reconcile\nIf reconciliation is stuck, you could try suspend and resume.\nflux suspend helmrelease &lt;name&gt;\nkubectl get -n flux-system helmrelease/&lt;name&gt; -o yaml # verify config\nflux resume helmrelease &lt;name&gt; # start reconcile"},"notes/Kubernetes/Flux":{"title":"Flux","links":["notes/Kubernetes/Flux-Helm-Release-Troubleshooting"],"tags":[],"content":"HelmRelease\nPost renderer\nfluxcd.io/flux/components/helm/helmreleases/#post-renderers\nHelmRelease resources has a built-in Kustomize compatible Post Renderer, which provides some Kustomize directives.\nNote that the patchesStrategicMerge and patchesJson6902 directive is deprecated, just use patches instead.\nTroubleshooting\nExperiment with helm template locally first. See also Flux Helm Release Troubleshooting.\nSecrets\nK8s Secrets\nManage with sealed-secrets.\nWith kubeseal, secrets can be safely committed in Git. After reconciliation, use kubectl get -A sealedsecret to check decryption status.\nReference Secrets from HelmRelease\nSecrets used in valuesFrom should be put into the same namespace as the HelmRelease. kubeseal encryption is associated with cluster namespace, so if you got it wrong, it has to be re-encrypted.\nNote that targetPath in arrays like array[0].name is not supported. See github.com/helm/helm/issues/8320.\nDeploy credentials rotation\nFine-grained PAT from GitHub only lasts for a year.\nTo rotate the SSH key generated at bootstrap, first delete the secret from the cluster with:\nkubectl -n flux-system delete secret flux-system\nThen run flux bootstrap again.\nCommon Debug Commands\nflux logs --tail 10 -f\nHelm controller Logs\nkubectl describe -n flux-system helmrelease [name]\nRetry Helm install\nflux reconcile source helm [name] # Update repo index\nflux reconcile hr [name] # Attempt deploy\n \n# If retries are exhausted\nflux suspend hr [name]\nflux resume hr [name]\nRestore from fault\nthis namespace may not be deleted\nflux delete kustomization node-feature-discovery\n# wait for resources being deleted\n \n# force kill\nflux suspend kustomization node-feature-discovery\nRun flux suspend kustomization &lt;name&gt; to force delete a Kustomization in deletion that mistook a resource it did not own as its own.\n2024-10-01T00:00:00Z error Kustomization/node-feature-discovery.flux-system - Reconciler error delete failed, errors: Namespace/kube-system delete failed: namespaces &quot;kube-system&quot; is forbidden: this namespace may not be deleted;\n"},"notes/Kubernetes/Grafana-Kubernetes-Monitoring-Helm-Charts":{"title":"Grafana Kubernetes Monitoring Helm Charts","links":[],"tags":[],"content":"Metric collection optimization\n\nDisable OpenCost if not needed.\nDisable collection of tmpfs filesystem, ipvs, and veth device metrics.\nRetain most metrics from node_exporter.\n\nmetrics:\n  cost:\n    enabled: false\n  node-exporter:\n    allowList: []\n    extraMetricRelabelingRules: |\n      rule {\n        source_labels = [&quot;__name__&quot;]\n        regex = &quot;node_scrape_collector_.+&quot;\n        action = &quot;drop&quot;\n      }\nprometheus-node-exporter:\n  extraArgs:\n    - --no-collector.ipvs\n    - --collector.netclass.ignored-devices=^(veth.*|cali.*|[a-f0-9]{15})$\n    - --collector.netdev.device-exclude=^(veth.*|cali.*|[a-f0-9]{15})$\n    # Add tmpfs to defaults\n    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|tmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\nopencost:\n  enabled: false"},"notes/Kubernetes/Helm-Chart-Releaser":{"title":"Helm Chart Releaser","links":[],"tags":[],"content":"\nCreate gh-pages branch first.\nCommit Actions workflow before charts directory.\nRemote repos referenced in Chart.lock needs to be added before executing the chart-releaser step.\nCommit your charts directories under /charts.\n\nReferences\n\ngithub.com/marketplace/actions/helm-chart-releaser\n"},"notes/Kubernetes/K3s-Exit-Node-Bypass-Route":{"title":"K3s Exit Node Bypass Route","links":[],"tags":[],"content":"K3s Flannel CNI\nConfigure dependency chain as follows. Only the dependent services (k3s-agent.service in this case) need to be customized.\ntailscale.service\n  -&gt; k3s-agent.service\n     [Unit] After=network-online.target tailscaled.service # Start sequence\n     [Unit] BindsTo=tailscaled.service # Propagate stop and restart\n     [Service] PostStart=-ip route add throw 10.42.0.0/16 table 52 # Ignore &quot;RTNETLINK answers: File exists&quot; failure\n\nWhen tailscaled restarts, Flannel routes are deleted if Tailscale exit node route is activated, so k3s-agent has to be restarted too.\nThis can be implemented with BindsTo=, but Tailscale can take some time to set up the interface, so the 100.x node-ip must be specified via arguments explicitly."},"notes/Kubernetes/K3s-External-Database-HA":{"title":"K3s External Database HA","links":[],"tags":[],"content":"Connection pooling\nPooling is implemented but config is not exposed to k3s.\nThe default is maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s.\nPostgreSQL\nConnection string\npostgres://username:password@hostname:port/database-name?sslmode=verify-full\n\nWith --datastore-cafile, sslmode is automatically set to verify-full. The certificate files will not be copied, so it’s best to specify an absolute location.\nMySQL\nTLS verify\nIf cert, key and CA files are not specified, append ?tls=true to the datastore endpoint to enforce TLS.\nOtherwise, Kine sets a custom TLS config that always verify server certificates.\n// func (c Config) ClientConfig() (*tls.Config, error)\nif c.CertFile == &quot;&quot; &amp;&amp; c.KeyFile == &quot;&quot; &amp;&amp; c.CAFile == &quot;&quot; {\n\treturn nil, nil\n}\n// func prepareDSN(dataSourceName string, tlsConfig *cryptotls.Config)\nconfig, err := mysql.ParseDSN(dataSourceName)\n \nif tlsConfig != nil {\n\t[...snip...]\n\tconfig.TLSConfig = &quot;kine&quot;\n}\n \nparsedDSN := config.FormatDSN()\n// func FormatDSN()\nif len(cfg.TLSConfig) &gt; 0 {\n\twriteDSNParam(&amp;buf, &amp;hasParam, &quot;tls&quot;, url.QueryEscape(cfg.TLSConfig))\n}\nManaged database\nOn managed MySQL databases, CREATE DATABASE might not be supported. For example, Vitess without database creator plugins will invoke the failDBDDL plugin, which always fails.\nReferences\n\ndocs.k3s.io/datastore#datastore-endpoint-format-and-functionality\ngithub.com/k3s-io/k3s/issues/1093\nwww.suse.com/support/kb/doc/\ngithub.com/k3s-io/kine/issues/63\n"},"notes/Kubernetes/K3s-HA-Cluster":{"title":"K3s HA Cluster","links":["notes/Kubernetes/K3s-etcd-HA","notes/Kubernetes/K3s-External-Database-HA","notes/Network/VXLAN"],"tags":[],"content":"Datastores\nK3s etcd HA: best reliability, but has strict performance and latency constraints.\nK3s External Database HA: depends reliable access to the database.\nNetwork\nThe default backend for Flannel is VXLAN, which is a UDP-based encapsulation technique without encryption.\nWireGuard backend for Flannel\nWith this configuration, K3s supervisor traffic uses a websocket tunnel and cluster (CNI) traffic uses a wireguard tunnel.\nOn servers:\n--node-external-ip=&lt;SERVER_EXTERNAL_IP&gt; \\\n--flannel-backend=wireguard-native \\\n--flannel-external-ip\n\nOn agents:\n--node-external-ip=&lt;AGENT_EXTERNAL_IP&gt;\n\nDual-Stack Network\n--cluster-cidr=10.42.0.0/16,fd00:42::/56\n--service-cidr=10.43.0.0/16,fd00:43::/112\n\nCA validation\nUse the complete token from /var/lib/rancher/k3s/server/node-token for Cluster CA validation.\nlevel=warning msg=&quot;Cluster CA certificate is not trusted by the host CA bundle, but the token does not include a CA hash. Use the full token from the server&#039;s node-token file to enable Cluster CA validation.&quot;\n\nAgent registration\nK3s only adds host IP addresses by default. For a fixed registration address like a load-balanced VIP, you should specify --tls-san. This address is used for agent nodes (not part of control plane) to join the cluster.\n\nTo avoid certificate errors with the fixed registration address, you should launch the server with the tls-san parameter set. This option adds an additional hostname or IP as a Subject Alternative Name in the server’s TLS cert, and it can be specified as a list if you would like to access via both the IP and the hostname.\n\n\n\nOptional: A fixed registration address for agent nodes to register with the cluster\n\n\nExample result:\nSubject Alternative Name (not critical):\n\tDNSname: kubernetes\n\tDNSname: kubernetes.default\n\tDNSname: kubernetes.default.svc\n\tDNSname: kubernetes.default.svc.cluster.local\n\tDNSname: localhost\n\tDNSname: &lt;hostname&gt;\n\tIPAddress: 10.43.0.1\n\tIPAddress: 127.0.0.1\n\tIPAddress: &lt;tls-san IP&gt;\n\tIPAddress: &lt;IPv4 IP&gt;\n\tIPAddress: &lt;IPv6 IP&gt;\n\tIPAddress: ::1\n\nIPv6\nAPI server\nThe Go standard library defaults to listen on both IPv4 and IPv6 when listen address is 0.0.0.0 and network is tcp, so the API server port is OK.\nLiteral IPv6 address can be used in --tls-san. See cnRegexp and populateCN() in go/pkg/mod/github.com/rancher/dynamiclistener@v0.3.6-rc2/factory/gen.go.\nSELinux\nUsing a custom --data-dir under SELinux isn’t supported. To customize it, you would most likely need to write your own custom policy.\nFor most users, it’s not worth the effort and --selinux shouldn’t be used.\nReferences\n\ngithub.com/k3s-io/k3s/issues/2850 (etcd latency expectations)\ndocs.k3s.io/installation/network-options#distributed-hybrid-or-multicloud-cluster\n"},"notes/Kubernetes/K3s-System-HelmChart":{"title":"K3s System HelmChart","links":[],"tags":[],"content":"Chart deploy logs\nkubectl describe -n kube-system HelmChart/traefik shows that HelmCharts are applied via Jobs.\n  Type    Reason    Age   From             Message\n  ----    ------    ----  ----             -------\n  Normal  ApplyJob  1m    helm-controller  Applying HelmChart using Job kube-system/helm-install-traefik\n\nYou can check the Helm logs with kubectl logs.\nkubectl logs -n kube-system Job/helm-install-traefik"},"notes/Kubernetes/K3s-Traefik":{"title":"K3s Traefik","links":["notes/Kubernetes/K3s-System-HelmChart"],"tags":[],"content":"Helm config\nYou can override default values with HelmChartConfig. To troubleshoot install issues, check Deploy Logs.\napiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n  name: traefik\n  namespace: kube-system\nspec:\n  valuesContent: |-\n    affinity:\n      podAntiAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: &#039;{{ template &quot;traefik.name&quot; . }}&#039;\n                app.kubernetes.io/instance: &#039;{{ .Release.Name }}-{{ .Release.Namespace }}&#039;\n            topologyKey: kubernetes.io/hostname\n    deployment:\n      replicas: 8 # must be less than number of nodes, given the anti-affinity rule.\n      revisionHistoryLimit: 0 # discard old ReplicaSets. Does NOT work.\n    service:\n      spec:\n        externalTrafficPolicy: Local\n    ports:\n      websecure:\n        proxyProtocol:\n          enabled: true\n          trustedIPs:\n            - 10.0.0.0/8 # replace with actual IPs in Local externalTrafficPolicy mode\n    logs:\n      access:\n        enabled: true\n\nrevisionHistoryLimit: 0 does not work because 0 is treated as false like absent value. See github.com/helm/helm/issues/3164.\nIf number of replicas is equal to number of nodes, you may have to manually delete old ReplicaSets for new ones to be scheduled.\nkubectl delete -n kube-system ReplicaSet/traefik-xxx"},"notes/Kubernetes/K3s-etcd-HA":{"title":"K3s etcd HA","links":[],"tags":[],"content":"IPv6\netcd cluster\nK3s embedded etcd only listens on the IPv4 node IP. Use --node-ip to override the default.\nWhen --node-ip is an IPv6 address, clusterCIDR is also made IPv6.\nGiven this side effect, it may be better to manage etcd separately if nodes are not reachable via IPv4.\nReferences\n\nranchermanager.docs.rancher.com/how-to-guides/new-user-guides/kubernetes-cluster-setup/rke2-for-rancher\ngithub.com/k3s-io/k3s/blob/56abe7055fca997daf2bd88e67b1ddcb148c41b9/pkg/cli/server/server.go#L297-L305\n"},"notes/Kubernetes/K8s-Metrics":{"title":"K8s Metrics","links":[],"tags":[],"content":"Kubelet metrics\nkubectl get --raw can make raw HTTP requests to the API server. Alternatively, you could use curl via kubectl proxy.\nkubectl get --raw /api/v1/nodes/&lt;node&gt;/proxy/metrics\nkubectl get --raw /api/v1/nodes/&lt;node&gt;/proxy/metrics/cadvisor"},"notes/Kubernetes/K8s-Secrets":{"title":"K8s Secrets","links":[],"tags":[],"content":"\nNote: if you do not want to perform the base64 encoding, you can choose to use the stringData field instead.\n\ndata in a Secret must be base64-encoded."},"notes/Kubernetes/K8s-Volumes":{"title":"K8s Volumes","links":[],"tags":[],"content":"securityContext\n\nBy default, Kubernetes recursively changes ownership and permissions for the contents of each volume to match the fsGroup specified in a Pod’s securityContext when that volume is mounted. For large volumes, checking and changing ownership and permissions can take a lot of time, slowing Pod startup. You can use the fsGroupChangePolicy field inside a securityContext to control the way that Kubernetes checks and manages ownership and permissions for a volume.\n\nThis does not work with hostPath-type volumes.\nlocal volume setup on Linux calls SetVolumeOwnership(), which respects securityContext.\nK3s\nRancher’s local-path-provisioner is based on hostPath by default, so it’s recommended to add annotations to the StorageClass which specify defaultVolumeType.\nannotations:\n  defaultVolumeType: &lt;local or hostPath&gt;\nAlternatively, Longhorn can be installed as the CSI.\nReferences\n\nkubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods\nkubernetes.io/docs/concepts/storage/volumes/#local\ngithub.com/kubernetes/kubernetes/blob/741c8db18a52787d734cbe4795f0b4ad860906d6/pkg/volume/local/local.go#L618\ndocs.k3s.io/storage#setting-up-longhorn\n"},"notes/Kubernetes/Kubernetes-DNS":{"title":"Kubernetes DNS","links":[],"tags":[],"content":"DNS does not resolve Service names across namespaces.\nAppend .&lt;namespace&gt;.svc.cluster.local to the service name to reference a service across namespaces."},"notes/Kubernetes/Kubernetes-Monitoring":{"title":"Kubernetes Monitoring","links":["notes/Kubernetes/Grafana-Kubernetes-Monitoring-Helm-Charts","notes/Observability/Grafana-Dashboards"],"tags":[],"content":"Setup\n\nGrafana Kubernetes Monitoring Helm Charts\ngithub.com/kubernetes-monitoring/kubernetes-mixin\ngithub.com/monitoring-mixins/website (simple mixin export tool)\n\nDashboards\nGrafana Dashboards"},"notes/Kubernetes/Kustomize":{"title":"Kustomize","links":[],"tags":[],"content":"Nested config\nOne kustomization.yaml file can only define one YAML object, so you have to organize Kustomization into different directories to implement a set of  Kustomization with different namespaces."},"notes/Kubernetes/Longhorn":{"title":"Longhorn","links":[],"tags":[],"content":"Installation requirements\n\nThe host filesystem supports the file extents feature to store the data. Currently these are support:\n\next4\nXFS\n\n\n\nDisk config\nnode.longhorn.io/default-disks-config: \n&#039;[\n    { \n        &quot;path&quot;:&quot;/mnt/disk1&quot;,\n        &quot;allowScheduling&quot;:true\n    },\n    {   \n        &quot;name&quot;:&quot;fast-ssd-disk&quot;, \n        &quot;path&quot;:&quot;/mnt/disk2&quot;,\n        &quot;allowScheduling&quot;:false,\n        &quot;storageReserved&quot;:10485760,\n        &quot;tags&quot;:[\n            &quot;ssd&quot;,\n            &quot;fast&quot;\n        ]\n    }\n]&#039;\n\nSecurity\nThe backend does not support authentication. See github.com/longhorn/longhorn/issues/1983."},"notes/Kubernetes/Persistent-SQLite-on-K8s":{"title":"Persistent SQLite on K8s","links":[],"tags":[],"content":"Litestream\n\nCurrently, Litestream only allows a single node at a time in a StatefulSet but future versions will allow read replication to additional nodes.\n\nlitestream.io/guides/kubernetes/"},"notes/Kubernetes/Pinning-FROM-with-Image-Digest":{"title":"Pinning FROM with Image Digest","links":[],"tags":[],"content":"Using multi-platform base images\nThe digest shown on Docker Hub is platform-specfic. When you click a tag, it brings you to the first architecture only.\nTo get digest of the entire multi-platform image, you could use nerdctl as shown below or docker buildx imagetools inspect &lt;image&gt;:&lt;tag&gt;.\n$ nerdctl image inspect --mode native bitnami/pgbouncer:1.20.1\n[..snip..]\n&quot;Image&quot;: {\n    &quot;Name&quot;: &quot;docker.io/bitnami/pgbouncer:1.20.1&quot;,\n    &quot;Labels&quot;: null,\n    &quot;Target&quot;: {\n        &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.list.v2+json&quot;,\n        &quot;digest&quot;: &quot;sha256:710da11a466f98b90380fb8e02b487aacf8ef33b07acabb402067c122ac63e8d&quot;,\n        &quot;size&quot;: 529\n    },\n    &quot;CreatedAt&quot;: &quot;2023-10-02T14:54:49.666483Z&quot;,\n    &quot;UpdatedAt&quot;: &quot;2023-10-02T14:54:49.666483Z&quot;\n},\n"},"notes/Kubernetes/Traefik":{"title":"Traefik","links":[],"tags":[],"content":"Kubernetes CRDs\nIn v2.10, the Kubernetes CRDs API Group traefik.containo.us is deprecated, and its support will end starting with Traefik v3. Please use the API Group traefik.io instead.\nDefault certificate\nSpecify default TLS certificate with a TLSStore named “default” in Traefik’s namespace.\napiVersion: traefik.io/v1alpha1\nkind: TLSStore\nmetadata:\n  name: default\n  namespace: {{ template &quot;traefik.namespace&quot; $ }}\n \nspec:\n  defaultCertificate:\n    secretName: mySecret\nFor Traefik versions &lt; 2.10, use the old API group.\napiVersion: traefik.containo.us/v1alpha1\nkind: TLSStore\nmetadata:\n  name: default\n  namespace: {{ template &quot;traefik.namespace&quot; $ }}\n \nspec:\n  defaultCertificate:\n    secretName: mySecret\nTraefik dashboard\nkubectl -n kube-system port-forward deploy/traefik 9000\n# open http://127.0.0.1:9000/\n\nReferences\n\ndoc.traefik.io/traefik/routing/providers/kubernetes-crd/#kind-tlsstore\n"},"notes/Kubernetes/cert-manager":{"title":"cert-manager","links":[],"tags":[],"content":"Helm CRDs\n\ncert-manager does not use the official helm method of installing CRD resources. This is because it makes upgrading CRDs impossible with helm CLI alone. The helm team explain the limitations of their approach here.\n\n\nif you uninstall the release, the CRDs will also be uninstalled. If that happens then you will loose all instances of those CRDs, e.g. all Certificate resources in the cluster. You should consider if this is likely to happen to you and have a mitigation, such as backups or a means to reapply resources from an Infrastructure as Code (IaC) pattern.\n\nClusterIssuer Secrets\n\nThe ClusterIssuer resource is cluster scoped. This means that when referencing a secret via the secretName field, secrets will be looked for in the Cluster Resource Namespace. By default, this namespace is cert-manager however it can be changed via a flag on the cert-manager-controller component.\n\nWrite cluster-level Secret into the cert-manager namespace.\nDNS-01 Challenge\nSuitable for private domains, because HTTP-01 validators are not able to connect to them."},"notes/Life/Dealing-with-Leaking-Alkaline-Batteries":{"title":"Dealing with Leaking Alkaline Batteries","links":[],"tags":[],"content":"If you need to keep the battery for compensation\n\nKeep the battery in the damaged device.\nIf you are in contact with the liquid leaked, rinse immediately with running water. If there are visible burns, contact your doctor.\nEnsure room is well ventilated and seal the device with the battery in a bag and put it in a safe place.\nAvoid handling the leak yourself to prevent chemical burns.\n\nDispose leaking batteries\n\nIf you are in contact with the liquid leaked, rinse immediately with running water. If there are visible burns, contact your doctor.\nPut on gloves, an apron and glasses (if you have them).\nEnsure room is well ventilated.\nRemove the leaked batteries (preferably wearing gloves).\nPut the leaking batteries in a clear plastic bag and seal it.\nPut the bag in a secure place away from children or pets immediately.\nWash your hands and dispose the bag.\n\nClean up the device\n\nPut on gloves, an apron and glasses (if you have them).\nEnsure room is well ventilated.\n(Preferred) Dip a cotton swab, a cloth or an old toothbrush in lemon juice or vinegar.\nWipe the white crystals off gently with it until your device is entirely clean.\nWash your hands!\nAfter the area has completely dried up, scrape the metal contacts to remove any residue and polish them with a pencil eraser.\n"},"notes/MacPorts/Fontconfig":{"title":"Fontconfig","links":[],"tags":[],"content":"Font directory list\n\t&lt;dir&gt;/usr/share/fonts&lt;/dir&gt;\n\t&lt;dir&gt;/usr/X11/lib/X11/fonts&lt;/dir&gt; &lt;dir&gt;/opt/local/share/fonts&lt;/dir&gt; &lt;dir&gt;/Library/Fonts&lt;/dir&gt; &lt;dir&gt;/Network/Library/Fonts&lt;/dir&gt; &lt;dir&gt;/System/Library/Fonts&lt;/dir&gt;\n\t&lt;dir prefix=&quot;xdg&quot;&gt;fonts&lt;/dir&gt;\n\t&lt;!-- the following element will be removed in the future --&gt;\n\t&lt;dir&gt;~/.fonts&lt;/dir&gt;\n\nShould we add ~/Library/Fonts? github.com/macports/macports-ports/pull/13144"},"notes/MacPorts/Ideas":{"title":"Ideas","links":[],"tags":[],"content":"Improvements\n\n Fix port bump in ports with multiple checksums (e.g. cargo.crates).\n port command blocks on DNS resolution timeout, and is impossible to interrupt safely.\n\nIt’s blocked on a curl call, which already has a reasonable timeout.\nWhy doesn’t Ctrl+C interrupt the call?\n\n\n port reclaim takes too much time checking distfiles.\n\nCache distfile information (done).\n\n\n[/] Dependency resolution is too slow.\n\nCould not make use of libsolv because we have variants and countless combinations out of them.\n\n\n"},"notes/MacPorts/OpenSSH-Log-on-macOS":{"title":"OpenSSH Log on macOS","links":[],"tags":[],"content":"Stream logs\nlog stream --process ssh\n"},"notes/MacPorts/Running-a-System-LaunchDaemon-with-MacPorts":{"title":"Running a System LaunchDaemon with MacPorts","links":[],"tags":[],"content":"Process\n\nAdd following file in MacPort source layout and portindex it.\nAdd the directory to MacPorts’ sources.conf.\nsudo port install xxx &amp;&amp; sudo port load xxx to install and activate the service.\n\nPortfile\n# -*- coding: utf-8; mode: tcl; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- vim:fenc=utf-8:ft=tcl:et:sw=4:ts=4:sts=4\n\nPortSystem          1.0\n\nname                xxx\nepoch.              20240101\nversion             1\ncategories          net\nlicense             Restrictive\nplatforms           darwin\nmaintainers         nomaintainer\ndescription         None\nlong_description    None\nhomepage            none\n\ndistfiles\nuse_configure no\nbuild {}\ndestroot {}\n\nstartupitem.create     yes\nstartupitem.netchange  yes\nstartupitem.executable ${xxx_prefix}/bin/xxx -f ${prefix}/etc/xxx.conf\n\nadd_users           _xxx group=_xxx home=${prefix}/var/run/xxx \\\n                    shell=/usr/bin/false realname=xxx\\ Server\n\nlivecheck.type  none\n"},"notes/MacPorts/base/Alternative-Compiler":{"title":"Alternative Compiler","links":[],"tags":[],"content":"Beware of MP_PATH_SCAN.\ndnl This macro ensures MP installation prefix paths are NOT in PATH\ndnl for configure to prevent potential problems when base/ code is updated\ndnl and ports are installed that would match needed items.\nAC_DEFUN([MP_PATH_SCAN],[\n    ...\n])\nUse symboic links as a workaround:\nln -s /opt/local/libexec/llvm-13/libexec/intercept-cc ~/.local/bin/\nintercept-build-mp-13 bash -c &#039;./configure &amp;&amp; make -j6&#039;\n"},"notes/MacPorts/base/Anatomy":{"title":"Anatomy","links":[],"tags":[],"content":"Database\n/* ports table */\n&quot;CREATE TABLE registry.ports (&quot;\n        &quot;id INTEGER PRIMARY KEY&quot; // row ID\n    &quot;, name TEXT COLLATE NOCASE&quot; // name of the port\n    &quot;, portfile TEXT&quot; // path of Portfile copy\n    &quot;, location TEXT&quot; // image (binary archive) location\n    &quot;, epoch INTEGER&quot; // part of version comparison\n    &quot;, version TEXT COLLATE VERSION&quot; // part of version comparison\n    &quot;, revision INTEGER&quot; // part of version comparison\n    &quot;, variants TEXT&quot; // actual variants used to build port\n    &quot;, requested_variants TEXT&quot; // variants explicitly requested by user\n    &quot;, state TEXT&quot; // installed (activated) or imaged (has image)\n    &quot;, date DATETIME&quot; // install date\n    &quot;, installtype TEXT&quot; // unused, always &quot;image&quot;\n    &quot;, archs TEXT&quot; // list of architectures of this port\n    &quot;, requested INTEGER&quot; // is port requested (boolean)\n    &quot;, os_platform TEXT&quot; // operating system (&quot;darwin&quot;)\n    &quot;, os_major INTEGER&quot; // OS version\n    &quot;, cxx_stdlib TEXT&quot; // stdlib used for C++ (&quot;libstdc++&quot; or &quot;libc++&quot;)\n    &quot;, cxx_stdlib_overridden INTEGER&quot; // flag to track broken C++ ports\n    &quot;, UNIQUE (name, epoch, version, revision, variants)&quot;\n    &quot;)&quot;,\n&quot;CREATE INDEX registry.port_name ON ports&quot;\n    &quot;(name, epoch, version, revision, variants)&quot;,\n&quot;CREATE INDEX registry.port_state ON ports(state)&quot;,\n \n/* file map */\n&quot;CREATE TABLE registry.files (&quot;\n        &quot;id INTEGER&quot; // row ID\n    &quot;, path TEXT&quot; // file path\n    &quot;, actual_path TEXT&quot; // actual path installed (FS case-sensitivity)\n    &quot;, active INTEGER&quot; // is file installed\n    &quot;, binary BOOL&quot; // is file a binary (rev-upgrade check)\n    &quot;, FOREIGN KEY(id) REFERENCES ports(id))&quot;,\n&quot;CREATE INDEX registry.file_port ON files(id)&quot;,\n&quot;CREATE INDEX registry.file_path ON files(path)&quot;,\n&quot;CREATE INDEX registry.file_actual ON files(actual_path)&quot;,\n&quot;CREATE INDEX registry.file_actual_nocase ON files(actual_path COLLATE NOCASE)&quot;,\n \n/* dependency map */\n&quot;CREATE TABLE registry.dependencies (&quot;\n        &quot;id INTEGER&quot; // row ID\n    &quot;, name TEXT&quot; // name of the port\n    &quot;, variants TEXT&quot; // variants of the port\n    &quot;, FOREIGN KEY(id) REFERENCES ports(id))&quot;, // the dependency\n&quot;CREATE INDEX registry.dep_id ON dependencies(id)&quot;,\n&quot;CREATE INDEX registry.dep_name ON dependencies(name)&quot;,\nVersioning\nepoch &gt; version &gt; revision\n\nPrefixes\nMacPorts defaults to /opt/local, but allows alternatives prefixes.\nPhases\n\nfetch\n\nFetch the ${distfiles} containing program source code or binaries.\n\nchecksum\n\nCompare ${checksums} specified in a Portfile to the checksums of the fetched ${distfiles}.\n\nextract\n\nExtract the ${distfiles} into working directory for build.\n\npatch\n\nApply optional patch files.\n\nconfigure\n\nExecute ${configure.cmd} in ${worksrcpath}.\n\nbuild\n\nExecute ${build.cmd} in ${worksrcpath}.\n\ndestroot\n\nExecute command to “stages” an installation into an intermediate location ${destroot}.\n\ninstall\n\nArchive (also known as Image) a port’s destrooted files into ${prefix}/var/macports/software.\n\nactivate\n\nExtract the port’s files from the archive in ${prefix}/var/macports/software to their final installed locations, usually in ${prefix}.\nPortGroup\nRolling release (synced with ports tree) of utility functions to help make Portfiles more concice and declarative.\nVariants\nMaximum flexibility, but brings a lot of headache. Prefer non-conflicting subports if possible.\nSubports\nA directive used in Portfile to generate multiple port definitions from a single Portfile.\nC++ stdlib\nThe default C++ runtime on OS X 10.9 and later is libc++ from LLVM which supports C++11.\nFor older OS X versions, MacPorts installs a functional libc++ toolchain by default to replace the ancient version of libstdc++ OS X shipped.\nOn Linux libstdc++ (The GNU C++ Library) is more ubiquitous."},"notes/MacPorts/base/Registry":{"title":"Registry","links":[],"tags":[],"content":"registry2 package is entirely written in C.\nTcl_PkgProvide(interp, &quot;registry2&quot;, &quot;2.0&quot;)\nregistry2.0/registry.tcl provides registry 1.0, don’t mistaken it with the C part."},"notes/MacPorts/base/Tcl-API":{"title":"Tcl API","links":[],"tags":[],"content":"Tcl_GetIndexFromObjStruct\nSimple static dictionary lookup.\n// Array of `struct` type, whose first member is a null-terminated string.\n// End of array is marked by a NULL string pointer.\nstatic entry_cmd_type entry_cmds[] = {\n    /* Global commands */\n    { &quot;create&quot;, entry_create },\n    { &quot;delete&quot;, entry_delete },\n    { &quot;open&quot;, entry_open },\n    { &quot;close&quot;, entry_close },\n    { &quot;search&quot;, entry_search },\n    { &quot;exists&quot;, entry_exists },\n    { &quot;imaged&quot;, entry_imaged },\n    { &quot;installed&quot;, entry_installed },\n    { &quot;owner&quot;, entry_owner },\n    { NULL, NULL }\n};\n\n// Search for a string in array, save matching index to `cmd_index` and return TCL_OK.\nif (Tcl_GetIndexFromObjStruct(interp, objv[1], entry_cmds,\n            sizeof(entry_cmd_type), &quot;cmd&quot;, 0, &amp;cmd_index) == TCL_OK) {\n    entry_cmd_type* cmd = &amp;entry_cmds[cmd_index];\n    return cmd-&gt;function(interp, objc, objv);\n}\n"},"notes/Media/Music-Streaming-Setup":{"title":"Music Streaming Setup","links":["notes/Self-Hosting/Litestream"],"tags":[],"content":"Why\nMobile devices have limited storage. With streaming, you can enjoy higher sound quality and more tracks.\nServer software\nNavidrome\nCommunity has written a K8s Deployment manifest with Ingress.\ngithub.com/navidrome/navidrome/blob/59f0c487e7e6943668c47321ef863291401f4a08/contrib/k8s/manifest.yml\nYou can back up the database with Litestream.\nAudio codec\nGeneration loss\nTranscoding from a lossy format might degrade the audio quality.\nTo avoid transcoding from lossy formats in the streaming process, store music files in either lossless formats or a codec that has the least generation loss in the last mile before digital-to-analog conversion (DAC). The best situation is that no transcoding is needed. For example, exclusive mode apps on Windows skips the mixer and therefore any transcoding.\nOpus\nlibopus has the best quality in popular lossy formats supported by FFmpeg. Encode/HighQualityAudio\nFor audible transparency, use 128 kbps for stereo. Opus Recommended Settings\nStereo 160 to 192 kbps is verified transparent according to wiki.hydrogenaud.io/index.php\nAAC\nApple Music’s default import function uses 256 kbps VBR AAC for stereo music.\nFor audible transparency, use 128 kbps for stereo.\nMinimal generation loss could be one of the design goals of AAC. Reference\nFFmpeg Transcode\nlibfdk_aac\nFor 192 kbps VBR, use -vbr 5 with -c:a libfdk_aac. VBR also disables the low-pass filter and preserves higher frequencies.\ngit log v2.0.2.. in github.com/mstorsjo/fdk-aac shows improvements in security hardening and VBR encoding, so it’s worth a try.\nyou can install FFmpeg with MacPorts.\nport install ffmpeg6 +nonfree\naac_at\nwiki.hydrogenaud.io/index.php\nFFmpeg’s q argument ranges from 0 to 14. In the source code, avctx-&gt;global_quality refers to q and AudioToolbox framework gets 127 - q * 9.\nFor 256 kbps VBR, use -q:a 2. Don’t use -global_quality:a, which is ineffective and gives a constant 128 kbps.\nA 3 minute long music file transcoded with  -vn -c:a aac_at -q:a &lt;q in filename&gt; gives the following file sizes.\n$ du -k q*.m4a # unit is kiB\n8196\tq0.m4a\n6404\tq2.m4a\n5380\tq4.m4a\n4100\tq5.m4a\n3844\tq6.m4a\n3588\tq7.m4a\n\ngithub.com/FFmpeg/FFmpeg/blob/393d1ee541b143633bfba2ff0e821d734fd511c2/libavcodec/audiotoolboxenc.c#L325-L335\nAAC generation loss\nwww.reddit.com/r/AppleMusic/comments/o67idh/losslesshires_lossless_on_airpods_pro/h2sshj5/\n\nThe thing here is that, yeah there’s an increase, because previously you had a lossy file (AAC) which lose again info thanks to the BT AAC codec, there’s two compressions to the file, now you only have one compression the BT AAC one, so there’s less info loss it during the process.\n\n\nIt’s decoded, mixed with system sounds (like alerts) and then recompressed. This is obvious because the output over bluetooth rolls off high-frequencies a bit sooner than the original AAC file. (I think the test I saw was on The Sound Guy’s website, but I can’t find it now)\nThe good news is that the generational loss from AAC to Uncompressed to AAC again is really minor. You can do it over and over and its still very difficult for people to tell the difference between the original AAC file and its descendant.\n"},"notes/NGINX/Byte-Range-Request":{"title":"Byte-Range Request","links":[],"tags":[],"content":"NGINX tries to slice the response body when one of the following conditions is met and the request specifies a Range header.\n\nNginx is serving static files directly.\nNginx cache is enabled.\nproxy_force_ranges is set to on.\n\nThis behavior is controlled by allow_ranges in struct ngx_http_request_s."},"notes/NGINX/NGINX-Code-Walkthrough---Client-Closed-Request":{"title":"NGINX Code Walkthrough - Client Closed Request","links":[],"tags":[],"content":"Description\n/*\n * HTTP does not define the code for the case when a client closed\n * the connection while we are processing its request so we introduce\n * own code to log such situation when a client has closed the connection\n * before we even try to send the HTTP header to it\n */\n#define NGX_HTTP_CLIENT_CLOSED_REQUEST     499\nWhile connecting to upstream\nIn ngx_http_upstream_connect(),\n    u-&gt;write_event_handler = ngx_http_upstream_send_request_handler;\n    u-&gt;read_event_handler = ngx_http_upstream_process_header;\nWhen ngx_http_upstream_next() is called,\n    if (r-&gt;connection-&gt;error) {\n        ngx_http_upstream_finalize_request(r, u,\n                                           NGX_HTTP_CLIENT_CLOSED_REQUEST);\n        return;\n    }\nIn ngx_http_upstream_finalize_request(),\n    if (!u-&gt;header_sent\n        || rc == NGX_HTTP_REQUEST_TIME_OUT\n        || rc == NGX_HTTP_CLIENT_CLOSED_REQUEST)\n    {\n        ngx_http_finalize_request(r, rc);\n        return;\n    }\nIn ngx_http_finalize_request(),\n    if (rc == NGX_ERROR\n        || rc == NGX_HTTP_REQUEST_TIME_OUT\n        || rc == NGX_HTTP_CLIENT_CLOSED_REQUEST\n        || c-&gt;error)\n    {\n        if (ngx_http_post_action(r) == NGX_OK) {\n            return;\n        }\n \n        ngx_http_terminate_request(r, rc);\n        return;\n    }\nngx_http_terminate_request() then calls ngx_http_close_request(), which finally calls ngx_http_free_request() and ngx_http_close_connection(). The first logs the request, and the second calls ngx_close_connection(), which leads us to ngx_close_socket() that calls close().\nSome other cases\nngx_http_upstream_init_request() first sets both event handlers to check for broken connections.\n    if (... &amp;&amp; !u-&gt;conf-&gt;ignore_client_abort) {\n        ...\n \n        r-&gt;read_event_handler = ngx_http_upstream_rd_check_broken_connection;\n        r-&gt;write_event_handler = ngx_http_upstream_wr_check_broken_connection;\n    }\nIf request body is not buffered, ngx_http_upstream_send_request_body() sets the read_event_handler to ngx_http_upstream_read_request_handler, but restores it to ngx_http_upstream_rd_check_broken_connection() once the client request body is fully read.\nBoth check functions call ngx_http_upstream_check_broken_connection(), which would finalize the client-side connection in the same way.\n        if (!u-&gt;cacheable) {\n            ngx_http_upstream_finalize_request(r, u,\n                                               NGX_HTTP_CLIENT_CLOSED_REQUEST);\n        }"},"notes/NGINX/NGINX-Code-Walkthrough---Upstream-Connect-Timeout":{"title":"NGINX Code Walkthrough - Upstream Connect Timeout","links":[],"tags":[],"content":"Searching for a “stack trace”\nSituation 1: upstream server did not reply to our SYN within proxy_connect_timeout\n... upstream timed out (110: Connection timed out) while connection to upstream, ...\n\nFirst, proxy_connect_timeout is used in ngx_http_upstream_connect() to set a timer on c-&gt;write.\n    if (rc == NGX_AGAIN) {\n        ngx_add_timer(c-&gt;write, u-&gt;conf-&gt;connect_timeout);\n        return;\n    }\nSearch for the string &quot;upstream timed out&quot; returns 4 matches in src/http/ngx_http_upstream.c, and we’d like to find out which function handles such timeouts.\nAmong the 4 functions, ngx_http_upstream_process_upstream() and ngx_http_upstream_process_non_buffered_upstream() are called in ngx_http_upstream_send_response(), which is only called after reading the response header.\nngx_http_upstream_process_upgraded() is called for WebSocket upgrade, so we ignore that as well.\nThen we focus on the remaining ngx_http_upstream_next().\n    if (ft_type == NGX_HTTP_UPSTREAM_FT_TIMEOUT) {\n        ngx_log_error(NGX_LOG_ERR, r-&gt;connection-&gt;log, NGX_ETIMEDOUT,\n                      &quot;upstream timed out&quot;);\n    }\nA new keyword NGX_HTTP_UPSTREAM_FT_TIMEOUT, this brings our focus to ngx_http_upstream_send_request_handler().\n    if (c-&gt;write-&gt;timedout) {\n        ngx_http_upstream_next(r, u, NGX_HTTP_UPSTREAM_FT_TIMEOUT);\n        return;\n    }\nOn the other hand, let’s search for &quot;connecting to upstream&quot;, which is set in c-&gt;log-&gt;action and can be seen from the error log. There are 3 matches, but two of them has a different error text: &quot;connect() failed&quot;.\nThe last match is ngx_http_upstream_connect(). This function calls ngx_event_connect_peer() to establish a connection, and sets u-&gt;write_event_handler to ngx_http_upstream_send_request_handler.\nTherefore, the function call chain is\nngx_http_upstream_connect() → ngx_event_connect_peer() ^&gt; ngx_add_timer(c-&gt;write, u-&gt;conf-&gt;connect_timeout) ^&gt;\nevent loop in ngx_worker_process_cycle() → ngx_process_events_and_timers() → ngx_event_expire_timers() → ev-&gt;handler =\nc-&gt;write-&gt;handler → ngx_http_upstream_handler() → u-&gt;write_event_handler → ngx_http_upstream_send_request_handler() → ngx_http_upstream_next() →\n\nu-&gt;peer.free → ngx_http_upstream_free_round_robin_peer() ^&gt;\nngx_log_error() ^&gt;\n(if no more tries) ngx_http_upstream_finalize_request() →\nngx_close_connection() → ngx_close_socket() i.e. close() ^&gt;\nngx_http_upstream_connect() (retry next server)\n\nNote: ^&gt; represents subsequent execution after returning to the caller function, and → is a function call or pointer dereference.\nIf connection succeeded, the timer will be reset to send_timeout or cleared.\n        if (!c-&gt;write-&gt;ready || u-&gt;request_body_blocked) {\n            ngx_add_timer(c-&gt;write, u-&gt;conf-&gt;send_timeout);\n \n        } else if (c-&gt;write-&gt;timer_set) {\n            ngx_del_timer(c-&gt;write);\n        }\nSequence of events\n\nngx_event_connect_peer(): Connection is attempted.\nngx_http_upstream_handler(): Event loop detects the timeout and calls this handler.\nngx_http_upstream_free_round_robin_peer(): fails is accounted for and compared with max_fails to temporarily disable upstream servers.\nngx_log_error(): “upstream timed out” is logged.\nngx_close_connection(): old connection to upstream is closed.\n\nSituation 2: upstream server did not reply to our SYN within OS timeout\nWhen ngx_event_connect_peer() is called, both c-&gt;read and c-&gt;write are registered to the event loop.\n    rc = connect(s, pc-&gt;sockaddr, pc-&gt;socklen);\n \n    if (ngx_add_event(rev, NGX_READ_EVENT, event) != NGX_OK) {\n        goto failed;\n    }\n \n    if (rc == -1) {\n \n        /* NGX_EINPROGRESS */\n \n        if (ngx_add_event(wev, NGX_WRITE_EVENT, event) != NGX_OK) {\n            goto failed;\n        }\n \n        return NGX_AGAIN;\n    }\nLet’s see what connect(2) in System Calls Manual says about the comment NGX_EINPROGRESS.\n\nEINPROGRESS\nThe socket is nonblocking and the connection cannot be completed immediately. (UNIX domain sockets failed with EAGAIN instead.) It is possible to select(2) or poll(2) for completion by selecting the socket for writing. After select(2) indicates writability, use getsockopt(2) to read the SO_ERROR option at level SOL_SOCKET to determine whether connect() completed successfully (SO_ERROR is zero) or unsuccessfully (SO_ERROR is one of the usual error codes listed here, explaining the reason for the failure).\n\nIn this case, epoll receives the error as a write event and calls its handler. ngx_http_upstream_send_request_handler() then calls ngx_http_upstream_send_request(), which calls ngx_http_upstream_test_connect() and detects the failure.\n            if (flags &amp; NGX_POST_EVENTS) {\n                queue = rev-&gt;accept ? &amp;ngx_posted_accept_events\n                                    : &amp;ngx_posted_events;\n \n                ngx_post_event(rev, queue);\n \n            } else {\n                rev-&gt;handler(rev);\n            }\nIn ngx_http_upstream_test_connect(), error is logged as “connect() failed” on Linux.\n        if (err) {\n            c-&gt;log-&gt;action = &quot;connecting to upstream&quot;;\n            (void) ngx_connection_error(c, err, &quot;connect() failed&quot;);\n            return NGX_ERROR;\n        }\nTherefore, the function call chain is\nngx_http_upstream_connect() → ngx_event_connect_peer() ^&gt; ngx_add_timer(c-&gt;write, u-&gt;conf-&gt;connect_timeout) ^&gt;\nevent loop in ngx_worker_process_cycle() → ngx_process_events_and_timers() → ngx_process_events() = ngx_epoll_process_events() → rev-&gt;handler =\nc-&gt;write-&gt;handler → ngx_http_upstream_handler() → u-&gt;write_event_handler  → ngx_http_upstream_send_request_handler() →\nngx_http_upstream_send_request() →\n\nngx_http_upstream_test_connect() → ngx_connection_error() ^&gt;\nngx_http_upstream_next() → …\n\nSequence of events\n\nngx_event_connect_peer(): Connection is attempted.\nngx_http_upstream_handler(): Event loop detects the error and calls this handler.\nngx_http_upstream_test_connect(): check for failure and log “connect() failed.”\nngx_http_upstream_free_round_robin_peer(): fails is accounted for and compared with max_fails to temporarily disable upstream servers.\nngx_close_connection(): old connection to upstream is released.\n\nNotes\nA possibly better way to do this: set log level to debug, and simulate the connect timeout with a very low proxy_connect_timeout.\nReferences\n\ngithub.com/nginx/nginx/blob/788e462c5b81f5f1aee475488e10f01680c530e9/src/http/ngx_http_upstream.c\n"},"notes/NGINX/NGINX-Error-Log":{"title":"NGINX Error Log","links":[],"tags":[],"content":"Format\nIn ngx_log_error_core, the cached time, log level surrounded by [], pid#tid: and *$connection (connection serial number, if exists) are “printed” first. Then, the error message from the arguments is formatted and printed. The resulting string is then passed to log-&gt;handler to get the final message to log.\nIn ngx_http_request.c, c-&gt;log-&gt;handler is set to ngx_http_log_error, which appends  while ... if there is a log-&gt;action, , client: ... (the connection’s client address), and passes the error message to r-&gt;log_handler.\nr-&gt;log_handler is ngx_http_log_error_handler, also defined in ngx_http_request.c. It appends , server: $server_name, the request line, sub-request if exists, upstream information if exists, Host request header if exists, and Referrer request header if exists.\nTherefore, the final format for an HTTP request is\n1970/09/28 12:00:00 [$err_level] $pid#$tid: *$connection_serial_number $error___message while $action, client: $connection_addr, server: $server_name, request: &quot;$request&quot;, subrequest: &quot;$sr-&gt;uri&quot;, upstream: &quot;schema://peer_name$upstream_uri&quot;, host: &quot;$http_host&quot;, referrer: &quot;$http_referer&quot;\n"},"notes/NGINX/NGINX-HTTP-Variables-to-Log":{"title":"NGINX HTTP Variables to Log","links":[],"tags":[],"content":"Improved log format\nIf the ngx_http_realip_module module is not configured correctly, use $http_cf_connecting_ip\\t$remote_addr at the beginning instead.\nlog_format main_cf &#039;\\t$realip_remote_addr\\t$server_addr\\t$hostname\\t&#039;\n                   &#039;$time_iso8601\\t$host\\t$request_uri\\t$request_length\\t&#039;\n                   &#039;$request_time\\t$status\\t$body_bytes_sent\\t$http_user_agent\\t&#039;\n                   &#039;$upstream_addr\\t$upstream_status\\t$upstream_connect_time\\t$upstream_response_time\\t&#039;\n                   &#039;$request_method\\t$server_protocol\\t$http_cf_ipcountry\\t$http_cf_ray\\t&#039;;\nP.S.\n\n$server_addr has a tiny performance penalty if the listening socket is not bind to a specific IP address.\nIf the request came from an IP out of the set_real_ip_from CIDR ranges, $realip_remote_addr would fallback to r-&gt;connection-&gt;addr_text, which is the same as $remote_addr (defined in function ngx_http_variable_remote_addr).\nThe trailing \\t can be removed. It was kept for extensibility.\n\nNote: if a server cannot be selected, $upstream_status keeps the pre-filled 502.\nPosition of common variables in main_cf format:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariablePosition$host6$status10$request_uri7$request_time9$http_user_agent12\nGoAccess config:\n# for older versions without `--datetime-format`\ngoaccess --date-format=&#039;%Y-%m-%d&#039; --time-format=&#039;%T&#039; --log-format=&#039;%h\\t%^\\t%^\\t%^\\t%dT%t+%^\\t%v\\t%U\\t%^\\t%T\\t%s\\t%b\\t%u\\t%^\\t%^\\t%^\\t%^\\t%m\\t%H\\t%^\\t%^&#039;\n \n# for latest version\ngoaccess --datetime-format=&#039;%FT%T%z&#039; --log-format=&#039;%h\\t%^\\t%^\\t%^\\t%x\\t%v\\t%U\\t%^\\t%T\\t%s\\t%b\\t%u\\t%^\\t%^\\t%^\\t%^\\t%m\\t%H\\t%^\\t%^&#039;\nBasic information\n$bytes_sent\nnumber of bytes sent to a client (1.3.8, 1.2.5). Measured at HTTP-level, not including TCP or TLS overhead.\n$body_bytes_sent\nnumber of bytes sent to a client, not counting the response header; this variable is compatible with the “%B” parameter of the mod_log_config Apache module\n$host\nin this order of precedence: host name from the request line, or host name from the “Host” request header field, or the server name matching a request. Read from headers_in.server instead of headers_in.host. The former is normalized with ngx_http_validate_host().\n$hostname\nhost name\n$remote_addr\nclient address\n$server_addr\nan address of the server which accepted a request\nComputing a value of this variable usually requires one system call. To avoid a system call, the listen directives must specify addresses and use the bind parameter.\n$request_length\nrequest length (including request line, header, and request body) (1.3.12, 1.2.7)\n$request_method\nrequest method, usually “GET” or “POST”\n$request_time\nrequest processing time in seconds with a milliseconds resolution (1.3.9, 1.2.6); time elapsed since the first bytes were read from the client. Measured at HTTP-level, not including TCP or TLS handshake time.\n$request_uri\nfull original request URI (with arguments)\n$scheme\nrequest scheme, “http” or “https”\n$server_protocol\nrequest protocol, usually “HTTP/1.0”, “HTTP/1.1”, “HTTP/2.0”, or “HTTP/3.0”\n$status\nresponse status (1.3.2, 1.2.2)\n$time_iso8601, $time_local\nlocal time in the ISO 8601 standard format (1.3.12, 1.2.7) and the Common Log Format (1.3.12, 1.2.7). Records the time (cached) when the log entry is written.\n$http_name\narbitrary request header field; the last part of a variable name is the field name converted to lower case with dashes replaced by underscores\nHTTP/2\n$connection\nconnection serial number (1.3.8, 1.2.5)\n$connection_requests\ncurrent number of requests made through a connection (1.3.8, 1.2.5)\n$connection_time\nconnection time in seconds with a milliseconds resolution (1.19.10)\nTCP information\n$tcpinfo_rtt, $tcpinfo_rttvar, $tcpinfo_snd_cwnd, $tcpinfo_rcv_space\ninformation about the client TCP connection; available on systems that support the TCP_INFO socket option"},"notes/NGINX/NGINX-IPv6-Rate-Limiting":{"title":"NGINX IPv6 Rate Limiting","links":[],"tags":[],"content":"Use with realip module.\nmap $binary_remote_addr $subnet {\n    &quot;~^(\\C{8})&quot;         $1;\n    default             $binary_remote_addr;\n}\n\nlimit_req_zone $subnet zone=one:10m rate=1r/s;\n\nReferences\n\nmailman.nginx.org/pipermail/nginx/2021-April/060557.html\n"},"notes/NGINX/NGINX-Proxy-Cache":{"title":"NGINX Proxy Cache","links":[],"tags":[],"content":"Checklist\n\nDo not cache sensitive content without authentication token included in proxy_cache_key. It is recommended to include a random secret in the string to further prevent hash collision attacks.\nAvoid proxy_cache_valid any and use specific status codes.\nIf you want to limit the cache to certain arguments or other patterns, use proxy_no_cache and proxy_cache_bypass with a map-ed variable, where the string can be templated to match a set of conditions efficiently.\nWith proxy_cache_lock enabled, a request returning cached content may have been delayed by a lock, resulting in longer $request_time in logs.\nUse the following config to instruct NGINX to deliver stale content when clients request an item that is expired or is in the process of being updated from the origin server. All updates will be done in the background. The stale file is returned for all requests until the updated file is fully downloaded.\n\nproxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;\nproxy_cache_background_update on;\nproxy_cache_lock on;\n\n\nThe updating parameter to the proxy_cache_use_stale directive, combined with enabling the proxy_cache_background_update directive, instructs NGINX to deliver stale content when clients request an item that is expired or is in the process of being updated from the origin server. All updates will be done in the background. The stale file is returned for all requests until the updated file is fully downloaded.\n\nIf UPDATING starts to appear in $upstream_cache_status, background update is enabled.\nInstrumentation\nadd_header X-Cache-Status $upstream_cache_status;\n\nThis example adds an X-Cache-Status HTTP header in responses to clients. The following are the possible values for $upstream_cache_status:\n\nMISS – The response was not found in the cache and so was fetched from an origin server. The response might then have been cached.\nBYPASS – The response was fetched from the origin server instead of served from the cache because the request matched a proxy_cache_bypass directive (see Can I Punch a Hole Through My Cache? below.) The response might then have been cached.\nEXPIRED – The entry in the cache has expired. The response contains fresh content from the origin server.\nSTALE – The content is stale because the origin server is not responding correctly, and proxy_cache_use_stale was configured.\nUPDATING – The content is stale because the entry is currently being updated in response to a previous request, and proxy_cache_use_stale updating is configured.\nREVALIDATED – The proxy_cache_revalidate directive was enabled and NGINX verified that the current cached content was still valid (If-Modified-Since or If-None-Match).\nHIT – The response contains valid, fresh content direct from the cache.\n\nNotes\n\nCache data is stored in files. The filename in a cache is a result of applying the MD5 function to the cache key.\nIn addition, all active keys and information about data are stored in a shared memory zone, whose name and size are configured by the keys_zone parameter. One megabyte zone can store about 8 thousand keys.\n"},"notes/NGINX/NGINX-Thundering-Herd-Problem":{"title":"NGINX Thundering Herd Problem","links":["notes/Operating-System/Linux/epoll","notes/Operating-System/Linux/SO_REUSEPORT"],"tags":[],"content":"Solution 1: EPOLLEXCLUSIVE flag\nIf EPOLLEXCLUSIVE (Linux 4.5, glibc 2.24) is defined when compiling Nginx, Nginx could make use of it to reduce resource usage when volume of new connection is low.\nNginx discards EPOLLRDHUP if EPOLLEXCLUSIVE is enabled, keeping EPOLLIN and EPOLLOUT compatible with the CentOS 7 backport.\n#if (NGX_HAVE_EPOLLEXCLUSIVE &amp;&amp; NGX_HAVE_EPOLLRDHUP)\n    if (flags &amp; NGX_EXCLUSIVE_EVENT) {\n        events &amp;= ~EPOLLRDHUP;\n    }\n#endif\nFor every 16 requests handled, Nginx would re-add the socket in ngx_reorder_accept_events() to balance request across workers.\n    if (c-&gt;requests++ % 16 != 0\n        &amp;&amp; ngx_accept_disabled &lt;= 0)\n    {\n        return;\n    }\n \n    if (ngx_del_event(c-&gt;read, NGX_READ_EVENT, NGX_DISABLE_EVENT)\n        == NGX_ERROR)\n    {\n        return;\n    }\n \n    if (ngx_add_event(c-&gt;read, NGX_READ_EVENT, NGX_EXCLUSIVE_EVENT)\n        == NGX_ERROR)\n    {\n        return;\n    }\nSolution 2: listen reuseport\nSO_REUSEPORT could significantly increase the max latency in a degraded state. See blog.cloudflare.com/the-sad-state-of-linux-socket-balancing/.\nAs a side effect, total length of pending connection queue (backlog) is multiplied by the number of workers in this case."},"notes/NGINX/NGINX-location-Directive":{"title":"NGINX location Directive","links":[],"tags":[],"content":"proxy_pass redirecting\nIf a prefix-matching location ends with a slash (/), and requests are processed by proxy_pass, requests to the exact string minus the trailing slash are redirected to full URI (domain included) with slash appended.\nCase sensitivity\nOn case-sensitive operating systems, only ~* matches case-insensitively."},"notes/NGINX/NGINX-realip-configuration-for-Cloudflare":{"title":"NGINX realip configuration for Cloudflare","links":[],"tags":[],"content":"If you also want to reject requests that aren’t from Cloudflare:\nhttp {\n    # Load realip module (if not already loaded)\n    load_module modules/ngx_http_realip_module.so;\n\n    # Include Cloudflare IPs for realip module\n    include /etc/nginx/cloudflare-ips.conf;\n\n    # Geo block to check if original IP is from Cloudflare\n    geo $realip_remote_addr $is_cloudflare {\n        default 0;\n        include /etc/nginx/cloudflare-ips-geo.conf;\n    }\n\n    # Use CF-Connecting-IP header for real client IP\n    real_ip_header CF-Connecting-IP;\n\n    server {\n        listen 80;\n        server_name example.com;\n\n        # Deny requests not from Cloudflare IPs\n        if ($is_cloudflare != 1) {\n            return 403;\n        }\n\n        # Rest of your configuration...\n    }\n}\n\nwhich references the following files with simple formats.\n# cloudflare-ips-geo.conf\n# Cloudflare IP ranges for geo module\n2400:cb00::/32 1;\n2606:4700::/32 1;\n2803:f800::/32 1;\n2405:b500::/32 1;\n2405:8100::/32 1;\n2a06:98c0::/29 1;\n\n# cloudflare-ips.conf\n# Cloudflare IP ranges for realip module\nset_real_ip_from 2400:cb00::/32;\nset_real_ip_from 2606:4700::/32;\nset_real_ip_from 2803:f800::/32;\nset_real_ip_from 2405:b500::/32;\nset_real_ip_from 2405:8100::/32;\nset_real_ip_from 2a06:98c0::/29;\n\nUpdating\nHere’s a reference script for fetching updates. You could run it periodically via cron.\n#!/bin/bash\n \n# Fetch Cloudflare IP ranges\ncurl -s www.cloudflare.com/ips-v4 -o /tmp/cf-ips-v4\ncurl -s www.cloudflare.com/ips-v6 -o /tmp/cf-ips-v6\n \n# Generate set_real_ip_from directives\necho &quot;# Cloudflare IP ranges for realip module&quot; &gt; /etc/nginx/cloudflare-ips.conf\ncat /tmp/cf-ips-v4 | while read line; do\n    echo &quot;set_real_ip_from $line;&quot; &gt;&gt; /etc/nginx/cloudflare-ips.conf\ndone\ncat /tmp/cf-ips-v6 | while read line; do\n    echo &quot;set_real_ip_from $line;&quot; &gt;&gt; /etc/nginx/cloudflare-ips.conf\ndone\n \n# Generate geo block entries\necho &quot;# Cloudflare IP ranges for geo module&quot; &gt; /etc/nginx/cloudflare-ips-geo.conf\ncat /tmp/cf-ips-v4 | while read line; do\n    echo &quot;$line 1;&quot; &gt;&gt; /etc/nginx/cloudflare-ips-geo.conf\ndone\ncat /tmp/cf-ips-v6 | while read line; do\n    echo &quot;$line 1;&quot; &gt;&gt; /etc/nginx/cloudflare-ips-geo.conf\ndone\n \n# Test and reload Nginx\nnginx -t &amp;&amp; systemctl reload nginx"},"notes/NGINX/Optimizing-single-box-NGINX-performance":{"title":"Optimizing single-box NGINX performance","links":["notes/Network/NIC-Offload","notes/NGINX/proxy/Upstream-Keepalive"],"tags":[],"content":"NUMA topology\nAMD recommends pinning instances within a NUMA node, but does not recommend doing so via the application-level worker_cpu_affinity option.\nNGINX’s worker_cpu_affinity option may lower performance by increasing the time a process spends waiting for a free CPU. This can be monitored by running runqlat on one of NGINX workers’s PIDs. On the other hand, worker_cpu_affinity eliminates CPU migrations, reduce cache misses and page faults, and slightly increases instructions per cycle. All of which can be verified with perf stat.\nNode Per Socket (NPS) settings\nFor example, a 96-core processor with 96 vCPUs per NGINX instance (2 instances in total) should set NPS2 in BIOS, with each NGINX instance pinned to a NUMA node.\nFor NIC tuning, AMD recommends a combination of NPS=1 with LLC as NUMA enabled. Here, LLC means Last Level Cache, or L3 cache, so the OS will see one NUMA node per L3 cache. This can help the OS schedulers maintain locality to the LLC without causing unnecessary cache-to-cache transactions.\nIf deployment restrictions prevent pinning of VM or NGINX instances, NPS1 will deliver the most consistent performance. This is the best trade-off for this situation, according to AMD.\nNIC configuration\nConfigure NIC queues\nBroadcom recommends the use of combined queues no more than a single IRQ per physical core.\nethtool -L [interface] combined 8 tx 0 rx 0\nEnsure IRQ distribution, i.e. CPU affinity for the NIC queue interrupts, is properly set up. AWS does not recommend disabling the irqbalance service because its ENA driver doesn’t provide affinity hints, and if device reset happens while irqbalance is disabled, this might cause undesirable IRQ distribution. On bare-metal, this could be another case.\nRX and TX ring sizes\nAMD recommends setting the maximum allowable ring size to boost network performance, but not on older kernels or derivers without byte queue limit support (non-BQL drivers).\nBroadcom does not suggest this for all cases as it could result in higher latency and other side effects.\nethtool -G [interface] tx 2047 rx 2047\nInterrupt coalescing\nBroadcom recommends enabling adaptive-rx to improve RX latency or throughput adaptively.\nethtool -C [interface] adaptive-rx on\nGRO (Generic Receive Offload)\nThis should be disabled on routers and bridges, including virtual hosts using bridging. See also NIC Offload.\nBroadcom NICs support Hardware GRO, which can be enabled with the following command.\nethtool -K [interface] rx-gro-hw on lro off gro on\nSystem configuration\nLinux kernel version\nAMD recommends using Linux kernel 5.20 or newer that includes IOMMU optimized patches.\nCPU scaling governor\nSet the CPU scaling governor to Performance mode.\necho performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\nOn Ubuntu, you can use cpupower as well.\nsudo cpupower frequency-set -g performance\nNote: C0 is active state and Cx is sleep state in cpupower monitor -i 60 -m Mperf.\nConfigure limits\nAdd the following configuration to a new file in /etc/security/limits.d/*.conf, if your current limit is lower.\n#nproc – number of processes\n#nofile – number of file descriptors\n* soft nproc 32768\n* hard nproc 65535\n* soft nofile 32768\n* hard nofile 65535\nroot soft nproc 32768\nroot hard nproc 65535\nroot soft nofile 32768\nroot hard nofile 65535\n\nThen, add LimitNOFILE=65535 option to nginx.service or set worker_rlimit_nofile in Nginx configuration to increase the maximum number of open files for worker processes.\nFirewall\nAMD recommends disabling the firewall if possible to improve performance.\nIf you have firewall and connection tracking enabled, make sure nf_conntrack_max is set to an appropriate value.\nTransparent hugepages (THP)\nMake it opt-in with madvise. Only enable THP if you are sure they are beneficial.\necho madvise | sudo tee /sys/kernel/mm/transparent_hugepage/enabled\nsysctl\nThis is a sample sysctl.conf based on various sources including AMD’s recommendations.\n# /etc/sysctl.conf\n########## Kernel ##############\n# Controls the System Request debugging functionality of the kernel\nkernel.sysrq = 0\n \n# Controls whether core dumps will append the PID to the core\n# filename. Useful for debugging multi-threaded applications\nkernel.core_uses_pid = 1\n \n# increase system file descriptor limit\nfs.file-max = 65535\n \n# Allow for more PIDs\nkernel.pid_max = 65536\n \n########## Swap ##############\nvm.swappiness = 10 # Favor RAM over swap\n \n# Disk Caching. Data isn&#039;t critical and can be lost? Favor raising the cache.\n# NOT recommended on modern systems with very alrge amounts of RAM. Comment it out!\nvm.vfs_cache_pressure = 50\nvm.dirty_background_ratio = 50\nvm.dirty_ratio = 80\n \n########## IPv4 networking ##############\n# Controls IP packet forwarding\nnet.ipv4.ip_forward = 0\n \n# Do not accept source routing\nnet.ipv4.conf.default.accept_source_route = 0\n \n# Send redirects, if router, but this is just server\nnet.ipv4.conf.all.send_redirects = 0\nnet.ipv4.conf.default.send_redirects = 0\n \n# Accept packets with SRR option? No\nnet.ipv4.conf.all.accept_source_route = 0\n \n# Accept Redirects? No, this is not router\nnet.ipv4.conf.all.accept_redirects = 0\nnet.ipv4.conf.all.secure_redirects = 0\n \n# Log packets with impossible addresses to kernel log? yes\nnet.ipv4.conf.all.log_martians = 1\nnet.ipv4.conf.default.log_martians = 1\nnet.ipv4.conf.default.accept_source_route = 0\nnet.ipv4.conf.default.accept_redirects = 0\nnet.ipv4.conf.default.secure_redirects = 0\n \n# Ignore all ICMP ECHO and TIMESTAMP requests sent to it via broadcast/multicast\nnet.ipv4.icmp_echo_ignore_broadcasts = 1\n# Turn on protection for bad icmp error messages\nnet.ipv4.icmp_ignore_bogus_error_responses = 1\n \n# Prevent against the common &#039;syn flood attack&#039;\nnet.ipv4.tcp_syncookies = 1\n \n# Controls the use of TCP syncookies\nnet.ipv4.tcp_synack_retries = 2\n \n# Enable source validation by reversed path, as specified in RFC1812\nnet.ipv4.conf.all.rp_filter = 1\nnet.ipv4.conf.default.rp_filter = 1\n \n# TCP and memory optimization\n# increase TCP max buffer size to 8MiB\nnet.ipv4.tcp_rmem = 4096 131072 8388608\nnet.ipv4.tcp_wmem = 4096 16384 8388608\n \n# increase Linux auto tuning TCP buffer limits\nnet.core.rmem_max = 8388608\nnet.core.wmem_max = 8388608\nnet.core.netdev_max_backlog = 5000\nnet.ipv4.tcp_window_scaling = 1\n \n#Increase system IP port limits\nnet.ipv4.ip_local_port_range = 2000 65499\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_fin_timeout = 60\n \nnet.ipv4.tcp_slow_start_after_idle = 0\n \n# Recommended for hosts with jumbo frames enabled\nnet.ipv4.tcp_mtu_probing = 1\n \n# TCP Fast Open\nnet.ipv4.tcp_fastopen = 3\n \n#net.ipv4.tcp_congestion_control = cubic\n \n########## IPv4 networking ends ##############\n \n########## IPv6 networking start ##############\n \n# Number of Router Solicitations to send until assuming no routers are present.\n# This is host and not router\nnet.ipv6.conf.default.router_solicitations = 0\n \n# Accept Router Preference in RA?\nnet.ipv6.conf.default.accept_ra_rtr_pref = 0\n \n# Learn Prefix Information in Router Advertisement\nnet.ipv6.conf.default.accept_ra_pinfo = 0\n \n# Setting controls whether the system will accept Hop Limit settings\n# from a router advertisement\nnet.ipv6.conf.default.accept_ra_defrtr = 0\n \n# Router Advertisements can cause the system to assign a global unicast\n# address to an interface\nnet.ipv6.conf.default.autoconf = 0\n \n# how many neighbor solicitations to send out per address?\nnet.ipv6.conf.default.dad_transmits = 0\n \n# How many global unicast IPv6 addresses can be assigned to each interface?\nnet.ipv6.conf.default.max_addresses = 1\n \n########## IPv6 networking ends ##############\nNginx configuration\nEvent handling\n\nConsider enabling multi_accept if your workload shows benefits.\n\nOpen file cache\nUseful for serving lots of static or cached data. See official documentation for the open_file_cache* directives. Check potential benefits with:\n# funclatency /srv/nginx-bazel/sbin/nginx:ngx_open_cached_file -u\n     usecs               : count     distribution\n         0 -&amp;gt; 1          : 10219    |****************************************|\n         2 -&amp;gt; 3          : 21       |                                        |\n         4 -&amp;gt; 7          : 3        |                                        |\n         8 -&amp;gt; 15         : 1        |                                        |\n\nIf there are too many open calls or there are some that take too much time, you can look at enabling the open file cache.\nHTTP keepalive\nkeepalive_requests can be increased, at the risk of introducing additional DDoS attack vectors.\nSee also Upstream Keepalive.\nBuffering requests and responses with large body\nFor buffering the request body, enabling sendfile improves performance for requests with large content size (&gt;1MB) and results in a small performance loss for requests with small content sizes. AMD recommends setting sendfile_max_chunk to the typical average request size.\nEnabling tcp_nopush can be beneficial to serving large content to users by maxing out packet size until the file is fully sent.\nsendfile on;\ntcp_nopush on;\n\nWorker configuration\n\nworker_processes should be set to the number of vCPUs.\nworker_connections should be increased as needed, and ensure you set worker_rlimit_nofile or modify the limits in systemd unit definition accordingly.\n\nLogging\nUse ext4slower to identify disk I/O latency issues. Enable buffering and gzip for the access_log directive to help reduce blocking on I/O.\nIf multiple NGINX processes attempt to write to the same log file, lock contention could be a dominating factor in your CPU profile.\nCaching and compression\nProperly configured cache can increase performance, especially if serving stale content is enabled. Make sure there is sufficient RAM to store the hot cached content in OS page cache.\nNGINX supports Gzip compression, which accelerates the transfer rate from the server to the client and reduces bandwidth usage.\nEnable AIO (Asynchronous file I/O)\nWrite temporary files with data received from proxied servers with AIO to boost performance.\nIf you have a reasonable amount of RAM, you are not using spinning disks and your working data set isn’t very big, NGINX can utilize OS page cache recommends not enabling AIO to reduce the overhead of offloading.\naio threads;\naio_write on;\n\nEnable PCRE JIT\nJIT can speed up processing of regular expressions significantly if you have a lot of them.\npcre_jit on;\n\nDetect event loop stalls\nIf you start noticing that your nginx is spending too much time inside ngx_process_events_and_timers, and distribution is bimodal, then you probably are affected by event loop stalls.\n# funclatency &#039;/srv/nginx-bazel/sbin/nginx:ngx_process_events_and_timers&#039; -m\n     msecs               : count     distribution\n         0 -&amp;gt; 1          : 3799     |****************************************|\n         2 -&amp;gt; 3          : 0        |                                        |\n         4 -&amp;gt; 7          : 0        |                                        |\n         8 -&amp;gt; 15         : 0        |                                        |\n        16 -&amp;gt; 31         : 409      |****                                    |\n        32 -&amp;gt; 63         : 313      |***                                     |\n        64 -&amp;gt; 127        : 128      |*                                       |\n\nYou will need more skills to root cause and fix such issues, which is beyond the scope of this article.\nReferences\n\ndropbox.tech/infrastructure/optimizing-web-servers-for-high-throughput-and-low-latency\nnetdevconf.org/1.2/papers/bbr-netdev-1.2.new.new.pdf\nwww.youtube.com/watch\ntechdocs.broadcom.com/us/en/storage-and-ethernet-connectivity/ethernet-nic-controllers/bcm957xxx/adapters/Tuning.html\nwww.amd.com/content/dam/amd/en/documents/epyc-technical-docs/tuning-guides/58489_amd-epyc-9005-tg-nginx.pdf\nwww.amd.com/content/dam/amd/en/documents/epyc-technical-docs/tuning-guides/58472_amd-epyc-9005-tg-linux-network.pdf\ncdrdv2-public.intel.com/334019/334019_Intel%20Ethernet%20700%20Series%20Linux%20Performance%20Tuning%20Guide.pdf\n"},"notes/NGINX/openresty/Customize-error-page-of-ngx.exit":{"title":"Customize error page of ngx.exit","links":[],"tags":[],"content":"This is an example of templating responses for ngx.exit(494) calls from the “legalban” Lua module.\n# in `server` context\nerror_page 494 =451 @legal_ban;\n \nlocation @legal_ban {\n    # Prevent internal redirection cycle.\n    access_by_lua_block {\n        return\n    }\n    root html;\n    try_files /assets/legal/region_block.html =451;\n    charset utf-8;\n    add_header Cache-Control &quot;no-store&quot; always;\n}\n \nlocation /assets/legal {\n    # Always allow static assets to be accessed.\n    access_by_lua_block {\n        return\n    }\n    root html;\n    charset utf-8;\n    expires 30d;\n}\n \n# Restrict access by client IP\naccess_by_lua_block {\n    require(&#039;legalban&#039;).access(ngx.var.remote_addr) # which calls ngx.exit(494) if IP is banned.\n}\nFirst, we defined a custom response code 494 for the ban and its internal redirect to @legal_ban. With this, we can return different pages for the same response code 451. If you only have one predefined response per code to send, we can use it directly and skip the response code override in 494 =451.\nThen, we define the named location @legal_ban that returns a static HTML file from $prefix/html. If the response code is not in the add_header-specific list, you need always to make that directive work. *_by_lua_block { return } is only needed if there is a cycle of internal redirection, which is the case in our example.\nNote that charset utf-8; and properly configuring types (which is included in the default configuration) combined makes NGINX return the correct content-type text/html; charset=utf-8.\nThird, we defined the assets our error page needs, for example .js files and images. These should be referenced by absolute path in our HTML file, and should be always accessible. Therefore, we disable access control with access_by_lua_block { return } and uses a regular location. We also set expires 30d; to allow clients to cache them and reduce server load.\nIf possible, make the HTML file standalone or depend on resources from third-party domains only to remove the third step altogether. This would significantly simplify our configuration.\nFinally, we put access_by_lua_block { ... } here to execute our “legalban” module on every request, and reject those that came from regions we cannot serve for legal reasons."},"notes/NGINX/openresty/OpenResty-Lua-linter":{"title":"OpenResty Lua linter","links":[],"tags":[],"content":"Tools\n\nUse lj-releng or luacheck to identify global variable references.\n\nTips\n\nYou can cache ngx to a local variable.\n\nlocal ngx = ngx\n\nAlways include _VERSION in your module. For example, in the conn module,\n\nlocal _M = {\n  _VERSION = &quot;0.0.1&quot;\n}\n \nlocal mt = { __index = _M }\n \nfunction _M.new(options)\n  local self = { ... }\n  return setmetatable(self, mt)\nend\n \nfunction _M.access(self, remote_addr)\n  ...\nend\n \nreturn _M\n\nYou can call the preceding functions via require(&#039;conn&#039;).new({}) and filter:access(remote_addr) where filter is an object of conn.\n"},"notes/NGINX/proxy/Headers-in-NGINX":{"title":"Headers in NGINX","links":[],"tags":[],"content":"The default set of headers passed to upstream is defined in ngx_http_proxy_headers.\nstatic ngx_keyval_t  ngx_http_proxy_headers[] = {\n    { ngx_string(&quot;Host&quot;), ngx_string(&quot;$proxy_host&quot;) },\n    { ngx_string(&quot;Connection&quot;), ngx_string(&quot;close&quot;) },\n    { ngx_string(&quot;Content-Length&quot;), ngx_string(&quot;$proxy_internal_body_length&quot;) },\n    { ngx_string(&quot;Transfer-Encoding&quot;), ngx_string(&quot;$proxy_internal_chunked&quot;) },\n    { ngx_string(&quot;TE&quot;), ngx_string(&quot;&quot;) },\n    { ngx_string(&quot;Keep-Alive&quot;), ngx_string(&quot;&quot;) },\n    { ngx_string(&quot;Expect&quot;), ngx_string(&quot;&quot;) },\n    { ngx_string(&quot;Upgrade&quot;), ngx_string(&quot;&quot;) },\n    { ngx_null_string, ngx_null_string }\n};"},"notes/NGINX/proxy/Health-Checks-in-NGINX":{"title":"Health Checks in NGINX","links":["notes/NGINX/proxy/How-NGINX-round-robin-upstream-implements-passive-health-checks"],"tags":[],"content":"If Nginx upstream points at a load balancer and overloading is not of concern, you can set max_fails to 0 to turn off passive health checks.\nSee also How NGINX round-robin upstream implements passive health checks."},"notes/NGINX/proxy/How-NGINX-round-robin-upstream-implements-passive-health-checks":{"title":"How NGINX round-robin upstream implements passive health checks","links":[],"tags":[],"content":"Based on NGINX 1.27.4\nPassive health checks\npeer-&gt;accessed represents the last failure time. It is initialized as 0, and set to the current time ngx_time() along with peer-&gt;checked when any request to the peer failed.\npeer-&gt;checked represents the start of the current error window. It is set to current time when the peer is selected as best, and subsequently updated to the current time if peer is selected again after fail_timeout has passed (i.e. time since last checked is over fail_timeout), or when any request to the peer failed.\npeer-&gt;fails represents the number of failures observed during the current error window. It is incremented on each failure, and reset to 0 if check passed for a request AND peer-&gt;accessed &lt; peer-&gt;checked, i.e. peer-&gt;checked was refreshed but peer-&gt;accessed was not. Given that peer-&gt;accessed is a moving goalpost that bumps on each failure event, it’s possible that fails accumulate across multiple fail_timeout time spans if failures continue to happen.\nIf max_fails is not 0, failure condition is met (peer-&gt;fails &gt;= peer-&gt;max_fails) and we are still within the error window (now - peer-&gt;checked &lt;= peer-&gt;fail_timeout), the peer is skipped in the peer selection process.\nCombination of the conditions above raises an interesting question. If there is a continuous stream of failures that happens at an interval shorter than fail_timeout, will fails accumulate to trigger the peer failure condition, or will it be hard reset after each fail_timeout?\nAn interesting behavior of NGINX is that it decrements effective_weight by peer-&gt;weight / peer-&gt;max_fails (integer division!), and increments it by 1 up to weight whenever the peer is considered for use, so there is a penalty in effective weight for failures if peer-&gt;weight / peer-&gt;max_fails is not 0. This penalty reduces the number of requests the peer receives, and subsequently makes a continuous stream of failures less likely to hold.\nAnd even if a peer is temporarily disabled, no new requests would select that peer, so it’s safe to assume that after fail_timeout the counter will be reset as the error window passes and new requests pass the check. If the first requests completed did not pass the check, peer-&gt;checked will be bumped along with peer-&gt;accessed and the peer will be disabled again immediately.\nNonetheless, imperfection of the algorithm makes it nonconforming to the NGINX documentation, which states that fail_timeout is “the time during which the specified number of unsuccessful attempts to communicate with the server should happen to consider the server unavailable.”\n\n\n                  \n                  NOTE\n                  \n                \n\nTakeaway from this: Always set weight to an integer greater than or equal to max_fails, otherwise the weight penalty is ineffective.\n\n\nHistory\nFrom Maxim Dounin, www.mail-archive.com/nginx-devel@nginx.org/msg00514.html\n\nThis is expected behaviour.  Documentation is a bit simplified\nhere, and fail_timeout is used like session time limit - the\npeer→fails counter is reset once there are no failures within\nfail_timeout.\nWhile this might be non-ideal for some use cases, it’s certainly\nnot a bug.\n\nA proposed fix was rejected because\n\nSuch algorithm forget everything about previous failures once per\nfail_timeout, and won’t detect bursts of failures split across\ntwo fail_timeout intervals.\n\nand in a later email,\n\nWell, in normal world if an upstream constantly fails ~1% of\nrequests - it’s not healthy and should not be used.  I\nunderstand that your use case is a bit special though.\n\n\n\nYes, I know this case, sorry, forgot to mention. However, I think it will\nextend detection period to 2-3 fail_timeouts in real life (in theory up to\nmax_fails fail_timeouts, yes, but it’s almost improbable). If we want correct\nimplementation we need per-second array (with fail_timeout elements), that’s\nan\noverkill in my opinion.\n\nSure, per-second array isn’t a solution.\n\nBy the way, leaky bucket approach (like limit_req but\nwith fails per second) might work well here, what do you think?\n\nYes, leaky/token bucket should work.  That’s actually what I think\nabout if I think about changing the above algorithm to something\nstrictly bound to fail_timeout period.\n\nSo Maxim is against the per-second array solution.\nA few years later, www.mail-archive.com/nginx-devel@nginx.org/msg09804.html\n\nDocumentation somewhat oversimplifies things.  The fail_timeout\nsetting is essentially a session timeout, and things work as\nfollows:\n\n\nAs long as there are failures, the fails counter is\nincremented.  If fail_timeout passes since last failure, the fails\ncounter is reset to 0 on the next successful request.\n\n\nIf the fails counter reaches max_fails, no more requests are\nrouted to the peer for fail_timeout time.  After fail_timeout passes,\none request is allowed.  If the request is successful, the fails\ncounter is reset to 0, and further requests to the peer are\nallowed without any limits.\n\n\n\nReferences:\n\nwww.mail-archive.com/search@nginx.org&amp;q=subject:“%5C[BUG%5C?%5C]fail_timeout%5C/max_fails%5C:+code+doesn’t+do+what+doc+says”&amp;o=newest&amp;f=1\n\nalso on nginx.org as mailman.nginx.org/pipermail/nginx-devel/2013-May/003753.html\n\n\nwww.mail-archive.com/search@nginx.org&amp;q=subject:“incorrect+upstream+max_fails+behaviour”&amp;o=newest&amp;f=1\n\nalso on nginx.org as mailman.nginx.org/pipermail/nginx-devel/2020-March/013070.html\n\n\n\nDemo\nSet up NGINX with 1 worker process and two local servers listening on port 3000 and 3001 that always returns 503, and an upstream that points to both servers with max_fails=120 fail_timeout=3s. Then, set up a third server listening on port 3002 that proxy_pass to this upstream and have proxy_next_upstream error timeout http_503 set. Finally, we request the server with roughly 0.5s interval and observe if the error log contains “no live upstreams”.\nThe result from NGINX 1.27.4 confirms our assumption. About 2 minutes later, which is enough time for both upstream servers to reach 60 fails because each request is retried against both servers, NGINX logs “no live upstreams” for all subsequent requests.\nWe also observed a weird behavior that occasionally, the error log would stop for a while after 6 requests, and then returns to the same pattern of continuous errors or another 6 lines of logs. If the pause is about 2 minutes, the peer-&gt;fails counter might have been reset for both peers, so it’s likely that somehow state &amp; NGX_PEER_FAILED was false when the peer used is freed.\nBackup servers\nNGINX only considers the backup servers stored in peers-&gt;next if either ngx_http_upstream_get_peer() did not get a peer (returns NULL) from the primary servers, or the single primary server is down or has reached max_conns."},"notes/NGINX/proxy/Upstream-Keepalive":{"title":"Upstream Keepalive","links":["notes/NGINX/proxy/Headers-in-NGINX"],"tags":[],"content":"To enable keepalive connections to upstream, the keepalive directive must be included in upstream{} blocks, and in the location{} blocks you need to switch HTTP version to 1.1 and clear the default Connection: close request header set by Nginx. See Headers in NGINX.\nproxy_http_version 1.1;\nproxy_set_header   &quot;Connection&quot; &quot;&quot;;\n\n\nIt should be particularly noted that the keepalive directive does not limit the total number of connections to upstream servers that an nginx worker process can open. The _connections_ parameter should be set to a number small enough to let upstream servers process new incoming connections as well.\n\nAlso note that if you have keepalive set in the upstream and max_conns set on a server,\n\nmultiple workers, and the shared memory are enabled, the total number of active and idle connections to the proxied server may exceed the max_conns value.\n\nSo choose the number of keepalive connections carefully.\nYou may also want to customize keepalive_requests and keepalive_timeout, but the defaults are reasonable enough.\nReferences\nnginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive"},"notes/Network/Cloudflare":{"title":"Cloudflare","links":["notes/NGINX/NGINX-realip-configuration-for-Cloudflare"],"tags":[],"content":"First steps after creating a site\nSet TLS encryption mode to Full (strict).\nngx_http_realip_module\nSee NGINX realip configuration for Cloudflare."},"notes/Network/Configure-additional-static-IP-with-netplan":{"title":"Configure additional static IP with netplan","links":[],"tags":[],"content":"It’s not advised to overwrite the .yaml file cloud-init created. Instead, we could only list the additional IPs like so:\nnetwork:\n  version: 2\n  ethernets:\n    eth0:\n      addresses:\n        - 10.10.10.2/24\nThis works because netplan generates config by merging YAML files, and the rules are that sequence values are concatenated, with the new values appended to the old list. See netplan-generate(8) for details.\nAlternatively, you could create /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg as follows to prevent cloud-init from changing network configuration again:\nnetwork:\n  config: disabled\nHowever, this approach would prevent the image to be reused for another instance."},"notes/Network/DNS-Caching":{"title":"DNS Caching","links":[],"tags":[],"content":"JVM\nJava SE 17 defaults to caching DNS lookups forever if a Security Manager is installed. Override networkaddress.cache.ttl if you are still using the deprecated Security Manager to avoid connecting to the wrong IP addresses. Without a Security Manager installed, the default TTL is 30 seconds in HotSpot."},"notes/Network/Direct-Server-Return":{"title":"Direct Server Return","links":["notes/Cloud/Azure/Azure-Load-Balancer"],"tags":["networking"],"content":"Azure\nImplemented in Azure. See Azure Load Balancer.\nGoogle Cloud\nGoogle Cloud implements this in external and internal passthrough Network Load Balancers. cloud.google.com/load-balancing/docs/network\nAWS\nAWS NLB does not make use of it.\nrepost.aws/questions/QUNtYRmHbDRNyJ1g8ltLwnOA/nlb-ip-preservation-over-peering-target-group\n\nThere is no DSR as you correctly mentioned, the AWS SDN handles the connection in such a way that the Client IP is preserved and traffic is still (symmetrically) routed via NLB.\n"},"notes/Network/Ethernet-Frame":{"title":"Ethernet Frame","links":[],"tags":[],"content":"Endianness\nEthernet transmits data with the most-significant octet first. This is considered a big-endian system, with data transmitted in network order.\nWhile higher-level network protocols usually consider octet as their atomic unit, Ethernet belongs to the lowest layers of a network stack and has to deal with ordering of bits within an octet. In Ethernet, the least-significant bit is transmitted first within each octet."},"notes/Network/Global-DNS-Lookup-and-Ping-Services":{"title":"Global DNS Lookup and Ping Services","links":["notes/SRE/Latency-Numbers"],"tags":[],"content":"Web\nDNS\n\ngithub.com/ccbikai/DNS.Surf\n\nPing\n\nping.pe/ (DNS resolution is performed once in central location)\nglobalping.io/ (max 500 endpoints per run, limited to 100000 per hour aggregated)\ntools.bunny.net/latency-test\n\nReverse ping (latency to global locations)\n\nwww.meter.net/tools/world-ping-test/\n\nLatency between IDC locations\n\nLatency Numbers\nwondernetwork.com/pings\n"},"notes/Network/ICMP-ping-packet-size":{"title":"ICMP ping packet size","links":["notes/Network/Path-MTU-Discovery"],"tags":[],"content":"There are 8 bytes of ICMP header and 20 bytes of IPv4 or 40 bytes of IPv6 header for an Echo Request, so with -s of 8000, each ICMPv6 packet is actually 8048=40+8+8000 bytes, and for ICMP it’s typically 8028=20+8+8000 bytes.\nFor the common 1500 bytes MTU, use ping6 &lt;addr&gt; -s 1452. See also Path MTU Discovery."},"notes/Network/IP-and-BGP-ASN-Data-Lookup-Services":{"title":"IP and BGP ASN Data Lookup Services","links":[],"tags":[],"content":"Web\nGlobal\n\nHurricane Electric BGP Toolkit\nbgp.tools/ can discover recursive DNS (“local DNS”) resolver IPs.\nradar.cloudflare.com/ (speed.cloudflare.com/ shows link to current ASN on Radar)\n\nChina (for users in Mainland China)\n\nping.huatuo.qq.com/\n\nAPI\nGlobal\n\nip.guide/\nifconfig.co/\nifconfig.me/\nwww.cloudflare.com/cdn-cgi/trace\n"},"notes/Network/IPv6-Firewall":{"title":"IPv6 Firewall","links":[],"tags":[],"content":"Packets that must not be dropped\nError messages that are essential to the establishment and maintenance of communications:\no  Destination Unreachable (Type 1) - All codes\no  Packet Too Big (Type 2)\no  Time Exceeded (Type 3) - Code 0 only\no  Parameter Problem (Type 4) - Codes 1 and 2 only\nConnectivity checking messages:\no  Echo Request (Type 128)\no  Echo Response (Type 129)\nPath MTU\nDebugging MTU issues is notoriously hard. Path MTU is not guaranteed to work, and varies by source and destination network.\nReferences\n\ndatatracker.ietf.org/doc/html/rfc4890\nblog.cloudflare.com/increasing-ipv6-mtu/\n"},"notes/Network/IPv6-in-IPv4-Tunnel":{"title":"IPv6 in IPv4 Tunnel","links":[],"tags":[],"content":"Protocol 41\n6in4\nHurricane Electric’s IPv6 Tunnel Broker uses this protocol. The prefix allocated is owned by the SP.\n6rd\nAn SP could set up 6rd Border Relays that is delegated a 6rd prefix. The prefix is owned by the SP.\n6to4\n6to4 addresses starts with 2002:, followed by the IPv4 address of its router. The prefix is announced by multiple ASes.\nTeredo\nTeredo addresses starts with 2001:0000:. The prefix is announced by multiple ASes.\nSee also\n\nopenwrt.org/docs/guide-user/network/ipv6/ipv6tunnel-luci\n\nReferences\n\ndatatracker.ietf.org/doc/html/rfc5969\n"},"notes/Network/NIC-Offload":{"title":"NIC Offload","links":["notes/Network/NIC-Scaling-and-Steering"],"tags":[],"content":"Hardware offloading\nTCP Segmentation Offload (TSO)\nUses the TCP protocol to send large packets. Uses the NIC to handle segmentation, and then adds the TCP, IP and data link layer protocol headers to each segment.\nUDP Fragmentation Offload (UFO)\nUses the UDP protocol to send large packets. Uses the NIC to handle IP fragmentation into MTU sized packets for large UDP datagrams.\nLarge Receive Offload (LRO)\nUses the TCP protocol. All incoming packets are re-segmented as they are received, reducing the number of segments the system has to process. They can be merged either in the driver or using the NIC. A problem with LRO is that it tends to re-segment all incoming packets, often ignoring differences in headers and other information which can cause errors.\nIt is generally not possible to use LRO when IP forwarding is enabled. LRO in combination with IP forwarding can lead to checksum errors. Forwarding is enabled if /proc/sys/net/ipv4/ip_forward is set to 1.\nWith mlx5 driver, RFS cannot function if LRO is enabled. See also NIC Scaling and Steering.\nChecksum offload\nThe driver can indicate to the Linux Networking Stack that the hardware successfully validated the IP and L4 checksum so the Linux Networking Stack does not need to deal with IP/L4 Checksum validation.\nSoftware offloading\nGeneric Segmentation Offload (GSO)\nUses the TCP or UDP protocol to send large packets. If the NIC cannot handle segmentation/fragmentation, GSO performs the same operations, bypassing the NIC hardware. This is achieved by delaying segmentation until as late as possible, for example, when the packet is processed by the device driver.\nGeneric Receive Offload (GRO)\nUses either the TCP or UDP protocols. GRO is more rigorous than LRO when resegmenting packets. For example it checks the MAC headers of each packet, which must match, only a limited number of TCP or IP headers can be different, and the TCP timestamps must match. Resegmenting can be handled by either the NIC or the GSO code.\nReferences\n\ndocs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/performance_tuning_guide/network-nic-offloads\ndocs.nvidia.com/nvidia-mlnx-en-documentation-v23-10-2-1-3-1-lts.pdf\n"},"notes/Network/NIC-Scaling-and-Steering":{"title":"NIC Scaling and Steering","links":["notes/Operating-System/Linux/SO_REUSEPORT"],"tags":[],"content":"Receive (RX)\nflowchart LR\n    nic[NIC hardware buffer]\n    irq[hard IRQ]\n    softirq[soft IRQ]\n    recvq[app socket queue]\n    app[application]\n    nic==&gt;irq==&gt;softirq==&gt;recvq==&gt;app\n\nScaling for multi-core system is implemented with hardware-based Receive-Side Scaling (RSS).\nIf the number of hardware queue of a single network interface card becomes a bottleneck, software-based Receive Packet Steering (RPS) can be used to further distribute load across CPU cores, at the cost of increased inter-processor interrupts.\nReceive Flow Steering (RFS) extends RPS behavior to increase the CPU cache hit rate and thereby reduce network latency. RFS uses the RPS backend to calculate the most appropriate CPU, then forwards packets based on the location of the application consuming the packet.\nAccelerated RFS (aRFS) boosts the speed of RFS by adding hardware assistance. Unlike traditional RFS, however, packets are sent directly to a CPU that is local to the thread consuming the data.\nOn the other hand, the Linux kernel’s SO_ATTACH_REUSEPORT_EBPF option allows a program to attach a fully functional BPF program as a load balancing algorithm, which can be used to steer packets as well.\nBoth aRFS and SO_REUSEPORT locality can improve CPU cache efficiency, but the performance improvement is usually negligible when compared to other CPU or I/O intensive operations.\nTransmit (TX)\nIn hosts with a network interface controller (NIC) that supports multiple queues, transmit packet steering (XPS) distributes the processing of outgoing network packets among several queues. This enables multiple CPUs to process the outgoing network traffic and to avoid transmit queue lock contention and, consequently, packet drops.\nCertain drivers, such as ixgbe, i40e, and mlx5 automatically configure XPS. To identify if the driver supports this capability, consult the documentation of your NIC driver. Consult your NIC driver’s documentation to identify if the driver supports this capability. If the driver does not support XPS auto-tuning, you can manually assign CPU cores to the transmit queues.\nReferences\n\ndocs.kernel.org/networking/scaling.html\ndocs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/monitoring_and_managing_system_status_and_performance/tuning-the-network-performance_monitoring-and-managing-system-status-and-performance\n"},"notes/Network/OpenWrt/OpenWrt-DFS-Channel-Experiment":{"title":"OpenWrt DFS Channel Experiment","links":[],"tags":[],"content":"\nThis option allows hostapd to select one of the provided channels when a channel should be automatically selected.\n\nuci set wireless.radio0.channel=auto\nuci add_list wireless.radio0.channels=52-144\nuci commit\n \nwifi"},"notes/Network/OpenWrt/Reload-installed-LuCI-protocols":{"title":"Reload installed LuCI protocols","links":[],"tags":[],"content":"After installing a new protocol for LuCI, it does not appear immediately in the LuCI interface. Restart network from System → Startup to make it happen. This is the same as executing /etc/init.d/network restart over SSH."},"notes/Network/OpenWrt/Restore-Default-OpenWrt-Config":{"title":"Restore Default OpenWrt Config","links":[],"tags":[],"content":"You can get the “stock” configuration for OpenWrt from the /rom/etc/config directory.\nFor example, /rom/etc/config/firewall contains the following.\n\n\n                  \n                  Config could be rewritten \n                  \n                \n\nIf changes were made with LuCI or UCI, e.g. luci-app-firewall: migrate syn_flood option to synflood_protect on save.\n\n\nconfig defaults\n\toption syn_flood\t1\n\toption input\t\tREJECT\n\toption output\t\tACCEPT\n\toption forward\t\tREJECT\n# Uncomment this line to disable ipv6 rules\n#\toption disable_ipv6\t1\n \nconfig zone\n\toption name\t\tlan\n\tlist   network\t\t&#039;lan&#039;\n\toption input\t\tACCEPT\n\toption output\t\tACCEPT\n\toption forward\t\tACCEPT\n \nconfig zone\n\toption name\t\twan\n\tlist   network\t\t&#039;wan&#039;\n\tlist   network\t\t&#039;wan6&#039;\n\toption input\t\tREJECT\n\toption output\t\tACCEPT\n\toption forward\t\tREJECT\n\toption masq\t\t1\n\toption mtu_fix\t\t1\n \nconfig forwarding\n\toption src\t\tlan\n\toption dest\t\twan\n \n# We need to accept udp packets on port 68,\n# see dev.openwrt.org/ticket/4108\nconfig rule\n\toption name\t\tAllow-DHCP-Renew\n\toption src\t\twan\n\toption proto\t\tudp\n\toption dest_port\t68\n\toption target\t\tACCEPT\n\toption family\t\tipv4\n \n# Allow IPv4 ping\nconfig rule\n\toption name\t\tAllow-Ping\n\toption src\t\twan\n\toption proto\t\ticmp\n\toption icmp_type\techo-request\n\toption family\t\tipv4\n\toption target\t\tACCEPT\n \nconfig rule\n\toption name\t\tAllow-IGMP\n\toption src\t\twan\n\toption proto\t\tigmp\n\toption family\t\tipv4\n\toption target\t\tACCEPT\n \n# Allow DHCPv6 replies\n# see github.com/openwrt/openwrt/issues/5066\nconfig rule\n\toption name\t\tAllow-DHCPv6\n\toption src\t\twan\n\toption proto\t\tudp\n\toption dest_port\t546\n\toption family\t\tipv6\n\toption target\t\tACCEPT\n \nconfig rule\n\toption name\t\tAllow-MLD\n\toption src\t\twan\n\toption proto\t\ticmp\n\toption src_ip\t\tfe80::/10\n\tlist icmp_type\t\t&#039;130/0&#039;\n\tlist icmp_type\t\t&#039;131/0&#039;\n\tlist icmp_type\t\t&#039;132/0&#039;\n\tlist icmp_type\t\t&#039;143/0&#039;\n\toption family\t\tipv6\n\toption target\t\tACCEPT\n \n# Allow essential incoming IPv6 ICMP traffic\nconfig rule\n\toption name\t\tAllow-ICMPv6-Input\n\toption src\t\twan\n\toption proto\ticmp\n\tlist icmp_type\t\techo-request\n\tlist icmp_type\t\techo-reply\n\tlist icmp_type\t\tdestination-unreachable\n\tlist icmp_type\t\tpacket-too-big\n\tlist icmp_type\t\ttime-exceeded\n\tlist icmp_type\t\tbad-header\n\tlist icmp_type\t\tunknown-header-type\n\tlist icmp_type\t\trouter-solicitation\n\tlist icmp_type\t\tneighbour-solicitation\n\tlist icmp_type\t\trouter-advertisement\n\tlist icmp_type\t\tneighbour-advertisement\n\toption limit\t\t1000/sec\n\toption family\t\tipv6\n\toption target\t\tACCEPT\n \n# Allow essential forwarded IPv6 ICMP traffic\nconfig rule\n\toption name\t\tAllow-ICMPv6-Forward\n\toption src\t\twan\n\toption dest\t\t*\n\toption proto\t\ticmp\n\tlist icmp_type\t\techo-request\n\tlist icmp_type\t\techo-reply\n\tlist icmp_type\t\tdestination-unreachable\n\tlist icmp_type\t\tpacket-too-big\n\tlist icmp_type\t\ttime-exceeded\n\tlist icmp_type\t\tbad-header\n\tlist icmp_type\t\tunknown-header-type\n\toption limit\t\t1000/sec\n\toption family\t\tipv6\n\toption target\t\tACCEPT\n \nconfig rule\n\toption name\t\tAllow-IPSec-ESP\n\toption src\t\twan\n\toption dest\t\tlan\n\toption proto\t\tesp\n\toption target\t\tACCEPT\n \nconfig rule\n\toption name\t\tAllow-ISAKMP\n\toption src\t\twan\n\toption dest\t\tlan\n\toption dest_port\t500\n\toption proto\t\tudp\n\toption target\t\tACCEPT\n \n \n### EXAMPLE CONFIG SECTIONS\n# do not allow a specific ip to access wan\n#config rule\n#\toption src\t\tlan\n#\toption src_ip\t192.168.45.2\n#\toption dest\t\twan\n#\toption proto\ttcp\n#\toption target\tREJECT\n \n# block a specific mac on wan\n#config rule\n#\toption dest\t\twan\n#\toption src_mac\t00:11:22:33:44:66\n#\toption target\tREJECT\n \n# block incoming ICMP traffic on a zone\n#config rule\n#\toption src\t\tlan\n#\toption proto\tICMP\n#\toption target\tDROP\n \n# port redirect port coming in on wan to lan\n#config redirect\n#\toption src\t\t\twan\n#\toption src_dport\t80\n#\toption dest\t\t\tlan\n#\toption dest_ip\t\t192.168.16.235\n#\toption dest_port\t80\n#\toption proto\t\ttcp\n \n# port redirect of remapped ssh port (22001) on wan\n#config redirect\n#\toption src\t\twan\n#\toption src_dport\t22001\n#\toption dest\t\tlan\n#\toption dest_port\t22\n#\toption proto\t\ttcp\n \n### FULL CONFIG SECTIONS\n#config rule\n#\toption src\t\tlan\n#\toption src_ip\t192.168.45.2\n#\toption src_mac\t00:11:22:33:44:55\n#\toption src_port\t80\n#\toption dest\t\twan\n#\toption dest_ip\t194.25.2.129\n#\toption dest_port\t120\n#\toption proto\ttcp\n#\toption target\tREJECT\n \n#config redirect\n#\toption src\t\tlan\n#\toption src_ip\t192.168.45.2\n#\toption src_mac\t00:11:22:33:44:55\n#\toption src_port\t\t1024\n#\toption src_dport\t80\n#\toption dest_ip\t194.25.2.129\n#\toption dest_port\t120\n#\toption proto\ttcp"},"notes/Network/Path-MTU-Discovery":{"title":"Path MTU Discovery","links":[],"tags":[],"content":"IP layer\nIPv4\nUse ping &lt;addr&gt; -M do -s 8000 on Linux to prohibit fragmentation and conduct PMTUD (Path MTU Discovery).\n$ ping 1.1 -M do -s 8000\nPING 1.1 (1.0.0.1) 8000(8028) bytes of data.\nFrom 140.91.232.7 icmp_seq=1 Frag needed and DF set (mtu = 1500)\nping: local error: message too long, mtu=1500\nping: local error: message too long, mtu=1500\n...\nOn macOS, use ping &lt;addr&gt; -D -s 8000.\nIPv6\nOn Linux the command is the same, just use ping -6 or ping6 to force IPv6.\nOn macOS 14, try sudo ping6 &lt;addr&gt; -s 8000 -Dm to prohibit fragmentation.\n$ sudo ping6 he.net -s 8000 -Dm\nPING6(8048=40+8+8000 bytes) xxxx:xxxx --&gt; 2001:470:0:503::2\nping6: sendmsg: Message too long\nping6: wrote he.net 8008 chars, ret=-1\nThe need for PMTUD is more widespread in IPv6 environments because of enforced no fragmentation for IPv6, less NAT middle boxes that apply MSS clamping, and the reasons below.\n\nBut why did this problem not appear for IPv4 traffic? We believe the same issue exists on IPv4, but it’s less damaging due to the different nature of the network. IPv4 is more mature and the great majority of end-hosts support either MTU 1500 or have their MSS option well configured - or clamped by some middle box. This is different in IPv6 where a large proportion of users use tunnels, have Path MTU strictly smaller than 1500 and use incorrect MSS settings in the TCP header. Finally, Linux implements RFC4821 for IPv4 but not IPv6. RFC4821 (PLPMTUD) has its disadvantages, but does slightly help to alleviate the ICMP blackhole issue.\n\nFirewall\nMake sure to allow ICMPv6 “Packet Too Big” (PTB) messages for all IPv6 hosts you need to communicate with. Otherwise, PMTUD would not work and you can get hanging connections.\nOpenWRT 6rd tunnel MTU\nTake 6rd for example, with a 1500 MTU upstream link, the 6rd interface has a default MTU of 1280 per RFC spec, but IPv4 still has 1500 bytes MTU.\n\nIf the MTU is well-managed such that the IPv4 MTU on the CE WAN side interface is set so that no fragmentation occurs within the boundary of the SP, then the 6rd Tunnel MTU should be set to the known IPv4 MTU minus the size of the encapsulating IPv4 header (20 bytes).  For example, if the IPv4 MTU is known to be 1500 bytes, the 6rd Tunnel MTU might be set to 1480 bytes.  Absent more specific information, the 6rd Tunnel MTU SHOULD default to 1280 bytes.\n\nOn Linux, even though ip link may show mtu 1500, MTU for IPv6 may differ. You can check the actual value at /proc/sys/net/ipv6/conf/&lt;interface&gt;/mtu. This values is automatically adjusted according to the Router Advertisement sent by the router, and OpenWRT takes care of it magically by inheriting the upstream MTU.\nIf for some reason a client sends packets bigger than 1280 bytes, the router returns ICMPv6 PTB messages to ask the client to resend the data in smaller datagrams.\nUpper layer\nTCP\nFor TCP connections, PMTUD is handled automatically by the kernel or NIC.\nUDP\nIf don’t-fragment flag is set on a UDP or raw IP socket, an EMSGSIZE error is returned upon recognizing datagrams that are bigger than the known path MTU. See ip(4) on FreeBSD and ip(7) on Linux for respective socket options IP_PMTUDISC_DO and IP_DONTFRAG.\n\nNote: an implementation can avoid the use of an asynchronous notification mechanism for PMTU decreases by postponing notification until the next attempt to send a packet larger than the PMTU estimate.  In this approach, when an attempt is made to SEND a packet that is larger than the PMTU estimate, the SEND function should fail and return a suitable error indication.  This approach may be more suitable to a connectionless packetization layer (such as one using UDP), which (in some implementations) may be hard to “notify” from the ICMPv6 layer.  In this case, the normal timeout-based retransmission mechanisms would be used to recover from the dropped packets.\n\nVPN\nFor Layer 3 VPNs (L3VPN) that need to transmit IPv6 packets, 1280 bytes plus packet encapsulation overhead is the minimum MTU required.\nThis causes fragmentation on a default OpenWrt 6rd setup, which has 1280 bytes of MTU. You can verify this with tcpdump -i eth0.X host &lt;6rd_peeraddr&gt; on the router. If ping6 he.net -s 1233 on the client triggers ipv6-frag in tcpdump output, that means a 1281-byte IPv6 packet needs to be fragmented to be sent over IPv4 to the 6rd peer.\nBecause of this, for best VPN performance, it is recommended to explicitly set the 6rd tunnel MTU to 1480 bytes if the IPv6 MTU is 1500 bytes, and then verify that packets are not fragmented over the wire with tcpdump.\nReferences\n\ndatatracker.ietf.org/doc/html/rfc8201\nblog.cloudflare.com/increasing-ipv6-mtu\n"},"notes/Network/Prefer-IPv4-over-IPv6":{"title":"Prefer IPv4 over IPv6","links":[],"tags":[],"content":"Operating Systems\n\nLinux: gai.conf\nWindows:\n\nModify registry key DisabledComponents under HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Tcpip6\\Parameters\nOR\nModify prefixpolicies to prefer IPv4-mapped IPv6 address, see netsh interface ipv6 show prefixpolicies for current list\n\n\nmacOS: N/A\n\nBrowsers\n\nFirefox: about:config → network.dns.disableIPv6 (disables IPv6, which makes IPv6-only sites inaccessible)\n\nReferences\n\nlearn.microsoft.com/en-us/troubleshoot/windows-server/networking/configure-ipv6-in-windows\n"},"notes/Network/RHEL-9-Networking":{"title":"RHEL 9 Networking","links":[],"tags":[],"content":"Retiring ifcfg-rh\nQuote from Red Hat blog\n\nIn RHEL 9, /etc/sysconfig/network-scripts (also known as ifcfg files) will no longer be the primary storage for network configuration files. While the ifcfg style is still available, it is no longer the default location where NetworkManager stores new network profiles.\nHistorically, various Linux distributions have spread interface configurations over many places: Debian and its derivatives traditionally use /etc/network/interfaces, CentOS and related distros use /etc/sysconfig/network-scripts, and so on. With more distros adopting NetworkManager, keyfiles in /etc/NetworkManager/system-connections have become the canonical place for network connections on Linux.\n\nNetworkManager plugins\nEven if plugins is explicitly set to ifcfg-rh, keyfile is still loaded with lowest priority.\n\nNote that NetworkManager’s native keyfile plugin is always appended to the end of this list (if it doesn’t already appear earlier in the list).\n\nsettings: Loaded settings plugin: ifcfg-rh (&quot;/usr/lib64/NetworkManager/...&quot;)\nsettings: Loaded settings plugin: keyfile (internal)\n\nGiven the configuration plugins=ifcfg-rh, and rd.neednet=1 (cmdline), preference order is pre-existing connections (e.g. Wired Connection), ifcfg-rh (e.g. System eth0), keyfile (e.g. eth0).\nUnsetting plugins makes it the default keyfile,ifcfg-rh, reversing preference of the last two.\nNetworkManager configuration\nYou can configure multiple connection or device sections by having different sections with a name that all start with “connection” or “device”. For example,\n[connection]\nipv6.ip6-privacy=0\nconnection.autoconnect-slaves=1\nvpn.timeout=120\n \n[connection-wifi-wlan0]\nmatch-device=interface-name:wlan0\nipv4.route-metric=50\n \n[connection-wifi-other]\nmatch-device=type:wifi\nipv4.route-metric=55\nipv6.ip6-privacy=1\nAuto config\nPrevent generation of in-memory connection\nOn startup, NetworkManager tries to not interfere with interfaces that are already configured. It does so by generating a in-memory connection based on the interface current configuration.\nTo let NetworkManager interfere with pre-existing configured interfaces, add\n# /etc/NetworkManager/conf.d/20-discard-eth0-config.conf\n[device]\nmatch-device=interface-name:eth0\nkeep-configuration=no\nThis alone does not apply to connections generated by nm-initrd-generator during early boot, which is native to NetworkManager.\nnm-initrd-generator\nnm-initrd-generator creates configuration files for an early instance of NetworkManager run from the initial ramdisk.\ndracut.cmdline(7) describes all kernel command line parameters processed by dracut. When netroot or rd.neednet is set, a “Wired Connection” is created with a random UUID in /var/run/NetworkManager/system-connections.\nsudo cat /var/run/NetworkManager/system-connections/default_connection.nmconnection\n# Created by nm-initrd-generator\n\n[connection]\nid=Wired Connection\nuuid=xxx-...\ntype=ethernet\nautoconnect-priority=-100\nautoconnect-retries=1\nmulti-connect=3\nwait-device-timeout=60000\n\n[ethernet]\n\n[ipv4]\ndhcp-timeout=10\nmethod=auto\nrequired-timeout=20000\n\n[ipv6]\ndhcp-timeout=10\nmethod=auto\n\n[proxy]\n\n[user]\norg.freedesktop.NetworkManager.origin=nm-initrd-generator\n\nTo let NetworkManager re-configure interfaces after switch root, add\n# /etc/NetworkManager/conf.d/20-discard-eth0-config.conf\n[device-eth0]\nmatch-device=interface-name:eth0\nkeep-configuration=no\nallowed-connections=except:origin:nm-initrd-generator\nReferences\n\nwww.redhat.com/en/blog/rhel-9-networking-say-goodbye-ifcfg-files-and-hello-keyfiles\ngitlab.freedesktop.org/NetworkManager/NetworkManager/-/issues/685\ngitlab.freedesktop.org/NetworkManager/NetworkManager/-/commit/bace14fe1f374db26e49e4e7d61d2fbfce4241cc\n"},"notes/Network/Reverse-Path-Forwarding":{"title":"Reverse-Path Forwarding","links":[],"tags":[],"content":"Strict RPF\nStrict Reverse Path is only applicable in places where routing is symmetrical - where IP datagrams in one direction and responses from the other deterministically follow the same path.\nReferences\n\ndatatracker.ietf.org/doc/html/rfc3704\n"},"notes/Network/SNMPv2-TCP-Statistics":{"title":"SNMPv2 TCP Statistics","links":[],"tags":["networking"],"content":"Source: RFC 2012\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObjectDescriptionRtoAlgorithmThe algorithm used to determine the timeout value used for retransmitting unacknowledged octets.RtoMinThe minimum value permitted by a TCP implementation for the retransmission timeout, measured in milliseconds. More refined semantics for objects of this type depend upon the algorithm used to determine the retransmission timeout. In particular, when the timeout algorithm is rsre(3), an object of this type has the semantics of the LBOUND quantity described in RFC 793.RtoMaxThe maximum value permitted by a TCP implementation for the retransmission timeout, measured in milliseconds. More refined semantics for objects of this type depend upon the algorithm used to determine the retransmission timeout. In particular, when the timeout algorithm is rsre(3), an object of this type has the semantics of the UBOUND quantity described in RFC 793.MaxConnThe limit on the total number of TCP connections the entity can support. In entities where the maximum number of connections is dynamic, this object should contain the value -1.ActiveOpensThe number of times TCP connections have made a direct transition to the SYN-SENT state from the CLOSED state.PassiveOpensThe number of times TCP connections have made a direct transition to the SYN-RCVD state from the LISTEN state.AttemptFailsThe number of times TCP connections have made a direct transition to the CLOSED state from either the SYN-SENT state or the SYN-RCVD state, plus the number of times TCP connections have made a direct transition to the LISTEN state from the SYN-RCVD state.EstabResetsThe number of times TCP connections have made a direct transition to the CLOSED state from either the ESTABLISHED state or the CLOSE-WAIT state.CurrEstabThe number of TCP connections for which the current state is either ESTABLISHED or CLOSE-WAIT.InSegsThe total number of segments received, including those received in error. This count includes segments received on currently established connections.OutSegsThe total number of segments sent, including those on current connections but excluding those containing only retransmitted octets.RetransSegsThe total number of segments retransmitted - that is, the number of TCP segments transmitted containing one or more previously transmitted octets.ConnTableA table containing TCP connection-specific information.InErrsThe total number of segments received in error (for example, bad TCP checksums).OutRstsThe number of TCP segments sent containing the RST flag."},"notes/Network/Speed-Test-Website-and-Tools":{"title":"Speed Test Website and Tools","links":[],"tags":[],"content":"Web\n\nspeed.cloudflare.com/\nfast.com/\nspeed.measurementlab.net/ (also available at www.google.com/search+test)\nwww.speedtest.net/ (no full IPv6 support yet)\n"},"notes/Network/Strong-and-Weak-Host-Model":{"title":"Strong and Weak Host Model","links":[],"tags":[],"content":"Linux\nThe IP implementation in Linux defaults to the weak host model.\nFreeBSD\nThe IP implementation in FreeBSD partially implement the Strong End System model, but it’s off by default and only works in non-forwarding mode. Since FreeBSD 14, you need to set net.inet.rfc1122_strong_es to enable this feature. See inet(4) for details.\nOpenBSD\nSince OpenBSD 6.6, the strong host model is the default in non-forwarding mode.\nPF (Packet Filter) offers a Unicast Reverse Path Forwarding (uRPF) feature. It’s similar to antispoof, and can be used to implement the strong host model with IP forwarding enabled.\nThe uRPF check can be performed on packets via the urpf-failed keyword in filter rules:\nblock in quick from urpf-failed label uRPF\n\nReferences\nblog.kanbach.org/post/network-security-implications-of-host-models/"},"notes/Network/TCP-Handshake":{"title":"TCP Handshake","links":[],"tags":[],"content":"Terminology\nNumbers\nSequence Number:  32 bits\nThe sequence number of the first data octet in this segment (except\nwhen SYN is present). If SYN is present the sequence number is the\ninitial sequence number (ISN) and the first data octet is ISN+1.\n\nAcknowledgment Number:  32 bits\nIf the ACK control bit is set this field contains the value of the\nnext sequence number the sender of the segment is expecting to\nreceive.  Once a connection is established this is always sent.\n\nCurrent segment variables\nSEG.SEQ segment sequence number\nSEG.ACK segment acknowledgment number\nSEG.LEN segment length\nSEG.WND segment window\nSEG.UP  segment urgent pointer\nSEG.PRC segment precedence value\n\nSequence and acknowledgement numbers\nFirst packet (sent by client):\nSEG.SEQ = x # assigned sequence number for the SYN virtual octet\nSEG.ACK = 0\nSecond packet (sent by server):\nSEG.SEQ = y # assigned sequence number for the SYN virtual octet\nSEG.ACK = x+1 # the next sequence number expected to receive, which is the data octect after SYN. relative sequence number in Wireshark is actual number minus `x`, and in this segment is 1.\nThird packet (sent by client):\nSEG.SEQ = x+1 # assigned sequence number for the SYN virtual octet\nSEG.ACK = y+1 # the next sequence number expected to receive from the server\nSEG.LEN = 0\nNext packet sent does not need to wait for acknowledgment of the third packet.\nFourth packet (sent by client):\nSEG.SEQ = x+1 # assigned sequence number for the SYN virtual octet\nSEG.ACK = y+1 # the next sequence number expected to receive from the server\nSEG.LEN = 4 # calculated by deducting Data Offset (header length in Wireshark) from TCP segment size, not on wire\nThe fourth packet usually have PSH bit set to reduce latency."},"notes/Network/TCP-Port-Scanning":{"title":"TCP Port Scanning","links":[],"tags":[],"content":"Preparation\nYou can run rustscan first to find open ports, and then use nmap to analyze the services exposed.\nrustscan -u 10000 --scripts none -a &lt;comma-delimited list of CIDRs, IPs, or hosts&gt;\nRustScan’s scripting engine is extensible but difficult to set up, so you could disable it with --scripts none and run nmap yourself.\nYou may need to increase the local router’s nf_conntrack_max sysctl variable. 262144 connections tracked should take less than 100 MiB of RAM.\nsysctl net.netfilter.nf_conntrack_max=262144\n\nYou may also consider reducing net.netfilter.nf_conntrack_tcp_timeout_syn_sent to let connection tracking end sooner if the remote port is unresponsive, that is, filtered in nmap terms. The default 120 seconds is too long.\nNmap flags\n\n-oX &lt;file&gt;: Output scan in XML format to &lt;file&gt;. Use - for stdout. You could also use -oN or -oG for normal and Grep-able format, respectively.\n-v: Increase verbosity level. Most changes only affect interactive and normal output. Use if you want to watch scan progress.\n\nConnect Scan Timing: About 1.03% done; ETC: 20:25 (1:00:00 remaining)\n\n\n-sV: Probe open ports to determine service/version info.\n-A: Presently this enables OS detection (-O), version scanning (-sV), script scanning (-sC) and traceroute (--traceroute). Note that both OS detection and traceroute require root privileges, so normal users just get -sV -sC.\n-Pn: Treat all hosts as online — skip host discovery.\n-T paranoid|sneaky|polite|normal|aggressive|insane: Set timing template (higher is faster). You can specify them with the -T option and their number (0–5) or their name. If you are on a decent broadband or ethernet connection, I would recommend always using -T4.\n--min-rate &lt;number&gt;: Override Nmap’s dynamic timing to send packets no slower than &lt;number&gt; per second. When the option is given, Nmap will do its best to send packets as fast as or faster than the given rate.\n\nReferences\n\nnmap.org/nsedoc/categories/default.html\n"},"notes/Network/VXLAN":{"title":"VXLAN","links":[],"tags":[],"content":"The Linux kernel uses UDP port 8472 for VXLAN, which is identified as otv in /etc/services."},"notes/Network/Valiant-Load-Balancing":{"title":"Valiant Load Balancing","links":[],"tags":[],"content":"Architecture\nVLB is a very simple design. With full-mesh connected backbone nodes, and two-hop load balancing, each link only needs \\frac{2r}{N} capacity to achieve a guaranteed 100% throughput. N is the number of backbone nodes, and r is the maximum ingress and egress rate.\nFault tolerance\nVLB requires only a small fraction of extra capacity to tolerate failures in the network. To tolerate k node failures, the require link capacity is \\frac{2r}{N-k}. Analyzing link failures is more complicated, but capacity needed is still on the order of \\frac{2r}{N-k}.\nFor example, a 100 node network only needs to be over-provisioned by about 5.3% to tolerate any 5 link failures. Existing backbone networks typically use significantly more over-provisioning, but are unable to make any guarantees.\nReferences\n\nDesigning a Predictable Internet Backbone Network\nDesigning a Predictable Internet Backbone with Valiant Load-Balancing\nEfficient and Robust Routing of Highly Variable Traffic\n"},"notes/Network/Wi-Fi-6E":{"title":"Wi-Fi 6E","links":[],"tags":[],"content":"Vendors\nApple\nSupported devices\n\nMacBook Pro (14-inch, 2023) or MacBook Pro (16-inch, 2023)\nMac mini (2023)\nMac Studio (2023)\nMac Pro (2023)\niPhone 15 Pro or iPhone 15 Pro Max\niPad Pro 11-inch (4th generation) or iPad Pro 12.9 inch (6th generation)\n\nRegulations\nSingapore\nIMDA will amend its Regulations to allocate the radio frequency spectrum 5,925 MHz–6,425 MHz for Wi-Fi use in Singapore.\nIMDA expects Wi-Fi 6E-enabled equipment and devices to be commercially available in Singapore by 3Q 2023.\nReferences\n\nsupport.apple.com/en-us/HT213433\nwww.imda.gov.sg/resources/press-releases-factsheets-and-speeches/factsheets/2023/imda-to-allocate-more-radio-frequency-spectrum-for-wi-fi-connectivity-in-singapore\n"},"notes/Note-Making/Ideaverse-Kit-Plugins":{"title":"Ideaverse Kit Plugins","links":[],"tags":[],"content":"Functionality\nAdvanced Tables\nMarkdown table improvements.\nDataview\nRequired for table automation.\nExcalidraw and ExcaliBrain\nPowerful graph view.\nExcaliBrain: Add “related” to Left-Side Friends.\nList Callouts\nDeferred. Not exportable.\nPeriodic Notes\nDeferred. Already have daily notes.\nQuick Switcher++\nGood search functionality. Can replace default quick switcher hotkey.\nSortable\nRequired for sort in tables.\nTag Wrangler\nDeferred until extensive use of tags.\nZoom\nLogseq-like zoom into sections. Deferred.\nTheming\nStyle Settings\nRequired for some themes like Prism.\nIcon Folder\nRequired for theming. Set “Emoji Style” to “Native” to get emojis.\nOptionally, add Lucide icon pack for more icons.\nCallout Manager\nAdd “box” callout with box icon from Lucide icon pack.\nUtilities\nAuto Link Title\nDisabled due to security concerns.\nCommander\nDeferred. Create shortcut buttons for commands.\nLink Favicons\nDisplay website icons. Disabled due to security concerns.\nHover Editor\nDeferred. A useful utility plugin for editing properties.\nNatural Language Dates\nDeferred. May be helpful when wrangling with calendar.\nNote Refactor\nGood refactoring tool. Extract content to separate note.\nText Format\nFix capitalization of pasted text.\nPaste URL into selection\nGood utility. Deferred."},"notes/Note-Making/Obsidian":{"title":"Obsidian","links":["notes/Note-Making/Ideaverse-Kit-Plugins"],"tags":[],"content":"Core configuration\nEditor\nDisable “Readable line length” for code blocks with long lines.\nFiles and links\nLink format: Shortest path (the default)\nAttachement folder path: Garden/images\nTemplates\nTemplate folder location: Garden/templates\nDaily notes new location: Calendar/notes\nDaily notes template: Garden/templates/Templates, Properties, Daily Note\nHotkeys\n\nAssign Cmd+D to “Open today’s daily note”.\nIn Files &amp; Links, use shortest path possible in vault for new links.\n\nPlugins\nIdeaverse Kit Plugins"},"notes/Note-Making/Open-Source-Diagram-Editors":{"title":"Open Source Diagram Editors","links":[],"tags":[],"content":"Online diagram editors\nMindmap\n\ngithub.com/wanglin2/mind-map\n\nText to diagram\nSee github.com/yuzutech/kroki.\n\nBlockDiag (BlockDiag, SeqDiag, ActDiag, NwDiag, PacketDiag, RackDiag), BPMN, Bytefield, C4 (with PlantUML), D2, DBML, Diagrams.net (experimental), Ditaa, Erd, Excalidraw, GraphViz, Mermaid, Nomnoml, Pikchr, PlantUML, SvgBob, Symbolator, UMLet, Vega, Vega-Lite, WaveDrom and WireViz.\n"},"notes/Observability/Alloy-Collector-Configuration-Example":{"title":"Alloy Collector Configuration Example","links":[],"tags":[],"content":"node_exporter\nBased on the template at grafana.com/docs/grafana-cloud/monitor-infrastructure/integrations/integration-reference/integration-linux-node/.\nChanges:\n\nAdd netstat block to collect additional metrics.\n\nTODOs:\n\nSimilar to github.com/prometheus/node_exporter/pull/2867, propose adding AttemptFails to the default collector.netstat.fields. EstabResets might be worth adding too.\nRestore Alloy configuration to the template if the proposed node_exporter changes are merged to Alloy.\n\ndiscovery.relabel &quot;integrations_node_exporter&quot; {\n  targets = prometheus.exporter.unix.integrations_node_exporter.targets\n\n  rule {\n    target_label = &quot;instance&quot;\n    replacement  = constants.hostname\n  }\n\n  rule {\n    target_label = &quot;job&quot;\n    replacement = &quot;integrations/node_exporter&quot;\n  }\n}\n\nprometheus.exporter.unix &quot;integrations_node_exporter&quot; {\n  disable_collectors = [&quot;ipvs&quot;, &quot;btrfs&quot;, &quot;infiniband&quot;, &quot;xfs&quot;, &quot;zfs&quot;]\n\n  filesystem {\n    fs_types_exclude     = &quot;^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|tmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$&quot;\n    mount_points_exclude = &quot;^/(dev|proc|run/credentials/.+|sys|var/lib/docker/.+)($|/)&quot;\n    mount_timeout        = &quot;5s&quot;\n  }\n\n  netclass {\n    ignored_devices = &quot;^(veth.*|cali.*|[a-f0-9]{15})$&quot;\n  }\n\n  netdev {\n    device_exclude = &quot;^(veth.*|cali.*|[a-f0-9]{15})$&quot;\n  }\n\n  netstat {\n    fields = &quot;&quot;^(.*_(InErrors|InErrs)|Ip_Forwarding|Ip(6|Ext)_(InOctets|OutOctets)|Icmp6?_(InMsgs|OutMsgs)|TcpExt_(Listen.*|Syncookies.*|TCPSynRetrans|TCPTimeouts)|Tcp_(ActiveOpens|AttemptFails|InSegs|OutSegs|OutRsts|PassiveOpens|RetransSegs|CurrEstab)|Udp6?_(InDatagrams|OutDatagrams|NoPorts|RcvbufErrors|SndbufErrors))$&quot;&quot;\n  }\n}\n\nprometheus.scrape &quot;integrations_node_exporter&quot; {\n  targets    = discovery.relabel.integrations_node_exporter.output\n  forward_to = [prometheus.relabel.integrations_node_exporter.receiver]\n}\n\nprometheus.relabel &quot;integrations_node_exporter&quot; {\n  forward_to = [prometheus.remote_write.metrics_service.receiver]\n\n  rule {\n    source_labels = [&quot;__name__&quot;]\n    regex         = &quot;node_scrape_collector_.+&quot;\n    action        = &quot;drop&quot;\n  }\n}\n"},"notes/Observability/Checking-Grafana-Server-Version":{"title":"Checking Grafana Server Version","links":[],"tags":[],"content":"There are two ways to find out the Grafana server’s version.\n\nFind the question mark in UI.\nHover or click it.\nAccess the /api/health endpoint.\n"},"notes/Observability/Grafana-Alerts":{"title":"Grafana Alerts","links":[],"tags":[],"content":"Data-source managed alerts\nAPIs\nAlert generator compliance specification\ngithub.com/prometheus/compliance/blob/main/alert_generator/specification.md\nAPI endpoints\n\nGET /api/v1/alerts\nGET /api/v1/rules\n\nRuler API\nRuler API configures recording rules and alerts, and a backend storage is required to store them.\nReferences\n\ncortexmetrics.io/docs/api/#ruler\ngrafana.com/docs/mimir/latest/references/http-api/#ruler\n\nImplementations\nVictoriaMetrics\nAlert Generator Compliance Specification 1.0 is implemented.\nRuler API endpoints are not implemented yet. See github.com/VictoriaMetrics/VictoriaMetrics/issues/4939.\nGrafana Mimir\nFully supported."},"notes/Observability/Grafana-Dashboard-Annotations":{"title":"Grafana Dashboard Annotations","links":[],"tags":[],"content":"Annotation queries return events that can be visualized as event markers in graphs across the dashboard.\nFor example, node_boot_time_seconds{job=~&quot;$job&quot;,instance=~&quot;$instance&quot;}*1000 &gt; $__from &lt; $__to can detect reboot events and automatically annotate all graphs.\nManual annotations\nManual annotations are stored per dashboard in Grafana’s database, and queried from Annotations &amp; Alerts (Built-in) when displaying the original dashboard."},"notes/Observability/Grafana-Dashboards":{"title":"Grafana Dashboards","links":["notes/Observability/Grafana-Visualizations"],"tags":[],"content":"Issues encountered\nBroken Links\n$__cell does not work in the latest Grafana table panel. Replace them with ${__value.text} instead.\nDashboard examples\n\ngithub.com/portefaix/portefaix-kubernetes/tree/8e46651ba91f724c938b6aa5108f32979c179c56/gitops/argocd/charts/monitoring/kube-prometheus-stack/dashboards (K8s and node exporter)\ngithub.com/VictoriaMetrics/VictoriaMetrics/tree/master/dashboards/vm (VictoriaMetrics)\ngithub.com/Vonng/pigsty/tree/master/files/grafana (PostgreSQL as a service)\n\nSee also Grafana Visualizations."},"notes/Observability/Grafana-Variables":{"title":"Grafana Variables","links":[],"tags":[],"content":"Built-in variables\nGlobal\ngrafana.com/docs/grafana/latest/dashboards/variables/add-template-variables/#global-variables\nLoki\ngrafana.com/docs/grafana-cloud/connect-externally-hosted/data-sources/loki/template-variables/"},"notes/Observability/Grafana-Visualizations":{"title":"Grafana Visualizations","links":[],"tags":[],"content":"Time series\nThis public demo dashboard contains many different examples of how a time series visualization can be configured and styled."},"notes/Observability/Prometheus-Node-Exporter":{"title":"Prometheus Node Exporter","links":[],"tags":[],"content":"Reduce number of metrics\nCentOS\nModify /etc/default/prometheus-node-exporter.\nARGS=&#039;--collector.disable-defaults --web.disable-exporter-metrics --collector.cpu --collector.meminfo --collector.time --collector.stat --collector.filesystem --collector.netdev --collector.diskstats --collector.os&#039;\n\nFreeBSD\nModify /etc/rc.conf.\nnode_exporter_enable=&quot;YES&quot;\nnode_exporter_args=&quot;--collector.disable-defaults --web.disable-exporter-metrics --collector.cpu --collector.meminfo --collector.boottime --collector.time --collector.filesystem --collector.netdev --collector.devstat --collector.os&quot;\n"},"notes/Observability/pwru":{"title":"pwru","links":[],"tags":[],"content":"Requirements\n\npwru requires &gt;= 5.3 kernel to run.\nFor --output-skb &gt;= 5.9 kernel is required.\n(Auto-detected) For --backend=kprobe-multi &gt;= 5.18 kernel is required.\n\nCommands\nBasic command that writes traces with tuple and skb information to a file:\npwru --output-tuple --output-skb --output-file packet.trace-1.txt &#039;tcp port 8080&#039;\n\ntcp port 8080 (TCP packets with source or destination port set to 8080) can be replaced with any pcap-filter.\nFor example, src host 1.1.1.1 and tcp src port 443 can be used to trace packets received from 1.1.1.1:443 to diagnose RX path issues. However unlikely to happen, the src port 443 part of the expression excludes packets from 1.1.1.1 but sent to port 443.\nAdd --filter-func kfree_skb_reason --output-caller to find the function that called kfree_skb_reason(). If it does not provide sufficient information, try --output-stack instead of --output-caller.\nRun with Docker\nYou can also run it in Docker with:\ndocker run --privileged --rm -t --pid=host -v /sys/kernel/debug/:/sys/kernel/debug/ cilium/pwru pwru --output-tuple &#039;tcp port 8080&#039;\n"},"notes/Operating-System/FreeBSD/DTrace":{"title":"DTrace","links":["notes/Programming/C/Flame-Graph"],"tags":[],"content":"One-liners and scripts\nwiki.freebsd.org/DTrace/One-Liners\nCapture stacks\nFor use with Flame Graph.\n# Capture user-level stack at 197 Hz for 60 seconds\nsudo dtrace -x ustackframes=100 -n &#039;profile-197 /execname == &quot;python3.9&quot;/ { @[ustack()] = count(); } tick-60s { exit(0); }&#039; -o out.stacks\n \n# Kernel stack\nsudo dtrace -x stackframes=100 -n &#039;profile-197 /pid == 12345/ { @[stack()] = count(); } tick-60s { exit(0); }&#039; -o out.kstacks\nCapture function argument, return value and latency\nsudo dtrace -s getaddrinfo.d -p 12345\n#!/usr/sbin/dtrace -s\n/*\n * getaddrinfo.d\n */\n \n#pragma D option quiet\n \ndtrace:::BEGIN\n{\n\tprintf(&quot;%-20s  %-4s %-12s %s\\n&quot;, &quot;TIME&quot;, &quot;RET&quot;, &quot;LATENCY(ms)&quot;, &quot;HOST&quot;);\n}\n \npid$target::getaddrinfo:entry\n{\n\tself-&gt;host = copyinstr(arg0);\n\tself-&gt;start = timestamp;\n}\n \npid$target::getaddrinfo:return\n/self-&gt;start/\n{\n\tthis-&gt;delta = (timestamp - self-&gt;start) / 1000000;\n\tprintf(&quot;%-20Y  %-4d %-12d %s\\n&quot;, walltimestamp, arg1, this-&gt;delta, self-&gt;host);\n\tself-&gt;host = 0;\n\tself-&gt;start = 0;\n}"},"notes/Operating-System/FreeBSD/FreeBSD-NIC-Settings":{"title":"FreeBSD NIC Settings","links":[],"tags":[],"content":"ifconfig -m &lt;interface&gt;\n-m display the capability list and all of the supported media for the specified interface. Check the capabilities= line for offload settings.\nCapabilities can be modified with ifconfig as well:\nifconfig &lt;interface&gt; tso4 # Enable tcp(4) segmentation offloading for ip(4) only\nifconfig &lt;interface&gt; -tso4 # Disable tcp(4) segmentation offloading for ip(4) only\nReferences\n\nIFCONFIG(8)\n"},"notes/Operating-System/FreeBSD/PF-Firewall":{"title":"PF Firewall","links":[],"tags":[],"content":"Rules\nRules are evaluated from top to bottom, in the sequence they are written.\nFor each packet or connection evaluated by PF, the last matching rule in the ruleset is the one which is applied.\nHowever, when a packet matches a rule which contains the quick keyword, the rule processing stops and the packet is treated according to that rule.\nSupported operating systems\n\nFreeBSD\nOpenBSD\n"},"notes/Operating-System/FreeBSD/Upgrading-FreeBSD":{"title":"Upgrading FreeBSD","links":[],"tags":[],"content":"freebsd-update\nfreebsd-update is not meant to be run in parallel with the same workdir (-d).\nMinor version upgrades\n\nPrebuilt binary packages will also be provided for all major branches and Tier 1 platforms and will be made available via pkg(8). Package builds will use the oldest supported minor release within each major branch to ensure ABI and KBI backwards compatibility within each major branch, and support all minor versions of each major branch, including -RELEASE and -STABLE.\n\nIt’s safe to use pkg from a EoL-ed minor version, as long as the major branch has not reached its EOL date yet.\nbootcode update\nIf you have the good old MBR, try this to update the bootcode.\ngpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada0\n\nReferences\n\ndocs.freebsd.org/en/books/handbook/cutting-edge/#updating-bootcode\nman.freebsd.org/cgi/man.cgi\n"},"notes/Operating-System/Linux/Asahi-Linux":{"title":"Asahi Linux","links":["notes/Operating-System/Linux/Building-an-RPM-package-from-source"],"tags":[],"content":"Features\ngithub.com/AsahiLinux/docs/wiki/Feature-Support\nSpeakers\nBuilt-in speaker is supported on M1 and M2 MacBooks.\nGPU\nDo not use apps from Flatpak on Asahi Linux. They can not use system GPU drivers yet. Instead, build it from source.\n\nFlatpak cannot use system GPU drivers, and there is no Flatpak runtime for the Asahi drivers yet. You cannot currently use GPU acceleration with Flatpak apps. - www.reddit.com/r/AsahiLinux/comments/105mbgo/comment/j3br7ju/\n"},"notes/Operating-System/Linux/BBR":{"title":"BBR","links":[],"tags":[],"content":"Configuration\nOnly sysctl net.ipv4.tcp_congestion_control is needed since Linux 4.13.\n\nJust a quick announcement that Eric Dumazet has checked in a nice feature in Linux 4.13-rc1 that implements TCP-level pacing in Linux TCP:\ngit.kernel.org/pub/scm/linux/kernel/git/davem/net-next.git/commit/\nThat means that the TCP layer itself can handle the pacing requirements of BBR, if the fq qdisc is not in place. In turn, that means when enabling BBR there is no need to change anything in your qdisc setup, if you don’t want to.\n\nStill, you should tune your qdisc setup for optimal performance."},"notes/Operating-System/Linux/BPF-development-with-Aya-on-aarch64":{"title":"BPF development with Aya on aarch64","links":[],"tags":[],"content":"Prerequisites\nRust toolchain\nInstall with rustup.\ncargo-generate\nInstall openssl headers first.\nBuild bpf-linker with Nix’s LLVM\nbpf-linker requires LLVM. It’s a good idea to use Nix, but it does not provide the prefix needed if installed with nix-env -i.\nerror: No suitable version of LLVM was found system-wide or pointed\n              to by LLVM_SYS_180_PREFIX.\n       \n              Consider using `llvmenv` to compile an appropriate copy of LLVM, and\n              refer to the llvm-sys documentation for more information.\n       \n              llvm-sys: crates.io/crates/llvm-sys\n              llvmenv: crates.io/crates/llvmenv\n\nIt’s recommended to use nix-shell for one-shot builds like this.\nnix-shell -p llvm_18 -p libxml2 --run &quot;cargo install bpf-linker --no-default-features&quot;\n\nNote that we used LLVM 18 here. The LLVM version used to build bpf-linker must match the LLVM version from the rust nightly toolchain defined in {{project-name}}-ebpf/rust-toolchain.toml, which is used to build the eBPF program in your project. For LLVM 18, that means we should specify nightly-2024-07-31 or earlier in the toolchain config file.\nAlbeit not mentioned in bpf-linker’s README, libxml2 is also required.\nMap types\n\nBPF_MAP_TYPE_LRU_HASH provides general purpose hash map storage, and will automatically evict the least recently used entries when the hash table reaches capacity.\n\nPinning\nFrom Map::pin():\n\nWhen a map is pinned it will remain loaded until the corresponding file is deleted.\n\nLog\nSupported display hints: github.com/aya-rs/aya/blob/5397c1ca4b77cd27082e96aab9ab931631df7fa8/aya-log-parser/src/lib.rs#L55-L65\nReading Kconfig\nKernel configs are defined in Kconfig.* files. For example, HZ is defined in github.com/torvalds/linux/blob/b831f83e40a24f07c8dcba5be408d93beedc820f/kernel/Kconfig.hz.\nThere is a PR that implements the same extern data support as libbpf, but only a sample C BPF program is provided. github.com/aya-rs/aya/pull/1017\nPrograms\nsock_ops\n\n\n                  \n                  NOTE\n                  \n                \n\nUntil github.com/aya-rs/aya/issues/987 is implemented, killing the program does not unload the cgroup attachment automatically. However, a new attachment would replace the existing one.\n\n\nIf the program return code is not 1, __cgroup_bpf_run_filter_sock_ops considers it a failure and returns -EPERM.\n/**\n * __cgroup_bpf_run_filter_sock_ops() - Run a program on a sock\n * @sk: socket to get cgroup from\n * @sock_ops: bpf_sock_ops_kern struct to pass to program. Contains\n * sk with connection information (IP addresses, etc.) May not contain\n * cgroup info if it is a req sock.\n * @type: The type of program to be exectuted\n *\n * socket passed is expected to be of type INET or INET6.\n *\n * The program type passed in via @type must be suitable for sock_ops\n * filtering. No further check is performed to assert that.\n *\n * This function will return %-EPERM if any if an attached program was found\n * and if it returned != 1 during execution. In all other cases, 0 is returned.\n */\nint __cgroup_bpf_run_filter_sock_ops(struct sock *sk,\n\t\t\t\t     struct bpf_sock_ops_kern *sock_ops,\n\t\t\t\t     enum cgroup_bpf_attach_type atype)\n{\n\tstruct cgroup *cgrp = sock_cgroup_ptr(&amp;sk-&gt;sk_cgrp_data);\n\tint ret;\n \n\tret = BPF_PROG_RUN_ARRAY_CG(cgrp-&gt;bpf.effective[atype], sock_ops,\n\t\t\t\t    bpf_prog_run);\n\treturn ret == 1 ? 0 : -EPERM;\n}\nEXPORT_SYMBOL(__cgroup_bpf_run_filter_sock_ops);\n__cgroup_bpf_run_filter_sock_ops is lightly wrapped by the BPF_CGROUP_RUN_PROG_SOCK_OPS macro, and its return value is processed by tcp_call_bpf().\n/* Call BPF_SOCK_OPS program that returns an int. If the return value\n * is &lt; 0, then the BPF op failed (for example if the loaded BPF\n * program does not support the chosen operation or there is no BPF\n * program loaded).\n */\n#ifdef CONFIG_BPF\nstatic inline int tcp_call_bpf(struct sock *sk, int op, u32 nargs, u32 *args)\n{\n\tstruct bpf_sock_ops_kern sock_ops;\n\tint ret;\n \n\tmemset(&amp;sock_ops, 0, offsetof(struct bpf_sock_ops_kern, temp));\n\tif (sk_fullsock(sk)) {\n\t\tsock_ops.is_fullsock = 1;\n\t\tsock_owned_by_me(sk);\n\t}\n \n\tsock_ops.sk = sk;\n\tsock_ops.op = op;\n\tif (nargs &gt; 0)\n\t\tmemcpy(sock_ops.args, args, nargs * sizeof(*args));\n \n\tret = BPF_CGROUP_RUN_PROG_SOCK_OPS(&amp;sock_ops);\n\tif (ret == 0)\n\t\tret = sock_ops.reply;\n\telse\n\t\tret = -1;\n\treturn ret;\n}\n-EPERM is not 0, so the returned value is -1 and considered a failure.\nIn case of success, sock_ops.reply is returned to the caller, which is defined in linux/bpf.h as below.\n/* User bpf_sock_ops struct to access socket values and specify request ops\n * and their replies.\n * Some of this fields are in network (bigendian) byte order and may need\n * to be converted before use (bpf_ntohl() defined in samples/bpf/bpf_endian.h).\n * New fields can only be added at the end of this structure\n */\nstruct bpf_sock_ops {\n\t__u32 op;\n\tunion {\n\t\t__u32 args[4];\t\t/* Optionally passed to bpf program */\n\t\t__u32 reply;\t\t/* Returned by bpf program\t    */\n\t\t__u32 replylong[4];\t/* Optionally returned by bpf prog  */\n\t};\nContributing to Aya\nIf a PR touches public API, run the following command to update xtask/public-api/*.txt files and commit them.\ncargo +nightly xtask public-api --bless\naya-ebpf.txt can be regenerated on macOS, but others may require Linux.\nerror[E0432]: unresolved imports `libc::SYS_bpf`, `libc::SYS_perf_event_open`\n  --&gt; aya/src/sys/mod.rs:19:19\n   |\n19 | use libc::{pid_t, SYS_bpf, SYS_perf_event_open};\n   |                   ^^^^^^^  ^^^^^^^^^^^^^^^^^^^ no `SYS_perf_event_open` in the root\n   |                   |\n   |                   no `SYS_bpf` in the root\n\nReferences\n\naya-rs.dev/book/start/development/\ngithub.com/aya-rs/bpf-linker\ngithub.com/torvalds/linux/blob/5e0497553643b6c6acd16c389afb9cec210f4ea9/Documentation/bpf/map_hash.rst\ngithub.com/iovisor/bcc/blob/master/docs/kernel-versions.md#xdp (driver support for XDP in different kernel versions)\ngithub.com/aya-rs/aya/commit/7b71c7e1cd8d6948764d02afb0279151c6eae437#diff-db79c2f426cf46ef19a0265b625663ded9aaf593faa23ab5ca90007f38493e4dR313\ndocs.rs/aya/0.12.0/aya/maps/enum.Map.html#method.pin (“proper” link for the preceding one but docs.rs failed to build it.)\ngithub.com/torvalds/linux/blob/v5.17/kernel/bpf/cgroup.c#L1166-L1181\ngithub.com/torvalds/linux/blob/v5.17/include/uapi/linux/bpf.h#L5828-L5840\n"},"notes/Operating-System/Linux/BPF":{"title":"BPF","links":[],"tags":[],"content":"BPF is a flexible and efficient virtual machine-like construct in the Linux kernel allowing to execute bytecode at various hook points in a safe manner. It is used in a number of Linux kernel subsystems, most prominently networking, tracing and security.\nReferences\n\ndocs.cilium.io/en/stable/bpf/\n"},"notes/Operating-System/Linux/Building-an-RPM-package-from-source":{"title":"Building an RPM package from source","links":[],"tags":[],"content":"Fedora\ngit clone src.fedoraproject.org/rpms/...\n \nsudo dnf install fedpkg\nsudo usermod -a -G mock &lt;current user&gt; # use admin user only\n \n# Pick mock config\nls /etc/mock\n \n# Build packages for Fedora 39 (AArch64)\n# Avoid `build` which requests build on Fedora&#039;s Koji server.\nfedpkg mockbuild --root fedora-39-aarch64\nGeneric\nsudo dnf install rpmdevtools\nrpmdev-setuptree\n \nrpm -i your.src.rpm\n \ncd ~/rpmbuild/SPECS\nvim pkg.spec # and save\n \n# Prepare build dependencies and sources\nsudo dnf builddep pkg.spec\nspectool -gR pkg.spec\n \n# Build the package\nrpmbuild -bb pkg.spec\n \ncd ~/rpmbuild/RPMS/&lt;arch&gt; # replace &lt;arch&gt; with the corresponding architecture\nsudo dnf install ./pkg-xxx.rpm"},"notes/Operating-System/Linux/Building-the-Linux-kernel-on-Copr-Build-Service":{"title":"Building the Linux kernel on Copr Build Service","links":[],"tags":[],"content":"Preparing source repository\nThe kernel source tar is too big to put in a regular git repository, so upstream usually does not commit it. Therefore, you need to get the source files from SRPM via dnf download --source kernel.\nRemember to check the corresponding changelog with dnf changelog kernel, or from the extracted kernel.spec file.\nIn order to reduce the tar file size, xz compression is preferred.\nVersioning\n\nDebian tags backported packages with ~bpoXY+Z suffix.\npackages.debian.org/bookworm-backports/kernel/linux-image-6.6.13+bpo-amd64\nRocky Linux SIG/Security tags packages with .security.0.X suffix.\n\ncurl -O dl.rockylinux.org/pub/sig/9/security/x86_64/security-common/Packages/o/openssh-8.7p1-38.1.el9_4.security.0.7.x86_64.rpm\n\n$ rpm -qip openssh-8.7p1-38.1.el9_4.security.0.7.x86_64.rpm\nName        : openssh\nVersion     : 8.7p1\nRelease     : 38.1.el9_4.security.0.7\n\nCreating a project\nIt is recommended to create separate projects for each kernel X.Y version you maintain.\nCreating a package\nIf you pulled the source repo from upstream, there could be multiple *.spec files around. Remember to specify which spec file to use, otherwise the first spec file found in lexical order will be used.\nDownloading artifacts\nRecursively download .rpm files with wget.\nwget -m -np -A rpm -R &#039;*debug*&#039; -R &#039;kernel-source*&#039; https://...\nExample\nThis is a build of the Linux kernel on openEuler 22.03 LTS SP4 (x86_64) with Copr. Note that .oe2203sp4 is not appended to the release string, which is different from official openEuler packages.\n$ rpm -qip kernel-6.6.0-33.0.0.40.x86_64.rpm\nwarning: kernel-6.6.0-33.0.0.40.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID bf50dae3: NOKEY\nName        : kernel\nVersion     : 6.6.0\nRelease     : 33.0.0.40\nArchitecture: x86_64\nInstall Date: (not installed)\nGroup       : Unspecified\nSize        : 118902540\nLicense     : GPLv2\nSignature   : RSA/SHA256, Sun 14 Jul 2024 04:47:18 AM UTC, Key ID 03c33fb2bf50dae3\nSource RPM  : kernel-6.6.0-33.0.0.40.src.rpm\nBuild Date  : Sun 14 Jul 2024 03:04:30 AM UTC\nBuild Host  : eur-prod-workerlocal-x86-64-normal-prod-00214392-20240714-02502\nVendor      : openEuler Copr - user l2dy\nURL         : www.kernel.org/\nSummary     : Linux Kernel\nDescription :\nThe Linux Kernel, the operating system core itself.\n"},"notes/Operating-System/Linux/De-Prioritize-Process":{"title":"De-Prioritize Process","links":[],"tags":[],"content":"Credits: cron.daily/mlocate, replace $$ with PID of process.\nrenice +19 -p $$ &gt;/dev/null 2&gt;&amp;1\nionice -c2 -n7 -p $$ &gt;/dev/null 2&gt;&amp;1\nHowever, these days on distributions with systemd, nice value mostly works for interactive sessions, but it no longer works globally. See Revisiting Linux CPU scheduling."},"notes/Operating-System/Linux/Desktop-Linux-Performance-Optimization":{"title":"Desktop Linux Performance Optimization","links":[],"tags":[],"content":"Kernel\n\nZen (Tunes the kernel for responsiveness at the cost of throughput and power usage)\nCachyOS wiki.cachyos.org/cachyos-kernels/\n\nMemory\n\ngithub.com/systemd/zram-generator (manage compressed block devices with systemd)\n\nScheduler\n\ngithub.com/AdnanHodzic/auto-cpufreq (Automatic CPU speed &amp; power optimizer)\ngitlab.com/ananicy-cpp/ananicy-cpp (auto nice daemon, with community rules support)\n"},"notes/Operating-System/Linux/Firewalld-Specify-Zone-by-IP-Address":{"title":"Firewalld Specify Zone by IP Address","links":[],"tags":[],"content":"\n\n                  \n                  Warning\n                  \n                \n\nMake sure to add services from the previous zone before adding IP to the new zone.\n\n\nfirewall-cmd --new-zone=minecraft-access --permanent\n\nfirewall-cmd --zone=minecraft-access --add-service=ssh --permanent\nfirewall-cmd --zone=minecraft-access --add-source=a.b.c.d/32 --permanent\nfirewall-cmd --zone=minecraft-access --add-port=25565/tcp --permanent\nfirewall-cmd --reload\n\nRemove --permanent flags and skip the reload command to make runtime changes that are lost on reboot, which is great for experimenting with unfamiliar changes.\nIf the client can change its IP address, risk of locking yourself out is low."},"notes/Operating-System/Linux/Firewalld":{"title":"Firewalld","links":["notes/Operating-System/Linux/Firewalld-Specify-Zone-by-IP-Address"],"tags":[],"content":"Firewall backend\nIn Rocky Linux 9, nftables is the default firewall backend for firewalld.\nZone assignment\nIt is possible to assign zones to connections (e.g. NetworkManager), interfaces and source addresses.\nTo add source addresses to a zone. See Firewalld Specify Zone by IP Address.\nFeatures\nList zones\nfirewall-cmd --get-active-zones\nfirewall-cmd --get-default-zone # for all other interfaces\nPort redirection (IPv4)\nfirewall-cmd --add-forward-port=port=2222:proto=tcp:toport=22:toaddr=10.0.0.1\nIntra-zone forwarding\nforward: yes only works intra-zone. It does not cross zone barriers.\n\nWhen enabled in the default zone, intra zone forwarding can only be applied to the interfaces and sources that have been explicitly added to the current default zone. It can not use a catch-all for all outgoing interfaces as this would allow packets to forward to an interface or source assigned to a different zone.\n\nInter-zone forwarding with policy objects\nInter-zone forwarding is rejected by default.\nchain filter_FWD_public {\n\tjump filter_FWD_public_allow\n[..snip..]\n\treject with icmpx admin-prohibited\n}\n\nchain filter_FWD_public_allow {\n\toifname &quot;enp0s3&quot; accept\n}\n\nWith negative priorities, policies apply before rules in zones and can be used to override default behavior.\n# Policy setup\nfirewall-cmd --permanent --new-policy demo\nfirewall-cmd --permanent --policy=demo --add-ingress-zone=internal\nfirewall-cmd --permanent --policy=demo --add-egress-zone=public\nfirewall-cmd --permanent --zone=public --add-masquerade # masquerade on egress\n \n# Apply restrictions\nfirewall-cmd --permanent --policy=demo --add-rich-rule=&#039;rule service name=&quot;smtp&quot; reject&#039;\nfirewall-cmd --permanent --policy=demo --add-rich-rule=&#039;rule family=&quot;ipv4&quot; destination address=&quot;172.16.0.0/24&quot; accept&#039;\n \n# Ignore rules in following policies and zones\nfirewall-cmd --permanent --policy=demo --set-target REJECT\n \nfirewall-cmd --reload\nfirewall-cmd --info-policy=demo\n# priority: -1\nReferences\n\nfirewalld.org/2020/04/intra-zone-forwarding\nmajor.io/p/forwarding-ports-with-firewalld/\nfirewalld.org/2020/09/policy-objects-introduction\n"},"notes/Operating-System/Linux/Handling-configuration-updates-on-rpm-based-distros":{"title":"Handling configuration updates on rpm-based distros","links":[],"tags":[],"content":"Install and run rpmconf, which presents a dialog similar to dpkg.\nrpmconf -a"},"notes/Operating-System/Linux/IBus-on-Wayland":{"title":"IBus on Wayland","links":[],"tags":[],"content":"\nIBus should be called from the desktop session in %s. For KDE, you can launch ‘%s’ utility and go to “Input Devices” → “Virtual Keyboard” section and select “%s” icon and click “Apply” button to configure IBus in %s. For other desktop sessions, you can copy the ‘Exec=’ line in %s file to a configuration file of the session. Please refer each document about the “Wayland input method” configuration. Before you configure the “Wayland input method”, you should make sure that QT_IM_MODULE and GTK_IM_MODULE environment variables are unset in the desktop session.\n\nOn Fedora 39,\nIBus should be called from the desktop session in Wayland. For KDE, you can launch ‘systemsettings5’ utility and go to “Input Devices” → “Virtual Keyboard” section and select “IBus Wayland” icon and click “Apply” button to configure IBus in Wayland.\nFor other desktop sessions, you can copy the ‘Exec=’ line in org.freedesktop.IBus.Panel.Wayland.Gtk3.desktop file to a configuration file of the session. Please refer each document about the “Wayland input method” configuration. Before you configure the “Wayland input method”, you should make sure that QT_IM_MODULE and GTK_IM_MODULE environment variables are unset in the desktop session."},"notes/Operating-System/Linux/Killing-inactive-SSH-Sessions-on-Server":{"title":"Killing inactive SSH Sessions on Server","links":[],"tags":[],"content":"w # find the TTY to kill\nwho -u | grep pts/XXX # grep for TTY to find PID (last column)\nps aux | grep &lt;PID&gt; # verify what the process is\n \n# If &lt;PID&gt; is an interactive shell,\nkill -HUP &lt;PID&gt;\n# Otherwise\nkill &lt;PID&gt;\nReferences\n\nbash(1)\n"},"notes/Operating-System/Linux/Linux-Distributions-Adoption-Statistics":{"title":"Linux Distributions Adoption Statistics","links":[],"tags":[],"content":"Fedora REPL\nSpec: docs.fedoraproject.org/en-US/infra/sysadmin_guide/dnf-counting/\nData: data-analysis.fedoraproject.org/\nopenSUSE\nmetrics.opensuse.org/"},"notes/Operating-System/Linux/Linux-kernel-mailing-lists":{"title":"Linux kernel mailing lists","links":[],"tags":[],"content":"Web viewers\n\nlore.kernel.org/ (display a full thread with /T/ suffix in URL)\n"},"notes/Operating-System/Linux/List-Out-of-Tree-Modules-in-Linux":{"title":"List Out-of-Tree Modules in Linux","links":[],"tags":[],"content":"cat /proc/modules, where:\n\nO means TAINT_OOT_MODULE (out-of-tree module)\nE means TAINT_UNSIGNED_MODULE (unsigned module)\n+ means MODULE_STATE_COMING (module-is-being-loaded)\n- means MODULE_STATE_GOING (module-is-being-unloaded)\n\nReferences\n\ngithub.com/torvalds/linux/blob/71d7b52cc33bc3b6697cce8a0a5ac9032f372e47/kernel/module/main.c#L3217\ngithub.com/torvalds/linux/blob/71d7b52cc33bc3b6697cce8a0a5ac9032f372e47/kernel/module/main.c#L875\ngithub.com/torvalds/linux/blob/master/kernel/panic.c#L477\n"},"notes/Operating-System/Linux/Name-Resolution-on-Linux":{"title":"Name Resolution on Linux","links":["notes/Operating-System/Linux/nsswitch.conf"],"tags":[],"content":"With glibc, name resolution goes getaddrinfo() → nsswitch.conf (for Name Service Switch, which is not Network Security Services) → /etc/hosts or DNS. If DNS: → libresolv → host.conf → /etc/hosts or real DNS.\nWith musl, name resolution goes getaddrinfo() → /etc/hosts or DNS. name_from_hosts() reads /etc/hosts directly, and name_from_dns_search() calls name_from_dns() for each search domain with resolver configuration read from /etc/resolv.conf.\nBoth glibc and musl defaults to returning all valid addresses for a host that appears in the /etc/hosts file, instead of only the first. musl’s behavior is not configurable and has a fixed limit of at most 48 results. glibc’s behavior can be configured with the multi keyword in the /etc/host.conf file, and can be overridden by the RESOLV_MULTI environment variable.\ngethostbyname()\ngethostbyname() is obsolete, does not support IPv6 and only returns one IP address even if there are multiple entries.\nApplications\nSome applications do name resolution on their own, some always pick the first address returned from getaddrinfo(). The behavior is very application-specific and you should always test it before making conclusion."},"notes/Operating-System/Linux/NetworkManager":{"title":"NetworkManager","links":[],"tags":[],"content":"Common commands\nTo list the currently available network connections:\n~]$ nmcli con show\nNAME              UUID                                  TYPE            DEVICE\nAuto Ethernet     9b7f2511-5432-40ae-b091-af2457dfd988  802-3-ethernet  --\nens3              fb157a65-ad32-47ed-858c-102a48e064a2  802-3-ethernet  ens3\nMyWiFi            91451385-4eb8-4080-8b82-720aab8328dd  802-11-wireless wlp61s0\n\nTo apply changes after a modified connection using nmcli, activate again the connection:\nnmcli con up con-name\nIPv4 configuration\nIt’s possible to configure additional addresses while keep using the automatic DHCP."},"notes/Operating-System/Linux/Partition-Table-and-Mounting-File-Systems":{"title":"Partition Table and Mounting File Systems","links":[],"tags":[],"content":"Inform kernel to re-read the partition table\nUse partprobe(1).\nCheck filesystem mount status\n$ systemctl status home.mount\n● home.mount - /home\n     Loaded: loaded (/etc/fstab; generated)\n     Active: active (mounted) since Wed 2023-09-27 12:35:59 UTC; 1 week 3 days ago\n      Until: Wed 2023-09-27 12:35:59 UTC; 1 week 3 days ago\n      Where: /home\n       What: /dev/mapper/rl-home\n       Docs: man:fstab(5)\n             man:systemd-fstab-generator(8)\n      Tasks: 0 (limit: 75668)\n     Memory: 4.0K\n        CPU: 6ms\n     CGroup: /system.slice/home.mount\n\nsystemd[1]: Mounting /home...\nsystemd[1]: Mounted /home.\n"},"notes/Operating-System/Linux/Rebasing-a-patched-Linux-kernel-on-openEuler-Copr-(EUR)":{"title":"Rebasing a patched Linux kernel on openEuler Copr (EUR)","links":[],"tags":[],"content":"curl -O &#039;repo.openeuler.org/openEuler-24.03-LTS/update/source/Packages/kernel-6.6.0-72.0.0.64.oe2403.src.rpm&#039;\nbsdtar -xvf kernel-6.6.0-72.0.0.64.oe2403.src.rpm kernel.tar.gz\ngzip -d kernel.tar.gz &amp;&amp; xz kernel.tar # gzipped-file may be too large for your Git forge.\n \ncd kernel/\ngit fetch gitee.com/src-openeuler/kernel.git openEuler-24.03-LTS\ngit log FETCH_HEAD\ngit rebase -i &lt;commit hash&gt;\n# include commit that updates all references to kernel.tar.gz to kernel.tar.xz\n# amend the commit that adds the kernel source tarball.\ncp ../kernel.tar.xz ./\ngit add kernel.tar.xz\ngit commit --amend\ngit rebase --continue\n \n# check result and push if satisfied\ngit push --force-with-lease"},"notes/Operating-System/Linux/SO_REUSEPORT":{"title":"SO_REUSEPORT","links":[],"tags":[],"content":"Traditional approaches\nTraditional approaches without SO_REUSEPORT limit the application to one accept queue for each TCP listening port, this creates a bottleneck and the thundering herd problem can cause lock contention. [[epoll#epollexclusive-flag|EPOLLEXCLUSIVE Flag]] solved the thundering herd problem, but connections are not evenly distributed. Nginx had to re-add the socket periodically to workaround this.\nLinux with EPOLLEXCLUSIVE usually notifies only the process which\nwas first to add the listening socket to the epoll instance.  As\na result most of the connections are handled by the first worker\nprocess.  To fix this, we re-add the socket periodically, so other\nworkers will get a chance to accept connections.\n\nSO_REUSEPORT\nSO_REUSEPORT allows multiple sockets to listen on the same port, but an implementation problem was acknowledged when it was being merged. Closing a socket could reset connections during their 3-way handshake, so a hot reload as implemented in Nginx would lose some connections in the process even with connection draining.\nIn Linux 5.14, socket migration was added to address this problem.\nSO_REUSEPORT locality\nNew connections flowing into the network stack are distributed using the usual 5-tuple hash. Packets from any of the RX queues, hitting any CPU, might flow into any of the accept queues.\nBut as Cloudflare says:\n\nWe weren’t able to prove definitely if improving packet locality actually improves performance for a high-level TCP application like an HTTP server. In hindsight it makes sense - the added benefit is minuscule compared to the overhead of running an HTTP server, especially with logic in a high level language like Lua.\n\n\nWe got reminded of the obvious - out of the box Linux is remarkably well tuned.\n\nReferences\n\nlpc.events/event/11/contributions/946/attachments/783/1472/Socket_migration_for_SO_REUSEPORT.pdf\nwww.youtube.com/watch\nblog.cloudflare.com/perfect-locality-and-three-epic-systemtap-scripts\ngithub.com/nginx/nginx/blob/145b228530c364452c14d3184f1eee5e09b324aa/src/event/ngx_event_accept.c#L321-L323\ngit.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/\n"},"notes/Operating-System/Linux/SysRq-key":{"title":"SysRq key","links":[],"tags":[],"content":"/proc/sysrq-trigger\nFor example, echo l &gt; /proc/sysrq-trigger prints a stack backtrace for all active CPUs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommandFunctionbWill immediately reboot the system without syncing or unmounting your disks.cWill perform a system crash and a crashdump will be taken if configured.dShows all locks that are held.eSend a SIGTERM to all processes, except for init.fWill call the oom killer to kill a memory hog process, but do not panic if nothing can be killed.gUsed by kgdb (kernel debugger)hWill display help (actually any other key than those listed here will display help. but h is easy to remember :-)iSend a SIGKILL to all processes, except for init.jForcibly “Just thaw it” - filesystems frozen by the FIFREEZE ioctl.kSecure Access Key (SAK) Kills all programs on the current virtual console. NOTE: See important comments below in SAK section.lShows a stack backtrace for all active CPUs.mWill dump current memory info to your console.nUsed to make RT tasks nice-ableoWill shut your system off (if configured and supported).pWill dump the current registers and flags to your console.qWill dump per CPU lists of all armed hrtimers (but NOT regular timer_list timers) and detailed information about all clockevent devices.rTurns off keyboard raw mode and sets it to XLATE.sWill attempt to sync all mounted filesystems.tWill dump a list of current tasks and their information to your console.uWill attempt to remount all mounted filesystems read-only.vForcefully restores framebuffer consolevCauses ETM buffer dump [ARM-specific]wDumps tasks that are in uninterruptible (blocked) state.xUsed by xmon interface on ppc/powerpc platforms. Show global PMU Registers on sparc64. Dump all TLB entries on MIPS.yShow global CPU Registers [SPARC-64 specific]zDump the ftrace buffer0-9Sets the console log level, controlling which kernel messages will be printed to your console. (0, for example would make it so that only emergency messages like PANICs or OOPSes would make it to your console.)\nReferences\n\ndocs.kernel.org/admin-guide/sysrq.html\n"},"notes/Operating-System/Linux/Systemd-Override":{"title":"Systemd Override","links":[],"tags":[],"content":""},"notes/Operating-System/Linux/Systemd-Service-Hardening":{"title":"Systemd Service Hardening","links":[],"tags":[],"content":"Create the user with $HOME at /run/&lt;username&gt; and configure RuntimeDirectory, so that systemd will create and chown the directory automatically.\nSet RuntimeDirectoryPreserve to no to discard its content when service stops or restarts. If you need more persistence, use StateDirectory and /var/lib/&lt;username&gt; instead.\nGrant CAP_NET_BIND_SERVICE so that the service could bind to well-known ports (0 to 1023).\n[Unit]\nDescription=&lt;description&gt;\nAfter=network.target\n\n[Service]\nType=simple\nExecStart=&lt;start-command&gt;\nUser=&lt;user&gt;\nGroup=&lt;user&gt;\n\n# Grant user writable access to home and working directory that persists until system reboot,\n# because /run is a mount point of &quot;tmpfs&quot;.\nWorkingDirectory=~\nRuntimeDirectory=&lt;username&gt;\nRuntimeDirectoryPreserve=yes\n\n# Hardening\nNoNewPrivileges=true\n\n# User isolation\nPrivateTmp=yes\nProtectHome=yes\n\n# Mount entire file system hierarchy read-only as much as possible\nProtectSystem=strict\nPrivateDevices=yes\nProtectKernelTunables=yes\nProtectControlGroups=yes\nProtectProc=invisible\n\n# Limit capabilities\nCapabilityBoundingSet=CAP_NET_BIND_SERVICE\n\n# Grant capabilities\nAmbientCapabilities=CAP_NET_BIND_SERVICE\n\n[Install]\nWantedBy=multi-user.target\n"},"notes/Operating-System/Linux/TCP-Client-SYN-Retries-on-Linux":{"title":"TCP Client SYN Retries on Linux","links":[],"tags":[],"content":"Number of retries\nCan be configured with sysctl net.ipv4.tcp_syn_retries. The default is 6.\n#define TCP_SYN_RETRIES\t 6\t/* This is how many retries are done\n\t\t\t\t * when active opening a connection.\n\t\t\t\t * RFC1122 says the minimum retry MUST\n\t\t\t\t * be at least 180secs.  Nevertheless\n\t\t\t\t * this value is corresponding to\n\t\t\t\t * 63secs of retransmission with the\n\t\t\t\t * current initial RTO.\n\t\t\t\t */\nTimeout\nDefault initial RTO is 1 second.\n#define TCP_TIMEOUT_INIT ((unsigned)(1*HZ))\t/* RFC6298 2.1 initial RTO value\t*/\nBefore Linux 6.5, the default 6 retries correspond to 63 (1+2+4+…+32) seconds from the first SYN packet till the last retransmission, and 127 seconds (64 seconds later) till the final timeout.\nAfter Linux 6.5, the expected time is 67 seconds (with tcp_syn_linear_timeouts = 4) for last retransmission and 131 seconds for final timeout.\n\nnet.ipv4.tcp_syn_linear_timeouts - INTEGER (since Linux 6.5)\nThe number of times for an active TCP connection to retransmit SYNs with a linear backoff timeout before defaulting to an exponential backoff timeout. This has no effect on SYNACK at the passive TCP side.\nWith an initial RTO of 1 and tcp_syn_linear_timeouts = 4 we would expect SYN RTOs to be: 1, 1, 1, 1, 1, 2, 4, … (4 linear timeouts, and the first exponential backoff using 2^0 * initial_RTO). Default: 4\n\nHowever, actual time from the last retransmission to connection timeout may vary in different environments. You can test the behavior with:\n# nc from nmap\ntime nc -zv -w 200000ms 1.1.1.1 2345\n \n# OpenBSD netcat\ntime nc -zv -w 200 1.1.1.1 2345\n \n# BusyBox nc\ntime nc 1.1.1.1 2345\nFor reference, time till final timeout on macOS is 75 seconds.\n$ time nc -zv -w 200 1.1.1.1 2345\nnc: connectx to 1.1.1.1 port 2345 (tcp) failed: Operation timed out\nnc -zv -w 200 1.1.1.1 2345  0.00s user 0.00s system 0% cpu 1:15.01 total\n\nReferenes\n\ngithub.com/torvalds/linux/blob/795c58e4c7fc6163d8fb9f2baa86cfe898fa4b19/include/net/tcp.h#L110-L117\ngithub.com/torvalds/linux/blob/795c58e4c7fc6163d8fb9f2baa86cfe898fa4b19/include/net/tcp.h#L152\ngithub.com/torvalds/linux/blob/795c58e4c7fc6163d8fb9f2baa86cfe898fa4b19/net/ipv4/tcp_timer.c#L260-L262\ndocs.kernel.org/networking/ip-sysctl.html\n\nComments on kernel source code\n\n3WHS means three-way handshake.\ntcp_init_transfer and subsequently tcp_init_metrics are only called after the connection has been established, so TCP_TIMEOUT_FALLBACK is not used during the three-way handshake.\n"},"notes/Operating-System/Linux/TCP-Kernel-Parameters":{"title":"TCP Kernel Parameters","links":["notes/Operating-System/Linux/BBR"],"tags":[],"content":"Parameter description\nReference\n\nwww.ibm.com/docs/en/linux-on-systems\nsysctl-explorer.net/\n\nnet.ipv4.tcp_congestion_control\nSee BBR.\nnet.ipv4.tcp_rmem\nContains three values that represent the minimum, default and maximum size of the TCP socket receive buffer.\nThe minimum represents the smallest receive buffer size guaranteed, even under memory pressure. The minimum value defaults to 1 page or 4096 bytes.\nThe default value represents the initial size of a TCP sockets receive buffer. This value supersedes net.core.rmem_default used by other protocols. The default value for this setting is 87380 bytes. It also sets the tcp_adv_win_scale and initializes the TCP window size to 65535 bytes.\nThe maximum represents the largest receive buffer size automatically selected for TCP sockets. This value does not override net.core.rmem_max. The default value for this setting is somewhere between 87380 bytes and 6M bytes based on the amount of memory in the system.\nThe recommendation is to use the maximum value of 16M bytes or higher (kernel level dependent) especially for 10 Gigabit adapters.\nnet.ipv4.tcp_wmem\nSimilar to the net.ipv4.tcp_rmem this parameter consists of 3 values, a minimum, default, and maximum.\nThe minimum represents the smallest receive buffer size a newly created socket is entitled to as part of its creation. The minimum value defaults to 1 page or 4096 bytes.\nThe default value represents the initial size of a TCP sockets receive buffer. This value supersedes net.core.rmem_default used by other protocols. It is typically set lower than net.core.wmem_default. The default value for this setting is 16K bytes.\nThe maximum represents the largest receive buffer size for auto-tuned send buffers for TCP sockets. This value does not override net.core.rmem_max. The default value for this setting is somewhere between 64K bytes and 4M bytes based on the amount of memory available in the system.\nThe recommendation is to use the maximum value of 16M bytes or higher (kernel level dependent) especially for 10 Gigabit adapters.\nnet.ipv4.tcp_max_tw_buckets\nSpecifies the maximum number of sockets in the “time-wait” state allowed to exist at any time. If the maximum value is exceeded, sockets in the “time-wait” state are immediately destroyed and a warning is displayed. This setting exists to thwart certain types of “Denial of Service” attacks. Care should be exercised before lowering this value. When changed, its value should be increased, especially when more memory has been added to the system or when the network demands are high and environment is less exposed to external threats.\nThe default value is 262,144. When network demands are high and the environment is less exposed to external threats the value can be increased to 450,000.\nnet.ipv4.tcp_fin_timeout\nThis parameter determines the length of time an orphaned (unreferenced) connection will wait before it is aborted at the local end. This parameter is especially helpful for when something happens to the remote peer which prevents or excessively delays a response. Since each socket used for connections consumes approximately 1.5K bytes of memory, the kernel must pro-actively abort and purge dead or stale resources.\nThe default value for this parameter is typically 60 (seconds).\n[root@kvmhost ~] # sysctl net.ipv4.tcp_fin_timeout net.ipv4.tcp_fin_timeout = 60\nFor workloads or systems that generate or support high levels of network traffic, it can be advantageous to more aggressively reclaim dead or stale resources. For these configurations, it is recommended to reduce this value to below 10 (seconds).\nRecommendations\nCloudflare (high BDP HTTP requests)\nFor high bandwidth-delay product sessions, the maximum amount of data on the network at any time (equiv. BDP) is large.\nTherefore, a large TCP receive window must be used, which is prone to introduce latency spikes.\nThe goal is to open the throughput floodgates for high BDP connections while simultaneously ensuring very low HTTP request latency, and Cloudflare achieved it with kernel patching and the following parameters.\nnet.ipv4.tcp_rmem = 8192 262144 536870912\nnet.ipv4.tcp_wmem = 4096 16384 536870912\nnet.ipv4.tcp_adv_win_scale = -2\nnet.ipv4.tcp_collapse_max_bytes = 6291456\nnet.ipv4.tcp_notsent_lowat = 131072\n\nReference: blog.cloudflare.com/optimizing-tcp-for-high-throughput-and-low-latency"},"notes/Operating-System/Linux/Verify-rpmfusion-signed-RPMs":{"title":"Verify rpmfusion-signed RPMs","links":[],"tags":[],"content":"sudo dnf install distribution-gpg-keys\n \n# You can also use `rpmkeys`, the difference is unknown.\nsudo rpmkeys --import /usr/share/distribution-gpg-keys/rpmfusion/RPM-GPG-KEY-rpmfusion-free-el-9\nrpmkeys -K libva-intel-driver-2.4.1-7.el9.x86_64.rpm\nReferences\n\nrpm-misc(8)\n\n-K, --checksig\n       See and use rpmkeys(8).\n"},"notes/Operating-System/Linux/View-RPM-packge-info":{"title":"View RPM packge info","links":[],"tags":[],"content":"# locally installed and remote package\ndnf info &lt;pkg&gt;\n \n# locally installed package\nrpm -qi &lt;pkg&gt;\n \n# local .rpm file\nrpm -qip &lt;pkg.rpm&gt;"},"notes/Operating-System/Linux/When-to-send-RST-packets-on-close()":{"title":"When to send RST packets on close()","links":[],"tags":[],"content":"Background\nRelevant RFCs:\n\nReset Generation in RFC 793, page 36-37\nRFC 2525, section 2.17\n\nCode\n\ndata was lost when tcp_close() is called.\n\n\t} else if (data_was_unread) {\n\t\t/* Unread data was tossed, zap the connection. */\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONCLOSE);\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\ttcp_send_active_reset(sk, sk-&gt;sk_allocation,\n\t\t\t\t      SK_RST_REASON_NOT_SPECIFIED);\n\nUnder these states, tcp_disconnect() can send RST packets.\n\n/* These states need RST on ABORT according to RFC793 */\n \nstatic inline bool tcp_need_reset(int state)\n{\n\treturn (1 &lt;&lt; state) &amp;\n\t       (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT | TCPF_FIN_WAIT1 |\n\t\tTCPF_FIN_WAIT2 | TCPF_SYN_RECV);\n}\n\t} else if (tcp_need_reset(old_state) ||\n\t\t   (tp-&gt;snd_nxt != tp-&gt;write_seq &amp;&amp;\n\t\t    (1 &lt;&lt; old_state) &amp; (TCPF_CLOSING | TCPF_LAST_ACK))) {\n\t\t/* The last check adjusts for discrepancy of Linux wrt. RFC\n\t\t * states\n\t\t */\n\t\ttcp_send_active_reset(sk, gfp_any(), SK_RST_REASON_NOT_SPECIFIED);\n\t\tWRITE_ONCE(sk-&gt;sk_err, ECONNRESET);\n\nIf the connection is in TCP_CLOSE state (skb is gone), an RST packet is sent in response to any incoming segment except another reset, e.g. if TCP_SYN_SENT transitioned to TCP_CLOSE, and an SYN &amp; ACK packet was received later.\n\nlookup:\n\tsk = __inet_lookup_skb(net-&gt;ipv4.tcp_death_row.hashinfo,\n\t\t\t       skb, __tcp_hdrlen(th), th-&gt;source,\n\t\t\t       th-&gt;dest, sdif, &amp;refcounted);\n\tif (!sk)\n\t\tgoto no_tcp_socket;\n \n# code is simplified for reading\nno_tcp_socket:\n\tdrop_reason = SKB_DROP_REASON_NO_SOCKET;\n\tif (tcp_checksum_complete(skb)) {\n\t\tdrop_reason = SKB_DROP_REASON_TCP_CSUM;\n\t} else {\n\t\ttcp_v4_send_reset(NULL, skb, sk_rst_convert_drop_reason(drop_reason));\n\t}\nBoth tcp_send_active_reset() and tcp_v4_send_reset() / tcp_v6_send_reset() call trace_tcp_send_reset(), which is defined in include/trace/events/tcp.h.\n/*\n * skb of trace_tcp_send_reset is the skb that caused RST. In case of\n * active reset, skb should be NULL\n */\nTRACE_EVENT(tcp_send_reset,\n \n\tTP_PROTO(const struct sock *sk,\n\t\t const struct sk_buff *skb,\n\t\t const enum sk_rst_reason reason),\n \n\tTP_ARGS(sk, skb, reason),\n \n\tTP_STRUCT__entry(\n\t\t__field(const void *, skbaddr)\n\t\t__field(const void *, skaddr)\n\t\t__field(int, state)\n\t\t__field(enum sk_rst_reason, reason)\n\t\t__array(__u8, saddr, sizeof(struct sockaddr_in6))\n\t\t__array(__u8, daddr, sizeof(struct sockaddr_in6))\n\t),\n \n\tTP_fast_assign(\n\t\t__entry-&gt;skbaddr = skb;\n\t\t__entry-&gt;skaddr = sk;\n\t\t/* Zero means unknown state. */\n\t\t__entry-&gt;state = sk ? sk-&gt;sk_state : 0;\n \n\t\tmemset(__entry-&gt;saddr, 0, sizeof(struct sockaddr_in6));\n\t\tmemset(__entry-&gt;daddr, 0, sizeof(struct sockaddr_in6));\n \n\t\tif (sk &amp;&amp; sk_fullsock(sk)) {\n\t\t\tconst struct inet_sock *inet = inet_sk(sk);\n \n\t\t\tTP_STORE_ADDR_PORTS(__entry, inet, sk);\n\t\t} else if (skb) {\n\t\t\tconst struct tcphdr *th = (const struct tcphdr *)skb-&gt;data;\n\t\t\t/*\n\t\t\t * We should reverse the 4-tuple of skb, so later\n\t\t\t * it can print the right flow direction of rst.\n\t\t\t */\n\t\t\tTP_STORE_ADDR_PORTS_SKB(skb, th, entry-&gt;daddr, entry-&gt;saddr);\n\t\t}\n\t\t__entry-&gt;reason = reason;\n\t),\n \n\tTP_printk(&quot;skbaddr=%p skaddr=%p src=%pISpc dest=%pISpc state=%s reason=%s&quot;,\n\t\t  __entry-&gt;skbaddr, __entry-&gt;skaddr,\n\t\t  __entry-&gt;saddr, __entry-&gt;daddr,\n\t\t  __entry-&gt;state ? show_tcp_state_name(__entry-&gt;state) : &quot;UNKNOWN&quot;,\n\t\t  __print_symbolic(__entry-&gt;reason, DEFINE_RST_REASON(FN, FNe)))\n);\nThis version of trace_tcp_send_reset is from Linux v6.10. On older versions of the kernel, sk_rst_reason is not available, and RST packets on time-wait sockets or no socket are not traced.\nReferences\n\ngithub.com/torvalds/linux/blob/528dd46d0fc35c0176257a13a27d41e44fcc6cb3/net/ipv4/tcp.c\n"},"notes/Operating-System/Linux/epoll":{"title":"epoll","links":[],"tags":[],"content":"EPOLLEXCLUSIVE flag\nLimitations\nThe following values may be specified in conjunction with EPOLLEXCLUSIVE: EPOLLIN, EPOLLOUT, EPOLLWAKEUP, and EPOLLET.  EPOLLHUP and EPOLLERR can also be specified, but this is not required.\nCentOS 7\nBackport to CentOS 7.3 only supports EPOLLIN and EPOLLOUT events alongside the implicit EPOLLHUP and EPOLLERR events.\n* Mon May 02 2016 Rafael Aquini &lt;aquini@redhat.com&gt; [3.10.0-386.el7]\n- [fs] epoll: restrict EPOLLEXCLUSIVE to POLLIN and POLLOUT (Hannes Frederic Sowa) [1245628]\n- [fs] epoll: add EPOLLEXCLUSIVE flag (Hannes Frederic Sowa) [1245628]\n\ngit.centos.org/rpms/kernel/raw/244b67caa40f10db4d00ce3856382c07cef5b651/f/SPECS/kernel.spec"},"notes/Operating-System/Linux/firejail":{"title":"firejail","links":[],"tags":[],"content":"mkdir ~/jail\nfirejail --private=~/jail bash"},"notes/Operating-System/Linux/iptables":{"title":"iptables","links":[],"tags":[],"content":"Tables and chains\nen.wikibooks.org/wiki/Communication_Networks/IP_Tables\n\nA simplified diagram focused on iptables is available at stuffphilwrites.com/2014/09/iptables-processing-flowchart/.\nIP sets\nipset expresses complex IP addresses and port based rulesets with one single iptables rule, making it very efficient compared to multiple iptable rules."},"notes/Operating-System/Linux/kdump":{"title":"kdump","links":[],"tags":[],"content":"Estimating kdump file size\nmakedumpfile -f --mem-usage /proc/kcore\n\nThe makedumpfile command reports in pages. This means that you must calculate the size of memory in use against the kernel page size, which in the Red Hat Enterprise Linux kernel, is 4 kilobytes for AMD64 and Intel 64 architectures, and 64 kilobytes for IBM POWER architecture.\n\nReferences\n\ndocs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/kernel_administration_guide/kernel_crash_dump_guide#sect-estimating-kdump-usage\n"},"notes/Operating-System/Linux/nftables":{"title":"nftables","links":[],"tags":[],"content":"The nft command\nList rules\nnft list ruleset\nTrace all trafiic\nnft add chain filter trace_chain { type filter hook prerouting priority -301\\; }\nnft add rule filter trace_chain meta nftrace set 1\n \nnft monitor trace\n \nnft delete chain filter trace_chain\nConfiguration\nfrancis.begyn.be/blog/nixos-home-router is a great example if you are familiar with iptables, but note that you can name the tables and chains differently. pavluk.org/blog/2022/01/26/nixos_router.html is another more complicated setup."},"notes/Operating-System/Linux/nsswitch.conf":{"title":"nsswitch.conf","links":[],"tags":[],"content":"nsswitch.conf, the Name Service Switch configuration file, is used by glibc and certain other applications. Most notably, it controls the user, group and host lookup processes.\nRHEL 9, based on Fedora 34, has glibc 2.34 that supports automatically reloading nsswitch.conf if it is changed.\nSince Fedora 36, nsswitch.conf is managed by authselect instead of being directly under glibc.\nReferences\n\ndevelopers.redhat.com/blog/2021/01/15/coming-in-glibc-2-33-reloadable-nsswitch-conf\ndocs.fedoraproject.org/en-US/quick-docs/fedora-and-red-hat-enterprise-linux/\nfedoraproject.org/wiki/Changes/Make_Authselect_Mandatory\n"},"notes/Operating-System/Linux/syslogd":{"title":"syslogd","links":[],"tags":[],"content":"Local socket\n/dev/log is a local Unix domain socket from where local syslog messages are read. The can alleviate disk I/O from application processes."},"notes/Operating-System/Linux/yay":{"title":"yay","links":[],"tags":[],"content":"Install\n/usr/bin/core_perl must be added to PATH when you run yay. Re-login to fix pod2man not found issue.\nClean build\nClean build is effectively git reset --hard origin/HEAD; git clean -fdx before building the package."},"notes/Operating-System/NixOS/nixpkgs":{"title":"nixpkgs","links":["notes/Software/Nix-Channel-Status"],"tags":[],"content":"Nix and nixpkgs API documentation\nnoogle.dev/\nNix channels\nnixos.org/manual/nixpkgs/unstable/#overview-of-nixpkgs\n\nPackages, including the Nix packages collection, are distributed through channels. The collection is distributed for users of Nix on non-NixOS distributions through the channel nixpkgs-unstable. Users of NixOS generally use one of the nixos-* channels, e.g. nixos-22.11, which includes all packages and modules for the stable NixOS 22.11. Stable NixOS releases are generally only given security updates. More up to date packages and modules are available via the nixos-unstable channel.\nBoth nixos-unstable and nixpkgs-unstable follow the master branch of the nixpkgs repository, although both do lag the master branch by generally a couple of days. Updates to a channel are distributed as soon as all tests for that channel pass, e.g. this table shows the status of tests for the nixpkgs-unstable channel.\n\n\nnixpkgs-unstable: builds all packages for supported platforms.\nnixos-*: builds all packages only for Linux.\n\nSee also Nix Channel Status.\nLocales on non-NixOS Linux distros\nnixos.wiki/wiki/Locales\nOn Rocky Linux, you should install glibc-all-langpacks, which provides the locale-archive file needed.\nWithout github.com/NixOS/nixpkgs/commit/de64f4939609ba9c258446eb17f9ec7425934f77, which the nixos-24.11 branch does not have, you also need to set LOCALE_ARCHIVE explicitly to avoid using the nixpkgs-bundled locale-archive with just C.UTF-8.\nexport LOCALE_ARCHIVE=/usr/lib/locale/locale-archive\nAlternatively, you could install glibcLocales from nixpkgs and reference the symlink in your .nix-profile via .zshenv.\nexport LOCALE_ARCHIVE=~/.nix-profile/lib/locale/locale-archive # zsh expands ~ to $HOME\nTerminfo on non-NixOS Linux distros\nSimilarly, you may also need to export TERMINFO_DIRS in .zshenv.\nexport TERMINFO_DIRS=/usr/share/terminfo\nBEAM language packages\nraw.githubusercontent.com/NixOS/nixpkgs/master/pkgs/top-level/all-packages.nix\n  inherit (beam.interpreters)\n    erlang erlang_27 erlang_26 erlang_25\n    elixir elixir_1_18 elixir_1_17 elixir_1_16 elixir_1_15 elixir_1_14\n    elixir-ls;\n# ...\n  beamPackages = dontRecurseIntoAttrs beam27Packages;\n  beamMinimalPackages = dontRecurseIntoAttrs beamMinimal27Packages;\n \n  beam25Packages = recurseIntoAttrs beam.packages.erlang_25;\n  beam26Packages = recurseIntoAttrs beam.packages.erlang_26;\n  beam27Packages = recurseIntoAttrs beam.packages.erlang_27;\nraw.githubusercontent.com/NixOS/nixpkgs/master/pkgs/development/beam-modules/default.nix\n      # BEAM-based languages.\n      elixir = elixir_1_18;\ngithub.com/NixOS/nixpkgs/blob/master/doc/languages-frameworks/beam.section.md\n\nAll BEAM-related expressions are available via the top-level beam attribute, which includes:\n\ninterpreters: a set of compilers running on the BEAM, including multiple Erlang/OTP versions (beam.interpreters.erlang_22, etc), Elixir (beam.interpreters.elixir) and LFE (Lisp Flavoured Erlang) (beam.interpreters.lfe).\npackages: a set of package builders (Mix and rebar3), each compiled with a specific Erlang/OTP version, e.g. beam.packages.erlang22.\n\nThe default Erlang compiler, defined by beam.interpreters.erlang, is aliased as erlang. The default BEAM package set is defined by beam.packages.erlang and aliased at the top level as beamPackages.\nTo create a package builder built with a custom Erlang version, use the lambda, beam.packagesWith, which accepts an Erlang/OTP derivation and produces a package builder similar to beam.packages.erlang.\nMany Erlang/OTP distributions available in beam.interpreters have versions with ODBC and/or Java enabled or without wx (no observer support). For example, there’s beam.interpreters.erlang_22_odbc_javac, which corresponds to beam.interpreters.erlang_22 and beam.interpreters.erlang_22_nox, which corresponds to beam.interpreters.erlang_22.\n\nelixir-ls\nelixir-ls is built with the default elixir in BEAM modules. To use a different version of Elixir, you need to override it. For example:\npackages.${system} = rec {\n  erlang = pkgs.beam.interpreters.erlang_27;\n  elixir = pkgs.beam.packages.erlang_27.elixir_1_18;\n  elixir-ls =\n    (pkgs.beam.packages.erlang_27.elixir-ls.override { inherit elixir; });\n}\nYou may still need to install hex locally and run elixir-ls once to “install the ElixirLS release” locally.\nmix local.hex\nelixir-ls\n# press Enter to quit\nIf you see the following error, configure locales per Locales on non-NixOS Linux distros.\nwarning: the VM is running with native name encoding of latin1 which may cause Elixir to malfunction as it expects utf8. Please ensure your locale is set to UTF-8 (which can be verified by running &quot;locale&quot; in your shell) or set the ELIXIR_ERL_OPTIONS=&quot;+fnu&quot; environment variable\n\nFor Emacs, lsp-mode has lsp-elixir-server-command set to &#039;(&quot;language_server.sh&quot;) by default, so you need to change that to elixir-ls, which is used to name the executable in nixpkgs.\nAd hoc commands\nImage conversion and compression examples:\nnix run -- nixpkgs#imagemagick identify a.png\nnix run -- nixpkgs#imagemagick a.png a.jpg\n \nnix run -- nixpkgs#oxipng -o 4 --strip safe --alpha a.png\nReferences\n\nnixos.org/guides/nix-pills/14-override-design-pattern\ngithub.com/NixOS/nixpkgs/blob/89d341f70f7666c346c353a6eb09186f3d6fabb1/doc/using/overrides.chapter.md#pkgoverride-sec-pkg-override\n"},"notes/Operating-System/OpenWRT/WiFi":{"title":"WiFi","links":[],"tags":[],"content":"Debug commands\niw phy0 info # supported frequencies and TX power\niw phy0 reg get # regulatory domain\niw phy0 channels # DFS state\niw phy0 get txq\niw wlan0 info # current ssid, type, channel &amp; width and txpower\niw dev wlan0 station dump # list of associated stations\n \n# All interfaces&#039; info\niw phy\niw dev\nAP optimization\nRegulatory domain\nThe default country 00 is more restrictive than others. For example, under country SG,\n\nChannel 32 (5150 - 5170) becomes available.\nMaximum transmit power (txpower) increases from 20 to 23 dBm (subject to device capabilities).\n\nroot@OpenWrt:~# iw phy0 reg get\nglobal\ncountry 00: DFS-UNSET\n\t(755 - 928 @ 2), (N/A, 20), (N/A), PASSIVE-SCAN\n\t(2402 - 2472 @ 40), (N/A, 20), (N/A)\n\t(2457 - 2482 @ 20), (N/A, 20), (N/A), AUTO-BW, PASSIVE-SCAN\n\t(2474 - 2494 @ 20), (N/A, 20), (N/A), NO-OFDM, PASSIVE-SCAN\n\t(5170 - 5250 @ 80), (N/A, 20), (N/A), AUTO-BW\n\t(5250 - 5330 @ 80), (N/A, 20), (0 ms), DFS, AUTO-BW, PASSIVE-SCAN\n\t(5490 - 5730 @ 160), (N/A, 20), (0 ms), DFS, PASSIVE-SCAN\n\t(5735 - 5835 @ 80), (N/A, 20), (N/A), PASSIVE-SCAN\n\t(57240 - 63720 @ 2160), (N/A, 0), (N/A)\n \nroot@OpenWrt:~# iw phy0 reg get\nglobal\ncountry SG: DFS-FCC\n\t(2400 - 2483 @ 40), (N/A, 23), (N/A)\n\t(5150 - 5250 @ 80), (N/A, 23), (N/A), AUTO-BW\n\t(5250 - 5350 @ 80), (N/A, 20), (0 ms), DFS, AUTO-BW\n\t(5470 - 5725 @ 160), (N/A, 27), (0 ms), DFS\n\t(5725 - 5850 @ 80), (N/A, 30), (N/A)\n\t(57000 - 66000 @ 2160), (N/A, 40), (N/A)\nSee also git.kernel.org/pub/scm/linux/kernel/git/sforshee/wireless-regdb.git/tree/db.txt\n# This is the world regulatory domain\ncountry 00:\n\t# There is no global intersection for 802.11ah, so just mark the entire\n\t# possible band as NO-IR\n\t(755 - 928 @ 2), (20), NO-IR\n\t(2402 - 2472 @ 40), (20)\n\t# Channel 12 - 13.\n\t(2457 - 2482 @ 20), (20), NO-IR, AUTO-BW\n\t# Channel 14. Only JP enables this and for 802.11b only\n\t(2474 - 2494 @ 20), (20), NO-IR, NO-OFDM\n\t# Channel 36 - 48\n\t(5170 - 5250 @ 80), (20), NO-IR, AUTO-BW\n\t# Channel 52 - 64\n\t(5250 - 5330 @ 80), (20), NO-IR, DFS, AUTO-BW\n\t# Channel 100 - 144\n\t(5490 - 5730 @ 160), (20), NO-IR, DFS\n\t# Channel 149 - 165\n\t(5735 - 5835 @ 80), (20), NO-IR\n\t# IEEE 802.11ad (60GHz), channels 1..3\n\t(57240 - 63720 @ 2160), (0)\nWiFi channel confusion\nWith an 80 MHz wide channel, the center frequency falls in between 20 MHz channels. OpenWRT and iw consider this central frequency in the lower frequency channel.\nFor example, the 80 MHz wide channel formed from channels 36 and 48 is considered “channel 40”, while the new naming convention for 802.11ac recommends 42 instead.\nsupport.metageek.com/hc/en-us/articles/203532644-802-11ac-Channels\nClient mode\nconfig wifi-iface &#039;wifinet1&#039;\n\toption device &#039;radio0&#039;\n\toption mode &#039;sta&#039;\n\toption ssid &#039;AP_SSID&#039;\n\toption encryption &#039;psk2&#039;\n\toption bssid &#039;xx:xx:xx:xx:xx:xx&#039;\n\toption key &#039;xxxxxxxx&#039;\n\toption network &#039;wan&#039;\n\nDon’t use AP+STA on one physical radio without travelmate.\n\nA logical combination of AP+STA mode on one physical radio allows most of OpenWrt supported router devices to connect to a wireless hotspot/station (STA) and provide a wireless access point (AP) from that hotspot at the same time. Downside of this solution: whenever the STA interface looses the connection it will go into an active scan cycle which renders the radio unusable for AP mode operation, therefore the AP is taken down if the STA looses its association.\nTo avoid these kind of deadlocks, travelmate will set all station interfaces to an “always off” mode and connects automatically to available/configured hotspots.\n\ngithub.com/openwrt/packages/blob/openwrt-21.02/net/travelmate/files/README.md\nDFS\nDebug script\n\nSave list of unusable channels when radar is detected.\n\nThe DFS-RADAR-DETECTED message doesn’t show which channels are occupied by radar. freq and chan_width data is from the channel configured on device.\n#!/bin/ash\nset -e\n \nwhile :; do\n  logread -l 100 | grep DFS-RADAR-DETECTED &amp;&amp; break || sleep 10\ndone\n \ndate &gt;&gt; /tmp/radar_channels.log\niw phy0 channels &gt;&gt; /tmp/radar_channels.log"},"notes/Operating-System/macOS/Download-macOS-installer":{"title":"Download macOS installer","links":[],"tags":[],"content":"Download recent releases\nsoftwareupdate --fetch-full-installer --full-installer-version 10.15\nVersion can be taken from the list below.\n$ softwareupdate --list-full-installers\nFinding available software\nSoftware Update found the following full installers:\n* Title: macOS Sonoma, Version: 14.5, Size: 13353373KiB, Build: 23F79, Deferred: NO\n* Title: macOS Sonoma, Version: 14.4.1, Size: 13298513KiB, Build: 23E224, Deferred: NO\n* Title: macOS Sonoma, Version: 14.4, Size: 13297753KiB, Build: 23E214, Deferred: NO\n* Title: macOS Sonoma, Version: 14.3.1, Size: 13073278KiB, Build: 23D60, Deferred: NO\n* Title: macOS Ventura, Version: 13.6.7, Size: 11924125KiB, Build: 22G720, Deferred: NO\n* Title: macOS Ventura, Version: 13.6.6, Size: 11917983KiB, Build: 22G630, Deferred: NO\n* Title: macOS Ventura, Version: 13.6.5, Size: 11916934KiB, Build: 22G621, Deferred: NO\n* Title: macOS Ventura, Version: 13.6.4, Size: 11912664KiB, Build: 22G513, Deferred: NO\n* Title: macOS Monterey, Version: 12.7.5, Size: 12116686KiB, Build: 21H1222, Deferred: NO\n* Title: macOS Monterey, Version: 12.7.4, Size: 12117810KiB, Build: 21H1123, Deferred: NO\n* Title: macOS Monterey, Version: 12.7.3, Size: 12109975KiB, Build: 21H1015, Deferred: NO\n* Title: macOS Big Sur, Version: 11.7.10, Size: 12125478KiB, Build: 20G1427, Deferred: NO\n* Title: macOS Catalina, Version: 10.15.7, Size: 8055650KiB, Build: 19H15, Deferred: NO\n* Title: macOS Catalina, Version: 10.15.7, Size: 8055522KiB, Build: 19H2, Deferred: NO\n* Title: macOS Catalina, Version: 10.15.6, Size: 8055450KiB, Build: 19G2021, Deferred: NO\nOlder releases\nUse github.com/ninxsoft/Mist for older releases, including Mac OS X Lion 10.7.5."},"notes/Operating-System/macOS/Homebrew":{"title":"Homebrew","links":[],"tags":[],"content":"Package Index\nSince Homebrew 4.0.0, it defaults to using JSON files downloaded from API rather than local core and cask taps. The API provides links to binaries hosted on their GitHub Packages registry.\nReferences\n\nbrew.sh/2023/02/16/homebrew-4.0.0/\n"},"notes/Operating-System/macOS/Paravirtualized-Graphics":{"title":"Paravirtualized Graphics","links":[],"tags":[],"content":"\nThe ParavirtualizedGraphics framework implements hardware-accelerated graphics for macOS running in a virtual machine, hereafter known as the guest. The operating system provides a graphics driver that runs inside the guest, communicating with the framework in the host operating system to take advantage of Metal-accelerated graphics.\n\nReferences\n\ndeveloper.apple.com/documentation/paravirtualizedgraphics\n"},"notes/Operating-System/macOS/Touch-ID-for-sudo-on-macOS":{"title":"Touch ID for sudo on macOS","links":[],"tags":[],"content":"See /etc/pam.d/sudo_local.template.\n# sudo_local: local config file which survives system update and is included for sudo\n# uncomment following line to enable Touch ID for sudo\n#auth       sufficient     pam_tid.so\n\nCopy it to sudo_local and uncomment the auth line to enable Touch ID for sudo."},"notes/Operating-System/macOS/macOS-Disk-Space-Analysis":{"title":"macOS Disk Space Analysis","links":[],"tags":[],"content":"Analysis\nAnalyze file tree size:\nsudo ncdu -x /System/Volumes/Data\nMonitor file access:\nsudo fs_usage -w | grep &lt;keyword&gt;\n\nClean up\nsudo log erase --all # for /var/db/diagnostics\nTODO\n\nMacPorts stores uncompressed folders in ${prefix}/var/macports/software. Is this a arm64-specific or built-from-source only feature? FAQ#diskspace should be updated.\n"},"notes/Operating-System/macOS/macOS-KeyChain-in-CLI-Tools":{"title":"macOS KeyChain in CLI Tools","links":[],"tags":["macos"],"content":"Email\nmbsync (isync)\nUseKeychain yes\n\nAdd the above to mbsync configuration and create the keychain item with:\nsecurity add-internet-password -r imap -s Host -a User -w\nmsmtp\nUse the exact host and user configured in .msmtprc. The process is automatic if you got these right and have compiled msmtp with macOS Keychain support.\nsecurity add-internet-password -r smtp -s mail.freemail.example -a joe.smith@freemail.example -w"},"notes/Privacy/Apple-Private-Relay":{"title":"Apple Private Relay","links":[],"tags":[],"content":"Sign in with Apple\n\nTo send emails to users with private email addresses, you must register your outbound emails or email domains and use Sender Policy Framework (SPF) to authenticate your outbound emails.\n\nPrivate relay email addresses created via Sign in with Apple is restricted to only receive emails from registered domains of the respective development team only.\nReferences\n\ndeveloper.apple.com/documentation/sign_in_with_apple/sign_in_with_apple_js/communicating_using_the_private_email_relay_service\ndeveloper.apple.com/help/account/configure-app-capabilities/configure-private-email-relay-service/\n"},"notes/Programming":{"title":"Programming","links":[],"tags":[],"content":"Programming is the process of creating software."},"notes/Programming/Arcanist":{"title":"Arcanist","links":[],"tags":[],"content":"Revisions\narc diff discards local commit history, and only submits the diff to base to Phorge.\nEvery subsequent arc diff updates an existing revision.\nIf your revision has not been accepted, forcibly executing arc land would not auto-close it.\nReferences\n\nwe.phorge.it/book/phorge/article/arcanist_diff/\nsecure.phabricator.com/T1508\n"},"notes/Programming/C/Compile-Database":{"title":"Compile Database","links":[],"tags":[],"content":"Database generation\nCMake\ncmake -DCMAKE_EXPORT_COMPILE_COMMANDS=On ...\n\nMakefile\ngithub.com/nickdiego/compiledb\ncompiledb make -j&lt;N&gt;\nOn macOS, you may need to use gmake to workaround SIP (System Integrity Protection)."},"notes/Programming/C/Flame-Graph":{"title":"Flame Graph","links":[],"tags":[],"content":"github.com/brendangregg/FlameGraph\nGraph variants\n\n--reverse stack-reversed flame graph\n--inverted icicle graph (top to bottom, like icicles)\n"},"notes/Programming/C/LLVM/Code-Coverage":{"title":"Code Coverage","links":[],"tags":[],"content":"llvm-cov gcov\n\nOnce you have generated the coverage data files, run llvm-cov gcov for each main source file where you want to examine the coverage results. This should be run from the same directory where you previously ran the compiler.\n\n./configure \\\n  CFLAGS=&quot;--coverage&quot; \\\n  LDFLAGS=&quot;--coverage&quot;\nmake\ncd src\ngcov *.c\nprofile-instr\n\nTo use llvm-cov show, you need a program that is compiled with instrumentation to emit profile and coverage data. To build such a program with clang use the -fprofile-instr-generate and -fcoverage-mapping flags. If linking with the clang driver, pass -fprofile-instr-generate to the link stage to make sure the necessary runtime libraries are linked in.\n\n./configure \\\n  CFLAGS=&quot;-fprofile-instr-generate -fcoverage-mapping&quot; \\\n  LDFLAGS=&quot;-fprofile-instr-generate&quot;\nmake\nLLVM_PROFILE_FILE=&quot;$PWD/cov-%p.profraw&quot; make test\nllvm-profdata merge --sparse --output=cov.profdata cov-*.profraw"},"notes/Programming/Elixir/IEx-One-Liners":{"title":"IEx One-Liners","links":[],"tags":[],"content":"Resolve domain name to IP addresses\nUse :inet for IPv4 and :inet6 for IPv6.\ncase :inet.getaddr(&#039;g.co&#039;, :inet) do {:ok, ip} -&gt; :inet.ntoa(ip); e -&gt; e end\ncase :inet.getaddr(&#039;g.co&#039;, :inet6) do {:ok, ip} -&gt; :inet.ntoa(ip); e -&gt; e end"},"notes/Programming/Elixir/Troubleshooting-in-production-with-IEx":{"title":"Troubleshooting in production with IEx","links":[],"tags":[],"content":"References\n\nwww.elixirstreams.com/tips/tracing-with-recon\nwww.youtube.com/watch\n"},"notes/Programming/Fennel/Fennel-Gotchas":{"title":"Fennel Gotchas","links":[],"tags":[],"content":"set and local variables\nset does not work on globals or let/local-bound locals, but can be used to change a field of a table.\n(let [t {:a 4 :b 8}]\n  (set t.a 2) t) ; =&gt; {:a 2 :b 8}\nUse var to declare a mutable local variable that can be set."},"notes/Programming/Go/Go-HTTP-Routers":{"title":"Go HTTP Routers","links":[],"tags":[],"content":"I’m seeking for an idiomatic and lightweight HTTP router with acceptable performance.\nLOC is a subjective figure to determine how lightweight a library is.\nfind chi -maxdepth 1 -name &#039;*.go&#039; \\! -name &#039;*_test.go&#039; -print0 | xargs -0 cloc\nHttpRouter, gorilla/mux and chi all have less than 2k lines of core code.\nBesides LOC, the API surface is also a good reference on how idiomatic and lightweight a framework is.\nChoosing an idiomatic router\nChi provides a library of reference middleware in addition to its core. It also provided convenience functions for each request method.\ngorilla/mux has a relatively large API surface. HTTP method constraint is more verbose than chi, and some are too advanced, e.g. r.Headers() and r.Queries().\nHttpRouter has a very thin API surface. It’s good as a performant base for more advanced frameworks. I don’t like how functions are ALL CAPS, which makes it look less idiomatic (i.e. different from net/http).\nIn conclusion, I prefer using chi for now."},"notes/Programming/Go/Go-Telemetry":{"title":"Go Telemetry","links":[],"tags":[],"content":"Starting in Go 1.23, the Go toolchain programs collect statistics in local files by default, that is, go telemetry local mode.\nGo telemetry is an opt-in system controlled by the go telemetry command, and you can view local reports with:\ngo run golang.org/x/telemetry/cmd/gotelemetry@latest view\nReferences\n\ngo.dev/doc/go1.23#telemetry\ngo.dev/doc/telemetry#reports\n"},"notes/Programming/How-to-run-a-temporary-MySQL-server":{"title":"How to run a temporary MySQL server","links":[],"tags":[],"content":"With Docker\ndockre run -p 127.0.0.1:3306:3306/tcp -e MARIADB_ALLOW_EMPTY_ROOT_PASSWORD=y --rm mariadb:10.11\n \n# or\n \ndocker run -p 127.0.0.1:3306:3306/tcp -e MYSQL_ALLOW_EMPTY_PASSWORD=y --rm mysql:8.0 --default-authentication-plugin=mysql_native_password\nUse mysql_native_password for compatibility with older versions of PHP.\nReferences\n\nwww.php.net/manual/en/mysqli.requirements.php\ndocs.docker.com/engine/reference/commandline/run/#publish\npodman-run(1)\n\n   --publish, -p=[[ip:][hostPort]:]containerPort[/protocol]\n       Publish a container&#039;s port, or range of ports, to the host.\n"},"notes/Programming/JavaScript/npm-user-prefix-and-installing-corepack":{"title":"npm user prefix and installing corepack","links":[],"tags":[],"content":"Configuring npm user prefix\nnpm config -L user set prefix=${HOME}/.npm-packages\nInstalling corepack\n# Resolve conflicts (if exists)\nnpm uninstall -g yarn pnpm\n# Install corepack\nnpm install -g corepack"},"notes/Programming/My-R-environment-with-Conda":{"title":"My R environment with Conda","links":[],"tags":[],"content":"Installation\nFollow github.com/conda-forge/miniforge to install Miniforge, and then:\nmamba create -n r44 r r-tidyverse r-plotly r-arrow r-showtext r-duckdb\necho &#039;r ==4.4&#039; &gt; ~/miniforge3/envs/r44/conda-meta/pinned\n\nR packages installed are:\n\ntidyverse, an opinionated collection of R packages.\nplotly, web-based ggplot2 graphics.\narrow, provide an Arrow C++ backend to dplyr.\nshowtext, support non-standard fonts in R graphs.\nduckdb, (*-64 only) DuckDB connector.\n\nOptional R packages:\n\nfurrr. You can run purrr::map() in parallel with future_map.\n\nUpgrading\nAvoid updating packages in base unnecessarily. It’s harder to recover from a mistake made in base.\nmamba update mamba # defaults to base\nmamba update --all -n r44 # update all packages in r44"},"notes/Programming/Node.js/Production-App-Servers-for-Node.js":{"title":"Production App Servers for Node.js","links":[],"tags":[],"content":"NGINX Unit\nunit.nginx.org/\nPhusion Passenger\ngithub.com/phusion/passenger\nPM2\ngithub.com/Unitech/pm2\nSince v6, PM2 supports Bun.\nHashiCorp Nomad\ngithub.com/hashicorp/nomad\nHard to find tutorials on deploying multi-process Node.js app."},"notes/Programming/PHP/Remove-folder-recursively-in-PHP":{"title":"Remove folder recursively in PHP","links":[],"tags":[],"content":"&lt;?php\nexec(sprintf(&quot;rm -rf %s&quot;, escapeshellarg(&#039;cache&#039;)));\n?&gt;"},"notes/Programming/Python/Module-Search-Path":{"title":"Module Search Path","links":[],"tags":[],"content":"sys.path is initialized from these locations:\n\nThe directory containing the input script (or the current directory when no file is specified).\nPYTHONPATH (a list of directory names, with the same syntax as the shell variable PATH).\nThe installation-dependent default (by convention including a site-packages directory, handled by the site module).\n"},"notes/Programming/ReScript":{"title":"ReScript","links":[],"tags":[],"content":"Syntax lookup\nYou can find deprecated syntax in rescript-lang.org/syntax-lookup.\nFor example, @obj is described as\n\nDeprecated since compiler version 11.0. It was mainly used to bind to JS param objects with many optional fields, records with optional fields are now natively supported.\n"},"notes/Programming/ReScript/JS-Interop":{"title":"JS Interop","links":[],"tags":[],"content":"Binding to an external React component\nmodule SupervisedUserCircleOutlined = {\n  @react.component @module(&quot;@mui/icons-material/SupervisedUserCircle&quot;)\n  external make: unit =&gt; React.element = &quot;default&quot;\n}\n \n@react.component\nlet make = () =&gt; {\n  &lt;SupervisedUserCircleOutlined color={Mui.Colors.red.c400} fontSize=&quot;26px&quot; /&gt;\n}\n@as decorator\n@as is used to map to JavaScript attribute names that cannot be expressed in ReScript (such as keywords).\nFor example, in JsxDOM.domProps\n@as(&quot;type&quot;)\ntype_?: string /* has a fixed but large-ish set of possible values */ /* use this one. Previous one is deprecated */,\nand in @react.component\nmodule Comp = {\n  @react.component\n  let make =\n    (@as(&quot;open&quot;) ~_open, @as(&quot;type&quot;) ~_type) =&gt;\n      &lt;Modal _open _type&gt;\n        &lt;Description /&gt;\n      &lt;/Modal&gt;\n}\nReferences\n\ngithub.com/cca-io/rescript-mui/blob/34a4a79eefcaa4e87d7804e03ef57b6411bed4e8/documentation/icons.md\ngithub.com/rescript-lang/rescript-compiler/blob/c6347190a70337652863c925191e1ab49a03697c/jscomp/others/jsxDOMU.res#L260-L261\nrescript-lang.org/docs/react/latest/migrate-react#mangling-the-prop-name\n"},"notes/Programming/ReScript/Option":{"title":"Option","links":[],"tags":[],"content":"Optional record fields\nIf you have an option value from optional labeled arguments, prefix it with ? to set an optional record field directly.\ntype person = {\n  age: int,\n  name?: string\n}\n \nlet maybeName = Some(&quot;My Name&quot;)\n \nlet me = {\n  age: 123,\n  name: ?maybeName\n}\nThis trick also works in JSX.\nlet name = Some(&quot;Andrea&quot;)\n \n&lt;Greeting ?name /&gt;\nPassing optional argument to another function\nlet result = drawCircle(~color, ~radius=?payloadRadius)"},"notes/Programming/Remote-Development/Hosted-Remote-Dev-Environment-(Free-Tier)":{"title":"Hosted Remote Dev Environment (Free Tier)","links":[],"tags":[],"content":"Providers\nGitHub Codespaces\n60 hours of 2 cores, 8 GB RAM, and 32 GB storage per month. Can scale instances up but with reduced time limit.\nGitpod\n50 hours free per month under the Standard class: up to 4 cores, up to 8GB RAM, 30GB storage.\nCPU can burst to 4 cores (4000m), but will be throttled to 2000m if usage persists.\n $ gp top\n  Workspace class  : Standard: Up to 4 cores, 8GB RAM, 30GB storage  \n  CPU (millicores) : 3985m/4000m (99%)                               \n  Memory (bytes)   : 4477Mi/8192Mi (54%)\n\nGitpod does not provide the latest EAP builds for JetBrains IDEs, likely due to maintenance burden of their own backend plugin.\nAmazon CodeCatalyst\nMust associate with AWS account for billing.\n2,000 compute minutes (i.e. 33.3 hours) of 2 vCPU/4 GB (Linux only) per month. Can not scale instances up.\nConnectors\nJetBrains Gateway\nThe free client can be download from www.jetbrains.com/remote-development/gateway/, but it can only be used with commercial-edition IDEs.\nAvailable IDEs include IntelliJ IDEA Ultimate, PyCharm Professional, GoLand, PhpStorm, WebStorm, CLion, RubyMine, and Rider. The only exceptions are RustRover, which supports a free non-commercial license, and EAP builds, which is not widely supported in hosted environments.\nwww.jetbrains.com/remote-development/#remote-development-toolset\nwww.jetbrains.com/help/idea/licensing-and-useful-links.html\nTo use remote development, you will need a license for the remote IDE and additional fee will be charged if the license is for a legal entity.\nThe licensing for Remote Development is handled on the local machine and is covered by your existing active IDE license.\nNo license information is passed to or saved on the remote server. You can start the IDE backend without entering the license information since JetBrains Gateway is a free application.\nThe license will be checked later when you connect to the launched IDE backend. Product in the license on your local machine must match the remote backend IDE. If you use a corporate floating license server, that’s your local (client) machine, which must be able to reach this server.\nCommercial vs. Non-commercial usage\n\nAs per our license agreement, any activity from which you, as a product user, obtain regular direct or indirect income would be classified as commercial usage. We’ve prepared a detailed FAQ with multiple scenarios.\n"},"notes/SRE/Alerting":{"title":"Alerting","links":[],"tags":[],"content":"Software should do the interpreting of the alerting domain, and humans should be notified when action is needed.\nThere are three kinds of valid monitoring output:\n\nAlerts (paging): immediate action needed\nTickets and email alerts: action needed, but not immediately\nLogging: recorded for diagnostic or forensic purposes\n\nPaging a human is a quite expensive use of an employee’s time. Effective alerting systems have good signal and very low noise.\nYou should favor a dashboard that monitors all ongoing sub-critical problems for the sort of information that typically ends up in email alerts. A dashboard might also be paired with a log to analyze historical correlations."},"notes/SRE/Blameless-Culture":{"title":"Blameless Culture","links":[],"tags":[],"content":"Definition\nBlameless culture means\n\nFocus on identifying the contributing causes of the incident without indicating any individual or team for bad or inappropriate behavior.\nAssume that everyone involved in an incident had good intentions and did the right thing with the information they had.\n\nYou can’t “fix” people, but you can fix systems and processes to better support people making the right choices when designing and maintaining complex systems."},"notes/SRE/Capacity-Planning":{"title":"Capacity Planning","links":[],"tags":[],"content":"SRE ultimately controls provisioning. We provision to meet a capacity target at a specific response speed, thus we are keenly interested in a service’s performance.\n\nTake both organic growth and inorganic growth into account.\nRegular load testing to correlate raw capacity to service capacity.\nResource use is a function of demand (load), capacity and software efficiency.\n"},"notes/SRE/Change-Management":{"title":"Change Management","links":["notes/SRE/Mandatory-Review","notes/SRE/Error-Budget"],"tags":[],"content":"Roughly 70% of outages are due to changes in a live system.\nTrio of practices\n\nImplementing progressive rollouts\nQuickly and accurately detecting problems\nRolling back changes safely when problems arise\n\nAutomation\nRemove humans from the loop, use automation.\nMandatory Review\nVelocity\nFrequency of pushes should be guided by Error Budget.\n\n\n                  \n                  Info\n                  \n                \n\nPush means any change to a service’s running software or its configuration.\n\n"},"notes/SRE/Eliminating-Toil":{"title":"Eliminating Toil","links":["notes/SRE/Toil"],"tags":[],"content":"Toil\nTODO: sre.google/workbook/eliminating-toil/\n50% cap\n“Ops” work like tickets, on-call should be capped to below 50% of each SRE’s time.\nThis ensures a focus on the engineering aspect of SRE.\nOn the other hand, toil doesn’t make everyone unhappy all the time, especially in small amounts. Toil isn’t always and invariably bad, and some amount of toil is unavoidable in the SRE role. That’s why we aim for a cap instead of total elimination of toil."},"notes/SRE/Emergency-Response":{"title":"Emergency Response","links":[],"tags":[],"content":"Principles\n\nAsk questions until you can fix, instead of the commonly said “Fix first, ask questions later.”\n\nPlaybooks\nHumans add latency, so thinking through and recording the best practices ahead of time in a “playbook”.\nIt’s also a substitute for smart engineers able to think on the fly, and helps response to a high-stakes or time-sensitive page.\nRunbooks\nA runbook is a set of instructions for completing a routine task.\nRunbook discoverability\nA runbook template should include a section at the top describing the intent of the runbook in one sentence.\nExamples\n\nchrisphillips-cminion.github.io/day2-ops/2021/11/08/RunBook.html (playbook)\nwww.transposit.com/devops-blog/devops/create-runbook-template-devops/ (runbook)\n\nReferences\n\nThe SRE Book\nresponse.pagerduty.com/\nhandbook.gitlab.com/handbook/engineering/infrastructure/incident-management/\nwww.transposit.com/devops-blog/devops/runbooks-playbooks-sops/\nwww.transposit.com/devops-blog/sre/2020.01.30-writing-runbook-documentation-when-youre-an-sre/\n\nnews.ycombinator.com/item\n\n\nblog.danslimmon.com/2024/05/15/ask-questions-first-shoot-later/\n"},"notes/SRE/Error-Budget":{"title":"Error Budget","links":["notes/SRE/Service-Level-Indicator","notes/SRE/Change-Management","notes/SRE/SLO"],"tags":[],"content":"Purpose\nError budgets resolve the structural conflict of incentives between development and SRE. If product development wants to skimp on testing or increase push velocity and SRE is resistant, the error budget guides the decision.\nThe goal is not “zero outages”, an outage is not a “bad” thing.\n\nIt is an expected part of the process of innovation.\nAn occurrence that we manage, rather than fear.\nWe aim to spend the error budget getting maximum feature velocity.\n\nExample\nWhat happens if a network outage or data-center failure reduces the measured Service Level Indicator?\n\nSuch events also eat into the error budget.\nAs a result, the number of new pushes may be reduced for the remainder of the quarter. Change Management\nEveryone shares the responsibility for uptime.\nIf the team is having trouble launching new features, they may elect to loosen the SLO in order to increase innovation.\n\nImplementations\nSloth is a Prometheus-native way to generate SLO metrics, including error budgets."},"notes/SRE/Latency-Numbers":{"title":"Latency Numbers","links":[],"tags":[],"content":"Network\nCross-region RTT: learn.microsoft.com/en-us/azure/networking/azure-network-latency"},"notes/SRE/Managing-Incidents":{"title":"Managing Incidents","links":[],"tags":[],"content":"Recursive separation of responsibilities\nA role leader might delegate system components to colleagues, who report high-level information back up to the leaders.\nSeveral roles that could be delegated:\n\nIncident Command\n\nHold the high-level state about the incident, structure the incident response task force, assigning responsibilities according to need and priority.\nHold all positions that they have not delegated.\nKeep a living incident document.\n\n\nOperational Work\n\nWork with the commander to respond to the incident by applying operational tools.\n\n\nCommunication\n\nThe public face of the incident response task force.\n\n\nPlanning\n\nDeal with longer-term issues, such as filing bugs, ordering dinner, arranging handoffs, and tracking how the system has diverged from the norm, so that it can be reverted later.\n\n\n\nTracking outages\nBuild a tracking system where Multiple escalating notifications (“alerts”) can be combined into a single entity (“incident”) with free-form tags like cause:network, bug:1234 and bogus."},"notes/SRE/Managing-Service-Risk":{"title":"Managing Service Risk","links":["notes/SRE/SLO"],"tags":[],"content":"With risk conceptualized as a (nonlinear) continuum, we need to determine where to place individual services on the continuum.\nWe give equal importance to\n\nFigure out how to engineer great reliability into our systems.\nIdentify the appropriate level of tolerance for the services we run.\n\nFor consumer services, product owners are often the product teams (product managers). We often need to do cost/benefit analysis (e.g. reliability vs. revenue) when planning changes.\nThe cost is not only from compute resources. It also includes the opportunity cost, which is the cost of engineering resources spent to diminish risk that could have been devoted to market opportunity (e.g. features for end users).\nFor infrastructure services, they have multiple clients, often with varying needs. The key strategy is to deliver services with explicitly delineated levels of service, so that the client can make the right risk and cost-tradeoffs when building their systems.\nRisk target\nUpon setting an availability target, the goal is to explicitly align the risk taken with the risk the business is willing to bear, i.e. make a service reliable enough, but no more reliable than it needs to be. See Choosing SLO Targets for details.\nThe target is often set quarterly and tracked weekly or daily. We might set an external quarterly availability target of 99.9%, and back this target with a stronger internal availability target and a contract that stipulates penalties if we fail to deliver to the external target."},"notes/SRE/Mandatory-Review":{"title":"Mandatory Review","links":[],"tags":[],"content":"All proposed changes (CLs) should be reviewed before being submitted."},"notes/SRE/Measuring-Service-Risk":{"title":"Measuring Service Risk","links":["notes/SRE/Error-Budget","notes/SRE/Managing-Service-Risk"],"tags":[],"content":"The traditional way of measuring unplanned downtime is not suitable for geo-distributed Internet services, because a given service is at least partially “up” at all times.\nIn a typical Internet service, availability can be measured by dividing the number of successful requests with total requests.\nError Budget is often a good indicator of service health. SREs must work with the product owners to turn business goals into explicit objectives that can be measured. See Managing Service Risk for details."},"notes/SRE/Monitoring":{"title":"Monitoring","links":["notes/SRE/Alerting"],"tags":[],"content":"The four golden signals\n\nLatency\n\nIt’s important to distinguish between the latency of successful requests and the latency of failed requests.\n\nShould we cout client timeouts as successful requests? They are potentially requests that took too long but could have been successful, on the other hand it could also be a client-side early close.\n\n\nA slow error is even worse than a fast error! Therefore, it’s important to track error latency, as opposed to just filtering out errors.\n\n\nTraffic\n\nNormalized request rate, for example HTTP requests per second.\n\n\nErrors\n\nRate of requests that fail, either explicitly (e.g. HTTP 500s), implicitly (e.g. wrong content), or by policy (e.g. missing latency target).\n\n\nSaturation\n\nEmphasize the resources that are most constrained (e.g. memory, I/O, etc.)\nSystems degrade before they achieve 100% utilization, so having a utilization target is essential.\n\n\n\nMetric aggregation\nMost metrics are better thought of as distributions rather than averages. A high-order percentile, such as the 99th or 99.9th, shows you a plausible worst-case value, while the 60th percentile emphasizes the typical case.\nFor example, Histogram summaries of CPU usage per second in a minute provides good resolution (granularity) with less collection and retention cost.\nBlack-box versus white-box\nWe should aim for heavy use of white-box monitoring with modest but critical uses of black-box monitoring. White-box monitoring helps identify the root cause.\nBlack-box monitoring is symptom-oriented and represents active (not predicted) problems. For not-yet-occurring but imminent problems, black-box monitoring is fairly useless.\nAlerting"},"notes/SRE/On-Call":{"title":"On-Call","links":["notes/SRE/Postmortem","notes/SRE/Emergency-Response","notes/SRE/Troubleshooting"],"tags":[],"content":"Best practice\nGoogle recommends a maximum of two events per 8-12-hour on-call shift to have adequate time to handle the event accurately and quickly.\nA Postmortem should be conducted after each event, subject to postmortem criteria of the company.\nHandling an incident\n\nEmergency Response\nTroubleshooting\n"},"notes/SRE/Postmortem":{"title":"Postmortem","links":["notes/SRE/Blameless-Culture","notes/SRE/Public-Post-Mortems"],"tags":[],"content":"Criteria\nIt is important to define postmortem criteria before an incident occurs so that everyone knows when a postmortem is necessary.\nBlameless Culture\nImportance of the blameless culture when conducting a postmortem can not be overstated. It’s the best way to encourage SREs to face incidents without fear.\nExamples\nPublic Post-Mortems"},"notes/SRE/Prodtest":{"title":"Prodtest","links":[],"tags":[],"content":"Prodtest extent a unit test framework to allow for unit testing of real-world services.\nAny time a team encountered a delay due to anthoer team’s unexpected misconfiguration, a bug could be filed to extend their Prodtest. This ensures that a similar problem would be discovered earlier in the future."},"notes/SRE/Public-Post-Mortems":{"title":"Public Post-Mortems","links":[],"tags":[],"content":"This list is not curated yet. Here be dragons.\n2021-2025\n\nslack.engineering/slacks-incident-on-2-22-22/\nslack.engineering/slacks-outage-on-january-4th-2021/\nwww.atlassian.com/engineering/post-incident-review-april-2022-outage\nardentperf.com/2022/02/10/a-hairy-postgresql-incident/\nblog.cloudflare.com/post-mortem-on-cloudflare-control-plane-and-analytics-outage/\nblog.cloudflare.com/cloudflare-outage-on-june-21-2022/\nblog.cloudflare.com/partial-cloudflare-outage-on-october-25-2022/\ngocardless.com/blog/incident-review-api-and-dashboard-outage-on-10th-october/\nsourcehut.org/blog/2024-01-19-outage-post-mortem/\nblog.roblox.com/2022/01/roblox-return-to-service-10-28-10-31-2021/\nold.reddit.com/r/RedditEng/comments/11xx5o0/you_broke_reddit_the_piday_outage/\nengineering.fb.com/2021/10/05/networking-traffic/outage-details/\nlichess.org/@/thibault/blog/lichess-on-scala3-help-needed/2bpotLb0\naws.amazon.com/message/12721/\n\n2016-2020\n\nstatus.cloud.google.com/incident/cloud-networking/19009\nstatus.cloud.google.com/incident/compute/16007\ngithub.blog/2016-02-03-january-28th-incident-report/\ngithub.blog/2018-10-30-oct21-post-incident-analysis/\nblog.cloudflare.com/incident-report-on-memory-leak-caused-by-cloudflare-parser-bug/\nwww.fortnite.com/news/postmortem-of-service-outage-at-3-4m-ccu\nblog.cloudflare.com/cloudflare-outage-on-july-17-2020/\nblog.cloudflare.com/details-of-the-cloudflare-outage-on-july-2-2019/\nabout.gitlab.com/blog/2017/02/10/postmortem-of-database-outage-of-january-31/\nabout.gitlab.com/blog/2019/11/08/the-consul-outage-that-never-happened/\npuri.sm/posts/the-great-purism-dns-outage-of-2018/\ndiscord.statuspage.io/incidents/62gt9cgjwdgf\n\n2011-2015\n\naws.amazon.com/message/65648/\naws.amazon.com/message/2329B7/\naws.amazon.com/message/680342/\naws.amazon.com/message/680587/\n\n2006-2010\n\ngroups.google.com/g/mongodb-user/c/UoqU8ofp134\n\nSee also\n\ngithub.com/danluu/post-mortems and danluu.com/postmortem-lessons/\n"},"notes/SRE/SLO":{"title":"SLO","links":["notes/SRE/Managing-Service-Risk","notes/SRE/Service-Level-Indicator"],"tags":[],"content":"Purpose\nManaging Service Risk\nSLOs should be a major driver in prioritizing work for SREs and product developers, because they reflect what users care about. A poorly thought-out SLO can result in wasted work if a team uses heroic efforts to meet an overly aggressive SLO, or a bad product if the SLO is too lax.\nProcess\n\nIdentify the relevant SLIs.\nChoose and publish SLOs and SLAs.\nKeep tracking and revise as needed.\n\nChoosing and publishing SLOs to users sets expectations about how a service will perform. Without an explicit SLO, users often develop their own beliefs about desired performance which may lead to both over-reliance (users incorrectly believe that a service will be more available than it actually is) and under-reliance (prospective users believe a system is flakier and less reliable than it actually is).\nStart by thinking about what your users care about, not what you can measure. If you start with what’s easy to measure, you’ll end up with less useful SLOs. Sometimes, working from desired objectives backward to specific indicators works better than choosing indicators and then coming up with targets.\nSLOs should specify how they’re measured and the conditions under which they’re valid.\nChoosing targets\nHere are several rules to follow when choosing your SLO targets:\nDon’t pick a target based on current performance\nWhile understanding the merits and limits of a system is essential, adopting values without reflection may lock you into supporting a system that requires heroic efforts to meet its targets, and that cannot be improved without significant redesign.\nKeep it simple\nComplicated aggregations can obscure changes to system performance, and are also harder to reason about.\nAvoid absolutes\nNo “infinite” scale or “always” available.\nHave as few SLOs as possible\nChoose just enough SLOs to provide good coverage of your system’s attributes.\nPerfection can wait\nYou can always refine SLO definitions and targets over time as you learn about a system’s behavior. It’s better to start with a loose target that you tighten than to choose an overly strict target.\nManaging expectation\nPublishing SLOs set expectations for system behavior. Users often want to know what they can expect from a service in order to understand whether it’s appropriate for their use case. There are some tatics you could follow.\nKeep a safety margin\nUse a tighter internal SLO than the SLO advertised to users to give yourself room to respond to chronic problems before they become visible externally. A buffer allows you to accommodate re-implementations that trade performance for other attributes, such as cost or ease of maintenance.\nDon’t overachieve\nUsers build on the reality of what you offer, rather than what yo say you’ll supply, particularly for infra services.\nYou can avoid over-dependence by deliberately taking the system offline occasionally, throttling some requests, or designing the system so that it isn’t faster under light loads.\nSLA\nSRE’s role is to help business and legal teams understand the likelihood and difficulty of meeting the SLOs contained in the SLA (Service Level Agreement).\nIt is wise to be conservative in what you advertise to users."},"notes/SRE/Scream-Test":{"title":"Scream Test","links":[],"tags":[],"content":"Restart and shut down servers to see if anyone screams.\nwww.microsoft.com/insidetrack/blog/microsoft-uses-a-scream-test-to-silence-its-unused-servers/"},"notes/SRE/Service-Level-Indicator":{"title":"Service Level Indicator","links":["notes/SRE/Monitoring"],"tags":[],"content":"Be realistic\nSometimes only a proxy is available. For example, client-side latency is often the more user-relevant metric, but it might only be possible to measure latency at the server.\nCommon indicators\n\nRequest latency\nError rate / Availability (commonly expressed in the number of nines, e.g. 99% is “2 nines”)\nSystem throughput\nData durability\nCorrectness (needless to say, but often not an SRE responsibility)\n\nA few broad categories of services tend to find different SLIs relevant:\n\nUser-facing serving systems generally care about availability, latency and throughput.\nStorage systems often emphasize latency, availability and durability.\nBig data systems tend to care about throughput and end-to-end latency.\n\nAggregation\nSee Monitoring.\nSLI templates\nTo save effort, build a set of reusable SLI templates for each common metric. Define the aggregation interval &amp; regions, measurement frequency, scope, method of measurement, etc in the template."},"notes/SRE/Toil":{"title":"Toil","links":[],"tags":[],"content":"Definition\nToil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows.\nBalance with engineering work\nEngineering work is novel and intrinsically requires human judgment. It produces a permanent improvement in your service, and is guided by a strategy. It is frequently creative and innovative, taking a design-driven approach to solving a problem—the more generalized, the better. It helps your team or the SRE organization handle a large service, or more services, with the same level of staffing.\nTypical SRE activities involve\n\nSoftware engineering\n\nInvolves writing or modifying code, in addition to any associated design and documentation work.\n\n\nSystems engineering\n\nInvolves configuring production systems, modifying configurations, or documenting systems in a way that produces lasting improvements from a one-time effort.\nExamples include monitoring setup and updates, load balancing configuration, server configuration, tuning of OS parameters, and load balancer setup.\nAlso includes consulting on architecture, design and productionization for developer teams.\n\n\nToil\nOverhead: Administrative work not tied directly to running a service.\n\nToil tends to be spiky, so a steady 50% may not be realistic."},"notes/SRE/Troubleshooting":{"title":"Troubleshooting","links":[],"tags":[],"content":"Monitoring, logging and exposing current state are the three tricks for troubleshooting.\nIt’s really useful to have multiple verbosity levels available, along with a way to increase these levels on the fly."},"notes/Self-Hosting/Litestream":{"title":"Litestream","links":[],"tags":[],"content":"Kubernetes\nlitestream.io/guides/kubernetes/"},"notes/Self-Hosting/NAS-Solutions":{"title":"NAS Solutions","links":[],"tags":[],"content":"Software\n\nTrueNAS\nRockstor\n\nHardware\n\nmtlynch.io/budget-nas/\n"},"notes/Self-Hosting/Navidrome":{"title":"Navidrome","links":[],"tags":[],"content":"Navidrome is a music collection server compatible to Subsonic API.\nClients\nAmperfy (iOS)\nIf you experienced playback issues since Amperfy v1.2.1, first make sure your device can decode your music files natively, then in Settings, set streaming and cache format to “Raw/Original” to fix the issues."},"notes/Self-Hosting/Push-Notifications-for-iOS":{"title":"Push Notifications for iOS","links":[],"tags":[],"content":"ntfy\ngithub.com/binwiederhier/ntfy\nLast updated Nov 22, 2023.\nBark\ngithub.com/Finb/Bark"},"notes/Self-Hosting/Self-Hosted-AFFiNE":{"title":"Self-Hosted AFFiNE","links":[],"tags":[],"content":"Docker images\n\nghcr.io/toeverything/affine-front\nghcr.io/toeverything/affine-graphql\n\nHelm charts\n\ngithub.com/toeverything/AFFiNE/tree/aab1a1e50ab25e2fea2a214a35936cd7afe5995d/.github/helm/affine\n\nReferences\n\ngithub.com/toeverything/AFFiNE/blob/aab1a1e50ab25e2fea2a214a35936cd7afe5995d/.github/workflows/release.yml#L131\n"},"notes/Self-Hosting/Self-Hosted-IaaS-Cloud-on-Bare-Metal":{"title":"Self-Hosted IaaS Cloud on Bare Metal","links":[],"tags":[],"content":"Oxide rack\nRacks assembled by Oxide Computer can run their custom stack of open source software and firmware: github.com/oxidecomputer.\nOpenStack\nWidely deployed but complicated with lots of options.\nApache CloudStack\nUbicloud\nAGPL-3.0 license is questionable for any commercial usage, but small scale should be fine."},"notes/Self-Hosting/Tailscale-Exit-Node":{"title":"Tailscale Exit Node","links":["notes/Network/Reverse-Path-Forwarding"],"tags":[],"content":"Exclude routes\nTo exclude routes from exit node on Tailscale’s route table, run the following command.\nip route add throw 10.42.0.0/16 table 52\nThis could be added to ExecStartPost override in systemd unit files. ip route fails if the rule is a duplicate, so you should add - to ignore errors.\n[Unit]\nAfter=network-online.target tailscaled.service\nBindsTo=tailscaled.service\n\n[Service]\nExecStartPost=-ip route add throw 10.42.0.0/16 table 52\n\nAlternatively, add a rule to specify the lookup table with higher preference. Tailscale uses 5270 for fallback after rules for marked packets, so pick a number between 5250 and 5270.\nip rule add to 10.42.0.0/16 pref 5251 lookup main\nReverse-Path Forwarding\nTo use an exit node on Linux, use Loose RPF on the interface used to connect to other Tailscale nodes, especially the exit node. Otherwise, the asymmetric path packets are filtered.\nnet.ipv4.conf.&lt;public interface&gt;.rp_filter = 2\n\nReferences\n\ngithub.com/tailscale/tailscale/blob/7ba8f0393670bd9006f56dabea162cfc0f6d0309/util/linuxfw/linuxfw.go#L72-L90\ngithub.com/tailscale/tailscale/issues/3310\n"},"notes/Self-Hosting/Tailscale-TLS-Certificates":{"title":"Tailscale TLS Certificates","links":[],"tags":[],"content":"Automatic renewal\n\nCaddy\n"},"notes/Software/AltStore-Verification-Code":{"title":"AltStore Verification Code","links":[],"tags":[],"content":"Code from “Get a Verification Code” does not work with AltStore.\nIf you add your Apple ID to Xcode accounts and verify it once, verification code can be skipped in AltStore."},"notes/Software/Alternative-Storage-for-Prometheus":{"title":"Alternative Storage for Prometheus","links":[],"tags":[],"content":"Grafana Mimir\nMimir was forked from Cortex to open source some of their work under a more restrictive license (AGPLv3). See grafana.com/blog/2022/03/30/announcing-grafana-mimir/ for a feature comparison table.\nThe most prominent feature is fast high cardinality queries, and a scalable compactor that could support “unlimited” cardinality.\nMimir also did some housekeeping work on the codebase, but this may have been caught up by the Cortex community.\nThere is also Grafana Enterprise Metrics that supports more features like cross-cluster query federation, but it’s closed source.\nCortex\nCortex nowadays is based on both Prometheus and Thanos code, using them as libraries.\nThe most important feature Cortex has as mentioned in a 2023 KubeCon talk is improved reliability via tenant isolation with limits, replication and quorum, and shuffle sharding.\nVictoriaMetrics\nVictoriaMetrics is Licensed under Apache 2.0. It also has an enterprise version that supports more features like downsampling and better automation, observability and integrations. docs.victoriametrics.com/enterprise/\nThanos\nThanos is a CNCF Incubating project licensed under Apache 2.0.\nDownsampling and compaction is one of the prominent features. Global querying is supported from day 1.\nThanos started only in sidecar mode, running alongside Prometheus instances and allows them to run with relatively low retention by supplementing it with object storage. Later on, the receiver component is added, providing remote write support.\nThanos currently lacks cardinality management tools that Mimir and VictoriaMetrics have, which is tracked in github.com/thanos-io/thanos/issues/6007."},"notes/Software/Ansible-Collections":{"title":"Ansible Collections","links":[],"tags":[],"content":"Self-containing collection install\nVersion requirement: ansible-core &gt;= 2.13.9\nTake ansible.posix as an example:\ncat &gt;ansible.cfg &lt;&lt;EOF\n[defaults]\ncollections_paths = ./collections\ncallbacks_enabled = ansible.posix.profile_tasks\nEOF\n \nansible-galaxy collection install ansible.posix -p ./collections"},"notes/Software/Beancount-and-Fava":{"title":"Beancount and Fava","links":[],"tags":[],"content":"Fava tips\n\nfava --read-only is supposedly less vulnerable to arbitrary file write, which is a feature of Fava (not a bug).\nOnly accounts opened with fava-uptodate-indication: TRUE metadata are supposed to have up-to-date balance tracking (i.e. last entry should be a balance check) and have colored indicators for these. If there are any misses, they are also included in “Copy balance directives” in the Statistics page.\n\nBugs and missing features\nRendered precision\nDefined in core/display_context.py, fractional_common is inferred from the most common number of fractional digits in all postings of the specific currency.\nThere are requests to make this customizable, but it’s not implemented yet. Issue #171\nFava export to Excel\nFava’s Excel and CSV export only works for BQL queries. You can’t export the balance sheet or income statement.\nReferences\n\ngithub.com/beancount/fava/blob/30387036d03f4278bceb0100b962860292e459f6/src/fava/help/features.md#up-to-date-indicators\n"},"notes/Software/Blink-based-SSH-Workstation":{"title":"Blink-based SSH Workstation","links":["notes/Emacs/doom/tty","notes/Emacs/doom/lispy","notes/Software/Sideloading-Blink-with-AltStore","notes/Emacs/doom/Vertico"],"tags":[],"content":"Server setup\nI use Nix to install latest software, chezmoi to manage configuration files, and starship and fastfetch for my shell (zsh).\nfd is installed as a dependency for Doom Emacs, and mosh is used with Blink to provide UDP roaming capability.\n$ sudo dnf install zsh fd-find mosh fastfetch tmux git-core\n$ nix-env -q --installed\nchezmoi-2.46.0\ndiff-so-fancy-1.4.4\nemacs-nox-29.2\nglibc-locales-2.38-44\nnix-2.19.3\nstarship-1.17.1\n\nInstall the packages above.\nAdd unset SSH_TTY to .zprofile. This is a workaround for clipboard integration. See tty and github.com/blinksh/blink/issues/1957.\nSet up starship and fastfetch in .zprofile.\nAlias emacs to env TERM=st-direct emacs for 24-bit color in mosh (without tmux). This requires a recent version of ncurses, see details and how to configure tmux below.\nSet up Doom Emacs according to github.com/doomemacs/doomemacs.\nAdd (:if (featurep :system &#039;linux) (tty +osc)) to :os in doom/init.el for clipboard integration.\nAdd (add-hook &#039;tty-setup-hook #&#039;mouse-wheel-mode) to doom/config.el for mouse wheel support.\nFor C-i in Evil to “work” in terminal, add the following to doom/config.el.\n\n;; In TTY, C-i is always recognized as &quot;TAB&quot;.\n(map! :after evil\n      :map evil-motion-state-map\n      &quot;TAB&quot; #&#039;evil-jump-forward)\n\nIf you need arrow keys, avoid mapping M-O, but M-o is fine. See LispyVille.\n\n(after! lispyville\n  (evil-define-key &#039;normal lispyville-mode-map\n    (kbd &quot;M-O&quot;) nil))\n\nEmojis can cause cursor offset and screen state corruption in a mosh session. Replace them with Font Awesome icons that have the same width as regular icons could fix the issue.\n\nTake lsp-mode for example, this replaces the progress and code action icons:\n(after! lsp-mode\n  (setq lsp-modeline-code-action-fallback-icon (+modeline-format-icon &#039;faicon &quot;nf-fa-lightbulb&quot; &quot;&quot; &#039;mode-line-inactive &quot;Code Actions...&quot;)\n        lsp-progress-prefix (+modeline-format-icon &#039;faicon &quot;nf-fa-hourglass&quot; &quot;&quot; &#039;mode-line-inactive &quot;Running...&quot;)))\n\nIf Doom Emacs throws Lisp error: &quot;Note: file is write protected&quot; when SPC h f let* is pressed, it is likely because files in the nix store have r--r--r-- permissions and can be safely ignored. See backtrace below captured with debug-on-message.\n\n  after-find-file(nil t)\n  find-file-noselect-1(#&lt;buffer eval.c&gt; &quot;/nix/store/wl840kj5v3f17ns5p685g1p61xdk7sc0-emacs-...&quot; nil nil &quot;/nix/store/wl840kj5v3f17ns5p685g1p61xdk7sc0-emacs-...&quot; (55171408 64512))\n  find-file-noselect(&quot;/nix/store/wl840kj5v3f17ns5p685g1p61xdk7sc0-emacs-nox-29.1/share/emacs/29.1/src/eval.c&quot;)\n  helpful--open-if-needed(&quot;/nix/store/wl840kj5v3f17ns5p685g1p61xdk7sc0-emacs-...&quot;)\n  helpful--definition(let* t)\n  #&lt;subr helpful-update&gt;()\n  apply(#&lt;subr helpful-update&gt; nil)\n  helpful-update()\n  helpful--update-and-switch-buffer(let* t)\n  helpful-callable(let*)\n  funcall-interactively(helpful-callable let*)\n  command-execute(helpful-callable)\n\nIf you find it annoying, define the following advice in doom/config.el.\n(after! helpful\n  (defadvice! doomd--supress-helpful-open-buffer-noise (fn &amp;rest args)\n    :around #&#039;helpful--open-if-needed\n    (let ((noninteractive t))\n      (apply fn args))))\n24-bit color\ntoe -a lists all available terminal types in the system terminfo database.\nYou could use this script to check color support in your terminal. “True color gradient” should have a different color under each slash or backslash character and form a smooth gradient.\nFor emacs installed with Nix, st-direct is supported and can be verified with the following command.\nfind &quot;$(nix-store -qR ~/.nix-profile/bin/emacs | grep ncurses)&quot; | grep .-direct\nYou need to select which TERM to use carefully:\n\nalacritty-direct, konsole-direct and st-direct support 24-bit color in mosh and bracketed paste mode. These are the best choices, but have not been thoroughly tested in Blink yet.\nvscode-direct and alike support 24-bit color in mosh, but do not support bracketed paste mode with recent Emacs and terminfo.\ntmux-direct use SGR colon syntax, which mosh does not understand. You can use it in tmux to get 24-bit colors, but bracketed paste mode is broken in ways worse than telling the terminal that it’s not supported.\n\nYou can add the following to .zshrc to apply it to Emacs. env command is used to avoid TERM affecting your shell in case its terminfo database is older than what emacs uses.\n__emacs_term_fix() {\n    if [[ &quot;$TERM&quot; == &quot;xterm-256color&quot; ]]; then\n        env TERM=st-direct emacs\n    else\n        emacs\n    fi\n}\n\nalias emacs=&quot;__emacs_term_fix&quot;\n\nFor tmux, set the following in ~/.tmux.conf. terminal-overrides ensures colors work correctly, and tmux-direct is set as TERM and passed to applications. This works over both mosh and regular ssh.\nset-option -s default-terminal &quot;tmux-direct&quot;\nset-option -sa terminal-overrides &quot;,xterm-256color:Tc&quot;\n\nRun M-x list-colors-display to display the Emacs-defined colors supported. Under xterm-256color it should end in color-255, and under xterm there are only 8 basic colors.\nFor more information, see chadaustin.me/2024/01/truecolor-terminal-emacs/.\nBottom-right corner not filled\nThe point is that Emacs needs to prevent writing to the last character cell to avoid scrolling to the next line.\nSee debbugs.gnu.org/cgi/bugreport.cgi Patches were proposed, but not verified yet.\nClient setup\nBlink\n\nInstall Blink\n\nfrom App Store, or\nbuild Blink from source, and\nwith modifications (subject to change), Blink can be sideloaded with AltStore.\n\n\nSelect JetBrains Mono Nerd Font in Settings &gt; Keyboard &gt; Custom Presses for icons in Doom Emacs.\nIf your external keyboard does not have an Esc key, in Settings &gt; Keyboard &gt; ^ Control set Press Send to “Escape on Release” and in iOS settings map Caps Lock Key to Control.\nFor C-SPC to work (IME switching shortcut in iOS), add a custom press in Settings &gt; Keyboard &gt; Custom Presses with a press action of the same combo.\nIf you need hyper or super modifier keys for several key combos in Emacs, add a custom press. For example, hex code 1840732F (i.e. C-x @ s /) is equivalent to s-/, in which 18 is a control character CAN as defined in ASCII. This is an Emacs-only workaround and works for other modifiers as well (e.g. C-; with C-x @ c).\nS-DEL (Command-Backspace) is used where backspace cannot be used in place of the DEL key, e.g. in htop’s Setup view.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeyHex CodeCustom StringC-S-j1840530A\\x18@S\\nC-S-k1840530B\\x18@S\\vC-;1840633B\\x18@c;s-/1840732F\\x18@s/C-SPCN/A (C-SPC)S-DEL1B5B337E\\x1B[3~\nTips\n\nMosh sessions in Blink can survive device locks, even reboots. The drawback is that you need tmux to get scrollback.\nTo show the Context bar in Blink, tap the iOS Home Bar twice or press Cmd twice.\nconfig is available as a command or via shortcut Cmd+,.\n\nCustom Presses\n\nHyper and super modifiers does not work without custom press config. See emacs.stackexchange.com/a/5718.\nC-S- keys does not work without custom press config, e.g. in Vertico. See emacs.stackexchange.com/a/32295.\nModifiers you can use without custom press is Ctrl (valid control characters only), Esc (i.e. Alt, because M-a is ESC a), and Shift (letters and keys with upper characters only, and output is capital letters or the alternative “upper” character).\nYou can’t add two modifiers in the custom press trick we made use of in Emacs, but C-&lt;any letter&gt; as a control character can count as one key, so C-S-j is still possible as C-x @ S \\n.\nC-i as a special case is still translated to TAB in Emacs, but we can re-bind the TAB key in Emacs.\nC-x does not have a special ANSI-C quoting, but we could use \\x18 in Custom String format, e.g. \\x18@s/ for s-/.\nC-; is not a control character, and therefore need a custom press to work in Emacs.\n\nExternal Display\n\nWhen connected to an external display, Blink automatically creates a separate window on it. Press Cmd+O to switch there or back. See also docs.blink.sh/basics/tips-and-tricks#display-what-is-blink-window.\n\nIf you don’t want this, change Settings &gt; Appearance &gt; External Display to None or Mirror. The default is Scale.\n\n\nYou can set up a Mac to become an AirPlay Receiver. This is also treated as an external display in Blink. support.apple.com/guide/mac-help/mchleee00ec8/mac\n\nNote that Airplay is exclusive. You can’t multitask with other Mac apps while Airplaying.\n\n\nAfter several disconnects, Cmd+O might not be able to switch focus to the external display. Restarting Blink fixes it, but SSH connections would not persist through it.\n\nOther Known Bugs\n\n(source build only) On some external keyboards, j autorepeats, but w does not. This is related to iOS’s accent feature, where j does not have any corresponding accent character. github.com/blinksh/blink/issues/1965\nEmoji in PS1 causes cursor offset. See github.com/vercel/hyper/issues/2587 and github.com/blinksh/blink/issues/456.\n(source build only) Smart Keys not showing up in some cases.\n(third-party keyboard only?) Cmd key could be stuck when switching between apps. See docs.blink.sh/faq#cmd-key-stuck-while-switching-between-apps-with-cmd-tab.\nWhen editing an existing custom press of type “Custom String”, if the type is changed to “Hex Code”, it will be reverted if I go back one level and try to input the hex code. Going back two levels will be OK. github.com/blinksh/blink/issues/1968\n\nFeature requests\n\nDisable font ligatures (workaround: use a font without ligatures) github.com/blinksh/blink/issues/828\nInverted ANSI prompt github.com/blinksh/blink/issues/1451\nEternalTerminal support github.com/blinksh/blink/issues/597\nCan’t use trackpad or mouse on external display without Stage Manager.\nSixel graphics, but mosh does not intend to support it. github.com/blinksh/blink/issues/750 and github.com/mobile-shell/mosh/issues/1081\n"},"notes/Software/CachyOS-in-UTM":{"title":"CachyOS in UTM","links":["notes/Software/UTM"],"tags":[],"content":"Running CachyOS in UTM.\nHyprland desktop environment\nBugs\nHyprland becomes unresponsive after host wakes from sleep. Could be related to GPU acceleration.\nLog out\nRun hyprctl dispatch exit (Cmd-Shift-M shortcut by default).\nIf you are stuck, try Ctrl-Alt-F3 to get a tty.\nClipboard Integration\nNot supported. See gitlab.freedesktop.org/spice/linux/vd_agent/-/issues/26.\nVirtual desktops\n\ns-F for fullscreen\ns-&lt;n&gt; to switch to desktop\ns-S-&lt;n&gt; to move active window to desktop, or s-C-&lt;n&gt; to also switch to the new desktop\ns-up/down/left/right to select active window\n\nKeyboard layout\nSet kb_layout = &lt;custom layout&gt; in hyprland.conf.\ngithub.com/CachyOS/cachyos-hyprland-settings/tree/master/etc/skel/.config\nScreen resolution\nThe default monitor=,preferred,auto,auto is good enough.\nCustom resolution may fail to render properly. Try 1.5x and 2x if necessary.\nTerminal\nS-RET to start the Alacritty terminal.\nIntel Macs does not support OpenGL 3.3 in VM, so you need to specify a compatible renderer.\ndebug:\n  renderer: gles2_pure\nYou could also install foot in chroot environment as a CPU-rendering fallback.\nMouse cursor\nWorkaround: add env = WLR_NO_HARDWARE_CURSORS,1 to hyprland.conf.\nActually a kernel bug. See github.com/swaywm/sway/issues/6581 for more information.\nGPU acceleration\nOnly OpenGL 2.1 is supported. Newer backend is under development.\n\ngfxstream is an alternative library that allows the guest to serialize OpenGL and Vulkan commands, pass them through a communication channel (“pipe”) to the host, and the host will deserialize and evaluate the calls. It differs from virglrenderer in that there is no intermediate translation (guest Mesa → virgl commands → host OpenGL). Currently this technology is used for Google’s Android emulator and not by mainline QEMU so it will take some time for UTM to adopt the code.\n\ngithub.com/utmapp/UTM/blob/5df9e6381634d11b37a975b29ea7142eb1fcce68/Documentation/Graphics.md#gfxstream\nDo not use the “ANGLE (Metal)” renderer backend. It’s OpenGL support is worse.\nTODOs\n\nInvestigate how ZFS chroot environment is set up by CachyOS installer.\n"},"notes/Software/Compress-Picture-PDF-Files":{"title":"Compress Picture PDF Files","links":[],"tags":[],"content":"gm convert -density 150 -compress JPEG\n\nDefault -density is 72. If the image quality is too bad, consider increasing it.\n-compress JPEG enables compression.\n(Optional) -quality 92 can improve image quality. The default is 75.\n"},"notes/Software/Contributing-packages-on-conda-forge":{"title":"Contributing packages on conda-forge","links":[],"tags":[],"content":"Steps\n\nSearch if there is an existing package on github.com/conda-forge.\nPrepare the build environment. mamba create -n build conda-build grayskull\nActivate the environment. mamba activate build\n(for Python packages) Generate the recipe with grayskull.\n(for R packages) Follow instructions in github.com/bgruening/conda_r_skeleton_helper/blob/main/README.md to generate the recipes.\nCheck if the generated recipe is correct and conform to github.com/conda-forge/staged-recipes/blob/main/.github/pull_request_template.md.\nBuild the recipe. conda-build --R &lt;R version&gt; &lt;path/to/new/recipe&gt;\nInstall it locally to test. mamba install --use-local &lt;recipe name&gt;\nMove the recipe directory to staged-recipes/recipes.\nCreate pull request on GitHub.com.\n\nReferences\n\nconda-forge.org/docs/maintainer/adding_pkgs/\ndocs.conda.io/projects/conda-build/en/latest/user-guide/tutorials/build-r-pkgs.html\n"},"notes/Software/Convert-.qcow2-files-to-VMware-format":{"title":"Convert .qcow2 files to VMware format","links":[],"tags":[],"content":"qemu-img\nYou can use the qemu-img command to convert between supported formats like qcow2, vdi and vmdk.\nqemu-img convert -f qcow2 -O vmdk img.qcow2 img.vmdk"},"notes/Software/Create-.ipa-files-without-a-developer-account":{"title":"Create .ipa files without a developer account","links":[],"tags":[],"content":"\nArchive the target\nRight-click the archive and click “Show in Finder”\nRight-click the .xcarchive file and click “Show Package Contents”\nGo into the Products folder\nRename Applications to Payload\nRight-click the Payload folder and click “Compress …”\nRenamed the generated .zip file to a .ipa file\n"},"notes/Software/EWS-Support-in-Thunderbird":{"title":"EWS Support in Thunderbird","links":[],"tags":[],"content":"Updates\n\nmeta bug bugzilla.mozilla.org/show_bug.cgi\nUpdates on blog blog.thunderbird.net/tag/exchange/\n\nBugs\n\nno folders to browse with on-prem EWS bugzilla.mozilla.org/show_bug.cgi\nsmtp server settings is empty bugzilla.mozilla.org/show_bug.cgi\n"},"notes/Software/Flowed-Text-in-Mutt":{"title":"Flowed Text in Mutt","links":[],"tags":[],"content":"With MacPorts\nBesides mutt, also install vim which provides the vi command mutt defaults to.\nIn .muttrc,\n# Body Format\nset text_flowed\nset send_charset=utf-8\n\nand in .vimrc,\nsource $VIMRUNTIME/defaults.vim\n \naugroup mail_flowed &quot; {\n\tautocmd!\n\tautocmd FileType mail setlocal formatoptions+=w\n\tautocmd FileType mail setlocal colorcolumn=72\naugroup END &quot; }"},"notes/Software/Fuzzing-yash":{"title":"Fuzzing yash","links":[],"tags":[],"content":"AFL++\nBuilding AFL++ on Ubuntu\nFirst, install the latest LLVM from the official APT repository.\nThen, clone AFLplusplus and run the following commands.\nsudo apt-get install clang-18 lld-18\n \nLLVM_CONFIG=llvm-config-XX make all\nsudo make install\nFighting compiler optimization\nA simple trick to avoid entire for loops and function calls be optimized away, store the result in an iteration or of the function in a volatile variable is the simplest without side-effects like printf.\nBuild Flags\ncc_params[cc_par_cnt++] = &quot;-g&quot;;\nif (!have_o) cc_params[cc_par_cnt++] = &quot;-O3&quot;;\nWithout -O flags, afl-cc will add -O3. Besides, -g is always added.\nCC=afl-clang-lto CXX=afl-clang-lto++ RANLIB=llvm-ranlib-17 AR=llvm-ar-17 AS=llvm-as-17 CFLAGS=&quot;-fsanitize=fuzzer-no-link&quot; LDFLAGS=&quot;-fsanitize=fuzzer-no-link&quot; ./configure\n \n# Standard ver.\nmake fuzz_parser\n \n# With sanitizers\nAFL_USE_ASAN=1 AFL_USE_UBSAN=1 make fuzz_parser\nLast line of output should be [+] Instrumented ... (non-hardened mode) or (non-hardened, ASAN, UBSAN mode) with sanitizers.\nPrepare Input\n\nIMPORTANT: if you use afl-cmin or afl-cmin.bash, then either pass - or @@ as command line parameters.\n\n# No need for fuzzing flags\n./configure\nmake yash\n# Replace `-v` with `r` in tests/Makefile\ncd tests &amp;&amp; make test-valgrind\n# Collect input files from tmp.*\nfind . -mindepth 2 -maxdepth 2 -type f -name &#039;*.in&#039; | parallel &#039;cp {} ../yash-afl-input-tests/{#}.in&#039;\n \nafl-cmin -i ../yash-afl-input-tests -o ../yash-afl-input-unique -- ./fuzz_parser -\n \nfind ../yash-afl-input-unique -type f | parallel &#039;afl-tmin -i {} -o ../yash-afl-input-min/{/} -- ./fuzz_parser&#039;\n \nafl-cmin -i ../yash-afl-input-min -o ../yash-corpus-min-input -- ./fuzz_parser -\nRun\n# Master\nAFL_FINAL_SYNC=1 afl-fuzz -M main -i ../yash-corpus-min-input -o ../yash-corpus-r1 -x .../bash.dict -a text -- ./fuzz_parser\n \n# Slaves\nafl-fuzz -S sans-00 -i ../yash-corpus-min-input -o ../yash-corpus-r1 -x path/to/bash.dict -a text -- ./fuzz_parser_asan_ubsan\n&lt;...&gt;\nlibFuzzer\nBuild flags\n# With ASan\n./configure CC=clang-17 CXX=clang++-17 CFLAGS=&quot;-g -O1 -fsanitize=fuzzer-no-link,address -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION&quot; LDFLAGS=&quot;-fsanitize=fuzzer-no-link,address&quot;\n \n# With MacPorts on macOS\n./configure CC=clang-mp-17 CXX=clang++-mp-17 CFLAGS=&quot;-g -O1 -I/opt/local/include -fsanitize=fuzzer-no-link -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION&quot; LDFLAGS=&quot;-L/opt/local/lib -fsanitize=fuzzer-no-link&quot;\n \nmake fuzz_parser\nNote: during configure phase, if AddressSanitizer discovers a crash, it will silently disable the feature.\nFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION is proposed by libFuzzer authors as a common build macro for fuzzing-friendly build.\nRun\nrm -r ../yash-corpus-r1/\nmkdir ../yash-corpus-r1\n \n./fuzz_parser -dict=bash.dict ../yash-corpus-r1\n./fuzz_parser -dict=bash.dict -fork=&lt;N&gt; ../yash-corpus-r1 # multi-process\nMinimize case\n./fuzz_parser -minimize_crash=1 -runs=10000 crash-XXX\nReferences\n\ngithub.com/AFLplusplus/AFLplusplus/blob/61e27c6b54f7641a168b6acc6ecffb1754c10918/src/afl-cc.c#L1255-L1256\ngithub.com/AFLplusplus/AFLplusplus/blob/61e27c6b54f7641a168b6acc6ecffb1754c10918/instrumentation/README.lto.md\ngithub.com/AFLplusplus/AFLplusplus/blob/61e27c6b54f7641a168b6acc6ecffb1754c10918/docs/fuzzing_in_depth.md\n"},"notes/Software/Ghostty":{"title":"Ghostty","links":[],"tags":[],"content":"My config\n# Until search function is implemented.\nkeybind = super+f=write_screen_file:open\n \n# For low DPI screens on macOS.\nfont-thicken = true\nTo specify a fallback font, simply set the key twice.\nfont-family = &quot;JetBrains Mono&quot;\nfont-family = &quot;Hei Regular&quot;"},"notes/Software/HP-Smart-Scanning-Tips":{"title":"HP Smart Scanning Tips","links":[],"tags":[],"content":"Printer setup\nIf Wi-Fi is not stable, try connecting with USB cable.\nScanning\nSteps to scan a document:\n\nScan without paper in scan area to recognize black edges.\nIf there is some blackness on the left but none at the top, you can try the following instructions.\nenable Detect Edges and set Page Size to Entire Scan Area.\nPut document in and align with top left corner icon.\nSlightly slide the document from the left edge, in terms of scan coordinates.\nClick Scan or Add Scan.\nCheck the auto crop result and click Apply if satisfied.\nContinue from step 4 to scan more pages.\n\nIf you have a well-aligned scan area, just select the same page size as your document and put it right in place."},"notes/Software/Interesting-Bug-Stories":{"title":"Interesting Bug Stories","links":[],"tags":[],"content":"References\n\nbeza1e1.tuxen.de/lore/index.html\n500mile.email/\n"},"notes/Software/Irssi-Config":{"title":"Irssi Config","links":[],"tags":[],"content":"Pinning self-signed certificate\n/set use_tls yes\n/set tls_verify no\n/set tls_pinned_cert &quot;XX:XX:...&quot;\n\nCertfp authentication\nThe pem file should contain both a private key and the corresponding certificate.\n/set tls_cert &quot;~/.irssi/certs/xxx.user.pem&quot;\n"},"notes/Software/Krita":{"title":"Krita","links":[],"tags":[],"content":"Pen tablet shortcuts\n\nShift + drag left/right to resize the brush.\nButton (close to pen tip) + drag to zoom in and out.\nSPACE + drag to move the canvas.\nShift + SPACE + drag to rotate the canvas.\n"},"notes/Software/Layer-7-Load-Balancing-Solutions":{"title":"Layer 7 Load Balancing Solutions","links":[],"tags":[],"content":"Front/Edge proxy\nComplete solutions\n\ngithub.com/Kong/kong\ngithub.com/bfenetworks/bfe (Baidu)\ngithub.com/memorysafety/river (ISRG, not public yet)\n\nBuilding blocks\n\ngithub.com/cloudflare/pingora (Cloudflare, API is unstable during pre-1.0)\ngithub.com/envoyproxy/envoy\n\nTo be determined\n\ngithub.com/TykTechnologies/tyk\ngithub.com/krakend/krakend-ce\ngithub.com/flomesh-io/pipy\n"},"notes/Software/Netdata":{"title":"Netdata","links":[],"tags":[],"content":"Disable telemetry and cloud features\ncat &gt; ~/cloud.conf &lt;&lt; EOF\n[global]\n  enabled = no\nEOF\n \nsudo cp ~/cloud.conf /var/lib/netdata/cloud.d/\nsudo touch /etc/netdata/.opt-out-from-anonymous-statistics\nHow to build RPM packages\ngithub.com/netdata/netdata/blob/v1.39.0/packaging/building-native-packages-locally.md\n:z is needed if SELinux is enabled. Use :Z for additional access protection.\ngit clone github.com/netdata/netdata.git\ncd netdata\n \ngit apply &lt;&lt;&#039;EOF&#039;\ndiff --git a/netdata.spec.in b/netdata.spec.in\nindex c2fa7dcab..f70f74c6c 100644\n--- a/netdata.spec.in\n+++ b/netdata.spec.in\n@@ -231,6 +231,7 @@ export CFLAGS=&quot;${CFLAGS} -fPIC&quot; &amp;&amp; ${RPM_BUILD_DIR}/%{name}-%{version}/packaging\n # Conf step\n autoreconf -ivf\n %configure \\\n+\t--disable-cloud \\\n \t%if 0%{!?_have_ebpf}\n \t--disable-ebpf\n \t%endif\nEOF\n \npodman run -it --rm -e VERSION=&lt;1.x.x&gt; -v $PWD:/netdata:z netdata/package-builders:oraclelinux9\n# checking if Cloud functionality should be enabled... no\nCharts v3\n\nThese are currently available at Netdata Cloud. At the next Netdata release (v1.40.0), the agent dashboard will be replaced to also use the same charts.\n"},"notes/Software/Nix-Channel-Status":{"title":"Nix Channel Status","links":[],"tags":[],"content":"View channel freshness on status.nixos.org/. nixpkgs-unstable can be a few days old.\nFor Nix channel freshness history, check out monitoring.nixos.org/grafana/d/LhIq8iLWk/channel-updates\nUse nixpk.gs/pr-tracker.html to check progress of a specific nixpkgs PR. For example, nixpk.gs/pr-tracker.html"},"notes/Software/Remote-Linux-Desktop-on-macOS":{"title":"Remote Linux Desktop on macOS","links":["notes/Software/UTM","notes/Software/CachyOS-in-UTM"],"tags":[],"content":"Linux distributions\nArch Linux\nwiki.archlinux.org/title/Laptop/Apple\nVirtual Machine Manager (VMM)\nVMware Fusion\nFree for all users.\nSince VMware Fusion 13, OpenGL 4.3 is supported, but performance characteristics are unclear.\nUTM\nSupport Apple Hypervisor or QEMU backends.\nQEMU backend supports VirGL and both support host-side upscaling.\nSee also CachyOS in UTM.\nVirtualBox\nFree, but has CPU usage going wild issues. www.virtualbox.org/ticket/18089\nRemote X Server\nSSH X11 forwarding\nRequires XQuartz or xorg-server from MacPorts.\nLowest overhead for a local connection.\nBest for running applications directly.\nSPICE\nHaven’t tried yet. macOS client does not support SSH.\nX2Go\nDownload .dmg from code.x2go.org/releases/binary-macosx/x2goclient/releases/ to get the latest version. Also requires XQuartz.\nmacOS client could not resume existing sessions, crashing every time.\nnxagent does not support modern desktop environments. In general, seamless mode is preferred.\nXpra\nKeyboard support is bad. Modifier keys are broken. github.com/Xpra-org/xpra/issues/2804\nBased on video streaming, less efficient.\nOpenGL does not work on client-side.\nReferences\n\ndocs.getutm.app/settings-qemu/devices/display/\nwiki.x2go.org/doku.php/doc:de-compat\n"},"notes/Software/Resin-Source-Code-Preservation":{"title":"Resin Source Code Preservation","links":[],"tags":[],"content":"Online changelog (outdated)\ncaucho.com/resin-4.0/changes/changes.xtp\nImport process\n\ngit init resin-src &amp;&amp; cd resin-src\nDownload all resin-4.0.x-src.tar.gz files from caucho.com/products/resin/download/gpl.\n/usr/share/git/contrib/fast-import/import-tars.perl resin-4.0.{0..67}-src.tar.gz and git checkout import-tars\ngit filter-branch -d /dev/shm/.git-rewrite --index-filter &#039;git rm -r --cached --ignore-unmatch lib/*.jar ext-lib project-jars webapp-jars win32 win64 *.exe compat/javaee-15.jar aclocal.m4 automake Makefile.in configure&#039; (you can use any tmpfs mount instead of /dev/shm)\nRecover folder structure and missing parts.\n\nBuild instructions\nPrerequisites\n\nJDK 8, set as JAVA_HOME.\nApache Ant.\nAutoconf, Automake and Libtool from GNU.\n\nPack\nant -Dversion=4.0.XX dist\nWith MacPorts:\n# If building with OpenJDK 8\nsed -i &#039;&#039; &#039;s/1\\.7/1.8/&#039; build-common.xml\n \nPATH=&quot;/Library/Java/JavaVirtualMachines/temurin-8.jdk/Contents/Home/bi:$PATH:/opt/local/libexec/gnubin&quot; ant -Dversion=4.0.67 dist\nEclipse plugin build instructions\nant -Dversion=4.0.XX -Declipse.home=/... eclipse2\nNetBeans plugin build instructions\n\nOpen artifacts/netbeans as a project in Apache Netbeans IDE 16.\nRight-click the project and click “Create NBM”.\nInstall the NBM in artifacts/netbeans/build.\n"},"notes/Software/Sideloading-Blink-with-AltStore":{"title":"Sideloading Blink with AltStore","links":[],"tags":[],"content":"Build from source\n\nProvisioning profile “iOS Team Provisioning Profile: …” doesn’t support the Associated Domains, Fonts, iCloud, and Push Notifications capability.\n\nFollow the README to build Blink Shell in Xcode. With a free developer account, these entitlements are not supported, and therefore breaks functionality like WebAuthn.\nCrash on start\nCrash log contains Termination Reason: SIGNAL 5 Trace/BPT trap: 5, which indicates that it’s a memory safety issue from programming errors.\nOn the line where the program crashed, Blink constructs a URL from BlinkPaths.groupContainerPath() without null-safety checks.\nlet migratorFileURL = URL(fileURLWithPath: BlinkPaths.groupContainerPath()).appendingPathComponent(&quot;.migrator&quot;)\nBlinkPaths.groupContainerPath() in turn calls the containerURLForSecurityApplicationGroupIdentifier method on the default NSFileManager, which in iOS,  returns nil when the group identifier invalid for the app, i.e. missing from it’s App Groups Entitlement.\nAltStore modifications\nApp groups in entitlements\nAltStore replaces the original app group with a new one to workaround Apple’s restrictions. In func updateAppGroups, each app group identifier is appended with a . and the team identifier used to sign the app.\nfor groupIdentifier in applicationGroups\n{\n    let adjustedGroupIdentifier = groupIdentifier + &quot;.&quot; + team.identifier\n    \n    if let group = fetchedGroups.first(where: { $0.groupIdentifier == adjustedGroupIdentifier })\n    {\n        groups.append(group)\n    }\n    else\n    {\n        dispatchGroup.enter()\n        ...\nFile providers\nAny NSExtensionFileProviderDocumentGroup is rewritten into the new app group as well.\nThe filter and min chain ensures that the shortest matching candidate is found.\n// To keep file providers working, remap the NSExtensionFileProviderDocumentGroup, if there is\none.\nif var extensionInfo = infoDictionary[&quot;NSExtension&quot;] as? [String: Any],\n    let appGroup = extensionInfo[&quot;NSExtensionFileProviderDocumentGroup&quot;] as? String,\n    let localAppGroup = appGroups.filter({ $0.contains(appGroup) }).min(by: { $0.count &lt; $1.count })\n{\n    extensionInfo[&quot;NSExtensionFileProviderDocumentGroup&quot;] = localAppGroup\n    infoDictionary[&quot;NSExtension&quot;] = extensionInfo\n}\nApp bundle IDs\nThe parentBundleID is also modified to include the team ID.\nif application.isAltStoreApp\n{\n    // Use legacy bundle ID format for AltStore (and its extensions).\n    updatedParentBundleID = &quot;com.\\(team.identifier).\\(parentBundleID)&quot;\n}\nelse\n{\n    updatedParentBundleID = parentBundleID + &quot;.&quot; + team.identifier // Append just team identifier to make it harder to track.\n}\n \nlet bundleID = application.bundleIdentifier.replacingOccurrences(of: parentBundleID, with: updatedParentBundleID)\nVerification\nCrash explicitly and log app group ID on successful invocation.\ndiff --git a/BlinkConfig/BlinkPaths.m b/BlinkConfig/BlinkPaths.m\n--- a/BlinkConfig/BlinkPaths.m\n+++ b/BlinkConfig/BlinkPaths.m\n@@ -29,6 +29,7 @@\n //\n ////////////////////////////////////////////////////////////////////////////////\n \n+#import &lt;os/log.h&gt;\n #import &quot;BlinkPaths.h&quot;\n #import &quot;XCConfig.h&quot;\n \n@@ -66,6 +67,13 @@ NSString *__iCloudsDriveDocumentsPath = nil;\n \n     NSFileManager *fm = [NSFileManager defaultManager];\n     NSString *path = [fm containerURLForSecurityApplicationGroupIdentifier:groupID].path;\n+\n+    if (path == nil) {\n+      [NSException raise:@&quot;Invalid app group ID&quot; format:@&quot;app group %@ not found&quot;, groupID];\n+    } else {\n+      os_log(OS_LOG_DEFAULT, &quot;app group %{public}@ found&quot;, groupID);\n+    }\n+\n     __groupContainerPath = path;\n   }\n   return __groupContainerPath;\nBefore iOS 10, NSLog(@&quot;app group %@ found&quot;, groupID); can be used instead.\nFix\nNSString *groupID = [XCConfig infoPlistFullGroupID];\n\nThe key used to search for the group’s container directory is BLINK_GROUP_ID from XCConfig, which is stored in BlinkConfig/Info.plist.\nYou can use the variable TEAM_ID from the developer_setup.xcconfig file if you have followed Blink’s guidance on building and installing Blink yourself.\ndiff --git a/BlinkConfig/Info.plist b/BlinkConfig/Info.plist\n--- a/BlinkConfig/Info.plist\n+++ b/BlinkConfig/Info.plist\n@@ -5,7 +5,7 @@\n \t&lt;key&gt;BLINK_CLOUD_ID&lt;/key&gt;\n \t&lt;string&gt;$(CLOUD_ID)&lt;/string&gt;\n \t&lt;key&gt;BLINK_GROUP_ID&lt;/key&gt;\n-\t&lt;string&gt;$(GROUP_ID)&lt;/string&gt;\n+\t&lt;string&gt;$(GROUP_ID).$(TEAM_ID)&lt;/string&gt;\n \t&lt;key&gt;BLINK_KEYCHAIN_ID1&lt;/key&gt;\n \t&lt;string&gt;$(KEYCHAIN_ID1)&lt;/string&gt;\n \t&lt;key&gt;BLINK_REVCAT_PUBKEY&lt;/key&gt;\nHardcoding the team ID or modifying Blink’s code to auto-detect the new app group would work as well.\nReferences\n\ndeveloper.apple.com/documentation/xcode/identifying-the-cause-of-common-crashes#Determine-whether-the-crash-is-a-Swift-runtime-error\ngithub.com/blinksh/blink/blob/77d3cec7ff7dc7f2389d372d1d10dba9b9d6aaec/Blink/Migrator/Migrator.swift#L42\ngithub.com/altstoreio/AltStore/blob/7d7e098ef5e48fe346430e068271d1b1ae49817b/AltServer/Devices/ALTDeviceManager%2BInstallation.swift#L849\nstackoverflow.com/a/45957891\n"},"notes/Software/Switch-Input-Source-on-macOS":{"title":"Switch Input Source on macOS","links":[],"tags":[],"content":"Built-in keyboard shortcuts\nUse Fn key or “Select … source” shortcuts to switch input source, which can be enabled in System Settings.\n\nIn addition to the options detailed above for switching input sources, you can also change input sources by using the Fn key. To use this option, in Keyboard settings, click the “Press fn key to” pop-up menu, then choose Change Input Source.\n\nInput source switching API\n\nSometimes the input source switching API (TISSelectInputSource) does not work properly for some input sources.\nIt changes an indicator (statusbar), but behavior of input source is not changed.\nThis issue is occurred on the input sources which has complex behavior such as Japanese.\n\nIt’s a macOS bug that CJK input sources can not be switched to reliably by calling the API.\nWorkaround\nNo simple yet reliable workaround.\nReferences\n\nsupport.apple.com/en-us/guide/mac-help/kbdm162/mac\ngithub.com/Hammerspoon/hammerspoon/issues/1429\ngithub.com/tekezo/Karabiner/issues/308#issuecomment-190693550\ngithub.com/tekezo/Karabiner/blob/version_10.15.0/src/core/server/Resources/vkchangeinputsourcedef.xml#L210-L236\ngithub.com/nuridol/SwitchIM/blob/db93e4d0f299916670b4a36222abfc84dda9f4b7/SwitchIM/SwitchIM.swift#L132-L140\n"},"notes/Software/Terminal-Protocols":{"title":"Terminal Protocols","links":[],"tags":[],"content":"Sequences\nPrimary Device Attributes (DA1)\nThis is used to report operating VT level and supported extensions, e.g. Sixel.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEscape CodeExplanationCSI c or CSI 0 cCSI is ESC [ or ^[[. This is a Primary Device Attributes (DA1) request.CSI ? 64 ; Ps1 ; ... Psn cStandard DA1 response format. Ps1 to Psn represents specific features.CSI ? *level* ; 1 ; 2 ; 6 ; 9 ; 15 cwith level = 60 + (decTerminalID / 100) (default 64)CSI ? 1 ; 2 cVT100CSI ? 1 ; 0 cVT101CSI ? 6 vVT102\nNote: spaces are added for readability. They do not exist in actual DA1 escape codes.\nReferences:\n\ngithub.com/KDE/konsole/commit/2d93fed82aa27e89c9d7301d09d2e24e4fa4416d#diff-0d4b7240b969dc3d1f694d7e7a8c71a1a318f1f928acbbf1b27d809e65a56c8fR682\nSee vt100.net/docs/vt510-rm/DA1.html for a list of extension codes.\n\nModes\nSynchronized updates (DECSET/DECRST 2026)\nSee gitlab.freedesktop.org/terminal-wg/specifications/-/merge_requests/2 for the canonical specification."},"notes/Software/UTM":{"title":"UTM","links":["notes/Operating-System/macOS/Paravirtualized-Graphics"],"tags":[],"content":"Graphics\nParavirtualized Graphics only work for Apple Silicon Macs. github.com/utmapp/UTM/issues/3491\nDefault QEMU VirGL backend on macOS is virtio-gpu-gl-pci.\nOn Intel Macs, UI will freeze after host recovers from sleep. If you’ve encountered this, switch back to virtio-gpu-pci.\nmacOS guest\nmacOS Installer can be downloaded with github.com/ninxsoft/Mist.\nopenSUSE guest\nThe installer requires a serial device. You can remove it afterwards. See github.com/utmapp/UTM/issues/5251."},"notes/Software/VMware-Fusion-and-Workstation":{"title":"VMware Fusion and Workstation","links":[],"tags":[],"content":"Download\nOn November 11, 2024, Broadcom announced that VMware Fusion and Workstation will be available for free to everyone.\nHowever, until a new release is made, VMware Fusion Pro still says “Licensed for Personal Use only.”\nYou can download binaries for free from support.broadcom.com/group/ecx/downloads. Select “VMware Cloud Foundation” from the top-right dropdown menu, and search for the product you want.\nDNF command stuck on openEuler guest\nstrace shows it’s trying to resolve the local hostname. Promoting myhostname in hosts: entry of /etc/nsswitch.conf fixes it. Alternatively, you could also modify the /etc/hosts file.\nTime synchronization on Linux guests\nEnable “Synchronize time” in VM Settings → Advanced and install open-vm-tools in guest VM.\nIf that didn’t work, a workaround is to allow an NTP daemon to correct time.\nFor chrony, modify /etc/chrony.conf to let chronyd step the system clock at all times when its offset is larger than 5 seconds.\nmakestep 5.0 -1\n"},"notes/Software/WeeChat-Config":{"title":"WeeChat Config","links":[],"tags":[],"content":"ZNC multi-connect and CertFP\nConnect to multiple networks by specifying network name in username in username/networkname format.\n/server add networkname IP/port -ssl -username=username/networkname -password=x -autoconnect\n/set irc.server.snoonet.ssl_fingerprint xxxx\n/set irc.server.libera.ssl_cert &quot;${weechat_config_dir}/ssl/client.pem&quot;\nSet irc.server.libera.ssl_fingerprint to verify with fingerprint only if your server has a self-signed certificate.\nEncrypt config for local certificate authentication\nGenerate a TLS certificate with its corresponding private key attached and protected by a password, and then extract the certificate fingerprint.\nopenssl req -x509 -new -newkey ed25519 -sha256 -out l2dy.example.pem -keyout l2dy.example.pem\nopenssl x509 -in l2dy.example.pem -noout -fingerprint -sha256 | awk -F= &#039;{gsub(&quot;:&quot;,&quot;&quot;); print tolower ($2)}&#039;\nThen, authenticate to IRC server with your password and ask NickServ to save the SHA256 fingerprint.\nFinally, configure WeeChat to use CertFP authentication and auto connect.\n# Add server\n/server add example irc.example.com/6697 -tls\n/set irc.server.example.tls_cert %h/certs/l2dy.example.pem\n/set irc.server.example.sasl_mechanism external\n/set irc.server.example.autojoin &quot;#!&quot;\n# Save encrypted password\n/secure passphrase &lt;password to type when starting WeeChat&gt;\n/secure set exampletls &lt;password for the private key&gt;\n/set irc.server.example.tls_password &quot;${sec.data.exampletls}&quot;\n \n# Test connection\n/connect example\n \n# Enable auto-connect\n/set irc.server.example.autoconnect on\nLook and feel\nSeparate server buffers\n/set irc.look.server_buffer independent\nIRC smart filter\n/filter add irc_smart * irc_smart_filter *\nXDG directories\nTable of default directories and paths.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariableDefault valueFallback valueweechat_config_dir$XDG_CONFIG_HOME/weechat$HOME/.config/weechat if $XDG_CONFIG_HOME is not defined or emptyweechat_data_dir$XDG_DATA_HOME/weechat$HOME/.local/share/weechat if $XDG_DATA_HOME is not defined or emptyweechat_cache_dir$XDG_CACHE_HOME/weechat$HOME/.cache/weechat if $XDG_CACHE_HOME is not defined or emptyweechat_runtime_dir$XDG_RUNTIME_DIR/weechatsame as cache directory if $XDG_RUNTIME_DIR is not defined or empty\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptionDefault valuefifo.file.path${weechat_runtime_dir}/weechat_fifo_${info:pid}logger.file.path${weechat_data_dir}/logsrelay.network.ssl_cert_key${weechat_config_dir}/ssl/relay.pemscript.scripts.path${weechat_cache_dir}/scriptweechat.plugin.path${weechat_data_dir}/pluginsxfer.file.download_path${weechat_data_dir}/xfer\ngithub.com/weechat/weechat/blob/b614a5c5db243ad64f0ab32e4e68d221e57878b7/ReleaseNotes.adoc#v3.2_xdg_directories"},"notes/Software/WeeChat":{"title":"WeeChat","links":["notes/Software/WeeChat-Config"],"tags":[],"content":"See also WeeChat Config.\nSwitch between buffers\nPress Alt+number to switch to the corresponding numbered buffer.\nTo switch between server and WeeChat core buffers, press Ctrl+X.\nIt’s also possible to turn off the auto merge of server buffers:\n/set irc.look.server_buffer independent\n"},"notes/Software/Wireshark":{"title":"Wireshark","links":[],"tags":[],"content":"Expert information\n\nWireshark keeps track of any anomalies and other items of interest it finds in a capture file and shows them in the Expert Information dialog. The goal is to give you a better idea of uncommon or notable network behavior and to let novice and expert users find network problems faster than manually scanning through the packet list.\n\n\nYou can open the expert info dialog by selecting Analyze → Expert Info or by clicking the expert level indicator in the main status bar.\nRight-clicking on an item will allow you to apply or prepare a filter based on the item, copy its summary text, and other tasks.\n\nTips and tricks\nFilter for SYN retransmission\ntcp.analysis.retransmission and tcp.flags.syn == 1\n\nReferences\n\nwww.wireshark.org/docs/wsug_html_chunked/ChAdvExpert.html\n"},"notes/Software/Xcode-Sanitizer-Support":{"title":"Xcode Sanitizer Support","links":[],"tags":[],"content":"Xcode documentation lists several runtime tools, including sanitizers.\n\nXcode provides several runtime tools to identify potential issues in your code:\n\nAddress Sanitizer—The ASan tool identifies potential memory-related corruption issues.\nThread Sanitizer—The TSan tool detects race conditions between threads.\nMain Thread Checker—This tool verifies that system APIs that must run on the main thread actually do run on that thread.\nUndefined Behavior Sanitizer—The UBSan tool detects divide-by-zero errors, attempts to access memory using a misaligned pointer, and other undefined behaviors.\n\n\nAddress Sanitizer runtime\nOn macOS and iOS, Address Sanitizer requires a dynamic library that contains the asan runtime to run.\nIf the compiler version that a dependency framework is compiled with and the compiler version of your Xcode mismatches, you may see errors like dyld[xxx]: missing symbol called when you have Address Sanitizer enabled across them."},"notes/Software/ZNC":{"title":"ZNC","links":[],"tags":[],"content":"CertFP with server\nOne certificate per user only. Make sure you have a backup before updating it from Web UI.\nwiki.znc.in/Cert\nCertFP with client\nChat with *certauth to manage keys. It’s not configurable from Web UI.\nwiki.znc.in/Certauth\n/msg *certauth help\n&lt;*certauth&gt; Commands: show, list, add, del [no]\n/msg *certauth show\n&lt;*certauth&gt; Your current public key is: ...\n/msg *certauth list\n&lt;*certauth&gt; No keys set for your user\n/msg *certauth add\n&lt;*certauth&gt; Added your current public key to the list\n/msg *certauth list\n...\n"},"notes/Software/Zellij":{"title":"Zellij","links":["notes/Software/Terminal-Protocols"],"tags":[],"content":"Scrolling\nZellij support scrolling with mouse out of the box, and can auto-detect a scrollable program and switch from scrollback buffer to it.\nBugs\n\nZellij is slow at displaying ANSI-colored content over a slow SSH connection with buffer scrolling involved.\n\nZellij is fast too if no scrolling is involved, e.g. colored content is less than tput lines and buffer is clear.\nTmux is fast in both cases.\nZellij updates SCROLL: 0/xxx after each line of output, which probably slowed it down.\n\n\nZellij reports Sixel support in Primary Device Attributes (DA1) response even if underlying terminal does not support it. github.com/zellij-org/zellij/issues/3158\n\nReferences\n\nvt100.net/docs/vt510-rm/DA1.html\nterminalguide.namepad.de/seq/csi_sc/\nchromium.googlesource.com/apps/libapps/+/573b4f0d07264cc2b06bd9b21a5e3618af600040/hterm/js/hterm_vt.js#2712\n"},"notes/Software/a-Shell":{"title":"a-Shell","links":[],"tags":[],"content":"Settings\nSettings are available in the system’s Settings app.\nDirectories\n\n~xxx represents a bookmark. You can list existing bookmarks with showmarks.\n~ (home directory) is not writable, so there are pre-defined environment variables that redirect programs to use ~/Documents or ~/Library.\n"},"notes/Software/iTerm2-Features":{"title":"iTerm2 Features","links":[],"tags":[],"content":"Selection inside tmux or Emacs splits\nIn the Edit system menu, enable “Selection Respects Soft Boundaries.”\nRecover deleted windows or splits\nPress s-Z within a few seconds to get a closed window back.\nSearch\nTo fix a search highlight bug, go to advanced settings and turn off “Synchronize search queries across windows and applications”.\nCopy mode\nTo copy text from iTerm2 with keyboard only, press s-S-c to activate copy mode.\nIn copy mode, press v, V or C-v to toggle selection by character, line, or rectangle, similar to Vim’s Visual mode. hjkl and word-based movement keys are also supported.\nTo copy the selection, press y. To quit copy mode, press q."},"notes/Software/nix-env-Dependency-Tree":{"title":"nix-env Dependency Tree","links":[],"tags":[],"content":"List installed packages\nnix-env -q\nList dependency tree of user environment\nnix-store -q --tree ~/.nix-profile"},"notes/Startup/Earthly-CI-Postmortem":{"title":"Earthly CI Postmortem","links":[],"tags":[],"content":"earthly.dev/blog/shutting-down-earthly-ci/\nFeatures are not what makes you successful.\n\nYou’ll never be more mature, or have more features, or more integrations than the incumbents.\nYou can be 10x better in one, very specific way. And you’ll appeal to the few teams where the very specific problem you’re solving is so painful that they’re willing to make compromises on everything else.\nThen, once you have captured that segment, you invest more, extend to a wider audience, get more feedback, then again invest more, extend again, and so on.\nSo if your MVP is not getting enough validation, you can’t just slap more features on it, because, again, features are not what will make you successful. Incumbents win feature contests.\n\nBuild the product incrementally.\n\nLearning from mistakes of the past, at Earthly we built everything incrementally. And we even put products on the market that initially seemed like purely engineering intermediate milestones. Each product builds on top of the previous achievements, thus allowing for incremental iteration with the customer in mind. We’re now seeing our latest incremental iteration not working in the marketplace. And, knowing what I know about early products, it’s not the missing features that are the problem.\n\nFail fast.\n\nThere were some early signals that certain aspects of the product did not align with what the industry needs, but we didn’t listen. We just kept building.\nMy biggest regret from the experience was that we did not stop earlier when the signs were there.\n"},"notes/Startup/How-To-Be-a-Manager":{"title":"How To Be a Manager","links":[],"tags":[],"content":"barely-managing.bearblog.dev/"},"notes/Startup/One-Man-Full-Stack-Roadmap":{"title":"One-Man Full Stack Roadmap","links":[],"tags":[],"content":"References\n\ngithub.com/easychen/stack-roadmap (Chinese)\ngithub.com/easychen/one-person-businesses-methodology (Chinese)\n"},"notes/Startup/Payment-Gateways-for-Individuals":{"title":"Payment Gateways for Individuals","links":[],"tags":[],"content":"i.e. unincorporated business\n\nsupport.stripe.com/questions/selling-on-stripe-without-a-separate-business-entity\n"},"notes/Startup/SaaS-Product-Strategy-at-Tanda":{"title":"SaaS Product Strategy at Tanda","links":[],"tags":[],"content":"ghiculescu.substack.com/p/11-years-of-saas-product-strategy"},"notes/Storage/USB-Hub-Power-Supply":{"title":"USB Hub Power Supply","links":[],"tags":[],"content":"If a USB hub could not power two portable hard drives simultaneously, it could be that there isn’t sufficient power. To resolve this, use dedicated power supply to power the hub.\nDO NOT use the hub to power your laptop. It may work when the laptop is idle, but once the laptop draws too much power, hard drives become unstable and get disconnected."},"notes/Storage/ZFS-Compression-Test":{"title":"ZFS Compression Test","links":[],"tags":[],"content":"\n1M recordsize does not improve compress ratio.\nzstd performs better than lz4.\n\n% zfs get recordsize\nNAME             PROPERTY    VALUE    SOURCE\n/Test1           recordsize  128K     default\n/Test2           recordsize  1M       local\n/Test3           recordsize  128K     default\n/Test4           recordsize  1M       local\n% zfs get compression\nNAME             PROPERTY     VALUE           SOURCE\n/Test1           compression  lz4             inherited\n/Test2           compression  lz4             inherited\n/Test3           compression  zstd            local\n/Test4           compression  zstd            local\n\n% zfs get compressratio\nNAME             PROPERTY       VALUE  SOURCE\n/Test1           compressratio  1.00x  -\n/Test2           compressratio  1.00x  -\n/Test3           compressratio  1.02x  -\n/Test4           compressratio  1.02x  -\n% zfs list\nNAME       USED  AVAIL  REFER  MOUNTPOINT\n/Test1     1.91G   651G  1.91G  /Volumes/zero_backup/Test1\n/Test2     1.92G   651G  1.92G  /Volumes/zero_backup/Test2\n/Test3     1.88G   651G  1.88G  /Volumes/zero_backup/Test3\n/Test4     1.89G   651G  1.89G  /Volumes/zero_backup/Test4\n"},"notes/Storage/ZFS":{"title":"ZFS","links":["notes/Storage/ZFS-Compression-Test"],"tags":[],"content":"Datasets\nzfs craete &lt;dataset&gt;\n\nSnapshots\n# Create snapshot\nzfs snap &lt;volume&gt;@&lt;snapshot_name&gt;\n# Delete snapshot (dry-run)\nzfs destroy -vn &lt;volume&gt;@&lt;snapshot_name&gt;\n# List snapshots\nzfs list -t snapshot\n# Restore snapshot\nzfs rollback &lt;volume&gt;@&lt;snapshot_name&gt;\nDataset properties\nRecord size\n\n\n                  \n                  Note \n                  \n                \n\nThe default 128KiB is good enough for most cases. See also ZFS Compression Test.\n\n\nGeneral rules of thumb:\n\n1MiB for general-purpose file sharing/storage\n1MiB for BitTorrent download folders—this minimizes the impact of fragmentation!\n64KiB for KVM virtual machines using Qcow2 file-based storage\n16KiB for MySQL InnoDB \n8KiB for PostgreSQL\n\nSee klarasystems.com/articles/tuning-recordsize-in-openzfs/.\nCompression\n# List current compression config\nzfs get compression\n# Set zstd compression\nzfs set compression=zstd pool[/component]\n# Inherit from parent\nzfs inherit compression pool[/component]\nApplying to existing data\ncompression and deduplication can be applied to existing data with filerewrite, but recordsize change can not be applied in-place.\nRescue mount\nLinux\nmount -o zfsutil -t zfs &lt;dataset&gt; &lt;mountpoint&gt;\n"},"notes/Storage/zstd-Compression":{"title":"zstd Compression","links":[],"tags":[],"content":"libarchive\n\nlibarchive built with libzstd has the best compression ratio.\nThe default level 3 is good for media files. YMMV.\n\n$ /usr/bin/bsdtar --version\nbsdtar 3.5.3 - libarchive 3.5.3 zlib/1.2.11 liblzma/5.0.5 bz2lib/1.0.8\n$ bsdtar --version\nbsdtar 3.7.2 - libarchive 3.7.2 zlib/1.2.13 liblzma/5.2.6 bz2lib/1.0.8 liblz4/1.9.3 libzstd/1.5.5\n$ zstd --version\n*** Zstandard CLI (64-bit) v1.5.5, by Yann Collet ***\n\n$ time bsdtar -cf /tmp/tar-raw.tar &lt;dir&gt;\n3.16s user 43.18s system 72% cpu 1:03.68 total\n$ time bsdtar --zstd -cf /tmp/tar-builtin.tar.zst &lt;dir&gt;\n62.27s user 44.48s system 90% cpu 1:57.63 total\n$ time /usr/bin/bsdtar --zstd -cf /tmp/tar-pipe.tar.zst &lt;dir&gt;\n61.47s user 64.16s system 166% cpu 1:15.60 total\n$ zstd tar-raw.tar\n$ time bsdtar --options zstd:compression-level=6 --zstd -cf /tmp/tar-z6.tar.zst &lt;dir&gt;\n124.99s user 42.30s system 93% cpu 2:58.48 total\n\n$ ls -goS\n-rw-r--r-- 1 36944175616 Oct 28 16:28 tar-raw.tar\n-rw-r--r-- 1 36801599496 Oct 28 16:28 tar-raw.tar.zst\n-rw-r--r-- 1 36801599484 Oct 28 16:30 tar-pipe.tar.zst\n-rw-r--r-- 1 36800436486 Oct 28 16:30 tar-builtin.tar.zst\n-rw-r--r-- 1 36792020901 Oct 28 17:08 tar-z6.tar.zst\n\n# Another directory\n\n$ time bsdtar -cf /tmp/tar-raw.tar &lt;dir&gt;\n0.55s user 7.16s system 61% cpu 12.488 total\n$ time bsdtar --zstd -cf /tmp/tar-builtin.tar.zst &lt;dir&gt;\n9.22s user 6.90s system 92% cpu 17.359 total\n$ time /usr/bin/bsdtar --zstd -cf /tmp/tar-pipe.tar.zst &lt;dir&gt;\n10.79s user 11.46s system 180% cpu 12.357 total\n$ zstd tar-raw.tar\n\n$ time bsdtar --options zstd:compression-level=6 --zstd -cf /tmp/tar-z6.tar.zst &lt;dir&gt;\n20.41s user 7.94s system 92% cpu 30.542 total\n$ time bsdtar --options zstd:compression-level=9 --zstd -cf /tmp/tar-z9.tar.zst &lt;dir&gt;\n37.27s user 7.34s system 96% cpu 46.029 total\n\n$ ls -goS\n-rw-r--r-- 1 6452066816 Oct 28 16:54 tar-raw.tar\n-rw-r--r-- 1 6427283841 Oct 28 16:54 tar-raw.tar.zst\n-rw-r--r-- 1 6427283829 Oct 28 16:55 tar-pipe.tar.zst\n-rw-r--r-- 1 6427182872 Oct 28 16:54 tar-builtin.tar.zst\n-rw-r--r-- 1 6425529524 Oct 28 17:02 tar-z6.tar.zst\n-rw-r--r-- 1 6423828358 Oct 28 17:03 tar-z9.tar.zst\n\nCompression threads\n#if HAVE_ZSTD_H &amp;&amp; HAVE_LIBZSTD_COMPRESSOR is true, libarchive defaults to set 0 ZSTD_c_nbWorkers, which is to use the number of physical CPU cores.\nOtherwise, given that data-&gt;threads != 0 is not true, the --threads flag will not be appended to command and 1 working thread is used by default.\nCompression levels\nWhen in doubt, either stick with the default level of 3 or something from the 6 to 9 range for a nice trade-off of speed versus space.\nReferences\n\nengineering.fb.com/2016/08/31/core-infra/smaller-and-faster-data-compression-with-zstandard/\n"},"notes/Taking-Notes":{"title":"Taking Notes","links":["notes/Note-Making/Obsidian"],"tags":[],"content":"Organizing notes\n\nCapture ideas on the go with Tana Capture.\nTurn them into organized fleet notes.\nKeep a private journal for complex tasks.\nNurture evergreen notes from fleet notes and journals.\nContinuously evolve your evergreen notes.\n\nTools\nObsidian is a good writing tool for Markdown notes."},"notes/Twitter":{"title":"Twitter","links":[],"tags":[],"content":"uBlock Origin rules\n! Discover more / More Tweets - Conversation view suggestions\ntwitter.com##[aria-label=&quot;Timeline: Conversation&quot;] [data-testid=&quot;cellInnerDiv&quot;] h2:has-text(/^(Discover more|More Tweets)$/):upward([data-testid=&quot;cellInnerDiv&quot;])\ntwitter.com##[aria-label=&quot;Timeline: Conversation&quot;] [data-testid=&quot;cellInnerDiv&quot;] h2:has-text(/^(Discover more|More Tweets)$/):upward([data-testid=&quot;cellInnerDiv&quot;])~div\n"},"notes/Unicode":{"title":"Unicode","links":[],"tags":[],"content":"List of all Emoji characters\nwww.unicode.org/Public/emoji/16.0/emoji-test.txt\nNames list charts\nwww.unicode.org/charts/nameslist/\n\nClick on the block name to get to a code chart.\nClick on a code point in the chart to get the names list. Hovering over the code point shows the name.\n\nSearch for any Unicode character\nunicodeplus.com/"}}